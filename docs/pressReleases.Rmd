---
title: "Press Releases"
output: html_notebook
---

```{r}
library(tidyverse)
rm(list = ls())
```

```{r}
fdp <- read.csv("../scrapedata/fdp.csv", stringsAsFactors=FALSE)

# get the first sentence of the "gastbeitrag" releases
text <- unlist(strsplit(fdp$body, '\\:|\\.'))
ignore <- text[!grep(pattern = "Gastbeitrag", text, ignore.case = T)]

fdp <- fdp %>%
  filter(!grepl("interview",title,ignore.case = T)) %>%
  transmute(text = gsub(paste(ignore, sep = "|", collapse = "|"),"", body),
            date = as.Date(date, format = "%d.%m.%Y"),
            title = gsub('(.+?):','',title, perl = T),
            title_text = paste(title, text),
            source = party
         ) 
```

```{r}
linke <- read.csv("../scrapedata/linke.csv", stringsAsFactors=FALSE)

linke <- linke %>%
  transmute(text = body,
            title = title,
            title_text = paste(title, text),
            date = gsub("\n","",date, perl = T),
            date = as.Date(date, format = "%d. %B %Y"),
            source = party)

```

```{r}
gruene <- read.csv("../scrapedata/gruene.csv", stringsAsFactors=FALSE)

gruene <- gruene %>%
  transmute(text = gsub('Die Fraktionspressestelle auf Twitter: @GruenSprecher','',body),
            title = title,
            title_text = paste(title, text),
            date = as.Date(date, format = "%d.%m.%Y"),
            source = party
         )
```

```{r}
spd <- read.csv("../scrapedata/spd.csv", stringsAsFactors=FALSE)

spd <- spd %>%
  transmute(text = gsub('(.+?):','', body, perl = TRUE, ignore.case = TRUE),
            title = title,
            title_text = paste(title, text),
            date = as.Date(gsub('\\|.*','',date, perl = T), format = "%d.%m.%Y"),
            source = "spd"
         )
```

```{r}
cdu <- read.csv("../scrapedata/cdu.csv", stringsAsFactors=FALSE)

cdu <- cdu %>%
  transmute(text = gsub('Berlin','',body, ignore.case = TRUE, perl = TRUE),
         text = gsub('ots','',text, ignore.case = TRUE, perl = TRUE),
         text = gsub('Presse.*','', text, perl = TRUE, ignore.case = TRUE),
         text = gsub('Telefon.*','', text, perl = TRUE, ignore.case = TRUE),
         text = gsub('Fax.*','', text, perl = TRUE, ignore.case = TRUE),
         text = gsub('Internet.*','', text, perl = TRUE, ignore.case = TRUE),
         text = gsub('Email.*','', text, perl = TRUE, ignore.case = TRUE),
         title = gsub('(.+?):','',title, perl = T),
         title_text = paste(title, text),
         date = as.Date(gsub('\\-.*','',date, perl = T), format = "%d.%m.%Y"),
         source = "union"
         ) 
```


```{r}
afd <- read.csv("../scrapedata/afd.csv", stringsAsFactors=FALSE)

afd <- afd %>%
  filter(!grepl("interview",title,ignore.case = T)) %>%
  transmute(text = gsub('Berlin[^n\\.]*','',body, ignore.case = TRUE, perl = TRUE),
         text = gsub('Drucken','',text, ignore.case = TRUE, perl = TRUE),
         title = gsub('(.+?):','',title, perl = T),
         title_text = paste(title, text),
         source = "afd",
         date = gsub('Mrz',"M채rz",date),
         date = as.Date(date, format = "%d%b %Y")
         ) 
```

```{r}
pressReleases <- bind_rows(cdu, spd, afd, gruene, linke, fdp) %>%
  filter(date >= as.Date("2017-06-01")) %>%
  mutate(type = "press")
```

```{r}
pressReleases %>%
  ggplot(aes(source, fill=source)) +
  geom_bar(show.legend = F) +
  ggthemes::theme_hc() +
  ggthemes::scale_fill_hc() +
  labs(y = NULL, x = NULL, title = "Number of press releases (June 2017 - March 2018)")
```

# Clean Data
```{r}
pressReleases$text_cleaned <- gsub("[[:punct:]]", " ", pressReleases$title_text)
pressReleases$text_cleaned <- gsub("[[:cntrl:]]", " ", pressReleases$text_cleaned)
pressReleases$text_cleaned <- gsub("[[:digit:]]", " ", pressReleases$text_cleaned)
pressReleases$text_cleaned <- gsub("^[[:space:]]+", " ", pressReleases$text_cleaned)
pressReleases$text_cleaned <- gsub("[[:space:]]+$", " ", pressReleases$text_cleaned)
pressReleases$text_cleaned <- tolower(pressReleases$text_cleaned)

## Remove stopwords
# 1
german_stopwords_full <- read.table("dict/german_stopwords_full.txt", stringsAsFactors = F)
german_stopwords_full <- german_stopwords_full$V1

# 2
mystopwords <- c("januar","feburar","m채rz","april","mai","juni","juli",
                 "august","september","oktober","november","dezember",
                 "linke","cdu","csu","union","spd","afd","gr체ne","fdp"
                 )
stopwords <- c(german_stopwords_full, mystopwords)
stopwords <- unique(stopwords)

# 3
pressReleases$text_cleaned<- tm::removeWords(pressReleases$text_cleaned, stopwords)

## Stemming
stem_text<- function(text, language = "porter", mc.cores = 1) {
  # stem each word in a block of text
  stem_string <- function(str, language) {
    str <- strsplit(x = str, split = "\\s")
    str <- SnowballC::wordStem(unlist(str), language = language)
    str <- paste(str, collapse = " ")
    return(str)
  }
   
  # stem each text block in turn
  x <- parallel::mclapply(X = text, FUN = stem_string, language, mc.cores = mc.cores)
   
  # return stemed text blocks
  return(unlist(x))
}

pressReleases$text_cleaned1 <- stem_text(pressReleases$text_cleaned)
```

```{r}
pressReleases %>%
  sample_n(1) %>%
  select(text_cleaned, text_cleaned1) %>%
  htmlTable::htmlTable()
```

# Inspect Data

```{r}
library(tidytext)

tokens <- pressReleases %>% unnest_tokens(word, text_cleaned)
bigrams <- pressReleases %>% unnest_tokens(bigrams, text_cleaned, token="ngrams", n=2)
```

```{r}
tokens2 <- tokens %>%
  count(party, word, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word,party,n)

tokens2 %>% 
    arrange(desc(tf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(party) %>% 
  top_n(10) %>% 
  ungroup %>%
  ggplot(aes(word, tf, fill = party)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~party, ncol = 2, scales = "free") +
  coord_flip()
```

```{r}
bigrams2 <- bigrams %>%
  count(party, bigrams, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(bigrams,party,n)

bigrams2 %>% 
    arrange(desc(tf_idf)) %>%
  mutate(word = factor(bigrams, levels = rev(unique(bigrams)))) %>% 
  group_by(party) %>% 
  top_n(10) 
  ungroup %>%
  ggplot(aes(bigrams, tf_idf, fill = party)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~party, ncol = 2, scales = "free") +
  coord_flip()
```

# Structural Topic Model

## Combine with news articles
```{r}
load("../output/data_step2.Rda")
```

```{r}
btw %>%
  mutate(date = as.Date(date),
         type = "news",
         source = site
         ) %>%
  bind_rows(.,pressReleases) -> model_df
```

```{r fig.height=3, fig.width=6}
model_df %>%
  ggplot(aes(source, fill=type)) +
  geom_bar(show.legend = F) +
  facet_wrap(~type, scales = "free") +
  ggthemes::theme_economist() +
  ggthemes::scale_fill_economist()
```

```{r}
model_df %>%
  sample_n(10) %>% select(text_cleaned1, source) %>% htmlTable::htmlTable()
```

## Build Corpus
```{r eval=FALSE, include=FALSE}
library(stm)

processed <- textProcessor(model_df$text_cleaned1, 
                           metadata = model_df[,c("source","type","text_cleaned1")],
                           wordLengths = c(2,Inf),
                           lowercase = F,
                           removestopwords = F,
                           removenumbers = F,
                           removepunctuation = F,
                           stem = F)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
out$meta$source <- as.factor(out$meta$source)

save(model_df, out, file="../output/final_modeldf.Rda")
```

## Run Model
```{r eval=FALSE, include=FALSE}
k=40

stmOut <- stm(documents = out$documents, vocab = out$vocab,
              K = k, prevalence =~ source,
              max.em.its = 75, data = out$meta,
              init.type = "Spectral")

save(model_df, out, stmOut, file = "../output/models/finalmodel_40.Rda")
```

## Results
```{r}
library(stm)
library(tidyverse)
library(dplyr)

rm(list = ls())
load("../output/models/pressR_40.RData")
```

### 1. Label topics

```{r}
labels <- labelTopics(stmOut)

topics.df <- as.data.frame(labels$prob) %>%
  transmute(topic = as.numeric(rownames(.)),
            topic_name = paste("Topic",as.character(topic),":",V1,V2,V3,V4))
```

### Topic frequency
```{r}
overall_freq <- as.data.frame(colMeans(stmOut$theta)) %>%
  transmute(
    topic = as.numeric(rownames(.)),
    frequency = colMeans(stmOut$theta)
         ) %>%
  left_join(., topics.df, by = "topic") %>% 
  arrange(desc(frequency))%>%
  mutate(order = row_number())

head(overall_freq)
```

```{r fig.height=5, fig.width=4}
overall_freq %>%
  ggplot(aes(reorder(topic_name, -order), frequency)) +
  geom_col(alpha = 0.7) +
  coord_flip() +
  ggthemes::theme_fivethirtyeight() +
  labs(x=NULL, y=NULL, title="Expected Frequency of Topics") 
```

### Relationship between metadata and topics

```{r}
k <- 40
effect <- estimateEffect(c(1:k) ~party, stmOut, metadata = out$meta, uncertainty = "Global")
```


#### Mean prevalence of each topic within each news source corpus
```{r fig.height=16, fig.width=12}
par(mfrow = c(8,5))

labels <- c("CDU/CSU","SPD", "AfD", "B90/Gr체ne","DIE LINKE")
#topic <- 1

for (i in 1:k) {
  plot(effect, covariate = "party", topics = i,
     method = "pointestimate", model = stmOut,
     main = topics.df$topic_name[i],
     #xlim = c(-.02,.08),
     labeltype = "custom",
     custom.labels = labels
  )
}
```

```{r}
theta <- (as.data.frame(stmOut$theta)) %>%
  mutate(document = as.numeric(rownames(.))) %>%
  gather(key = topic, value = gamma, -document) %>%
  mutate(topic = as.numeric(gsub("V","",topic)))
```

```{r}
top_topics <- theta %>% 
  group_by(document) %>%
  mutate(therank = rank(-gamma)) %>%
  filter(therank == 1) %>%
  select(- therank)

head(top_topics)
```

```{r}
df <- pressReleases %>%
  mutate(document = as.numeric(rownames(.))) %>%
  left_join(.,top_topics, by="document")
```

```{r}
df %>% filter(topic==12) %>% 
  arrange(desc(gamma)) %>%
  select(party, title, gamma) %>% htmlTable::htmlTable()
```

