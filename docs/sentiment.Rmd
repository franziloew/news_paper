---
title: "Different types of media bias - tonality bias"
author: "Franziska Löw"
date: "`r format(Sys.Date())`"
output: html_document
---

```{r include=FALSE}
rm(list = ls())

library(tidyverse)
library(ggthemes)
source("func/functions.R")

normalize_data <- function(x) {
  # normalize data between -1,1
  if (is.numeric(x)) {
    y <- 2*((x - min(x, na.rm = T)) / (max(x, na.rm = T) - min(x, na.rm = T)))-1
    return(y)
  } else {
    return(x)
  }

}

load("../output/data_step2.Rda")

parties <- c("SPD",paste("CDU","CSU",sep = "|"),"FDP","Grüne","AfD","Linke")
btw %>% mutate(doc_index = as.numeric(rownames(.))) -> btw
```

To measure the tone (or sentiment) of an article a dictionary-based method is applied. To conduct such an analysis, a list of words (dictionary) associated with a given emotion, such as negativity is pre-defined. The document is then deconstructed into individual words and each word is assigned a sentiment value according to the dictionary, where the sum of all values results in the emotional score for the given document. Such lexical or "bag-of-words" approaches are widely presented in the finance literature to determine the effect of central banks' monetary policy communications on asset prices and real variables (\citet{nyman_news_2018} \citet{tetlock_giving_2007}, \citet{tetlock_more_2008}). \citet{hansen_shocking_2016} use a similar approach to measure "the two Ts" (Topic and tone). They explore the effects of FOMC (Federal Open Market Committee) statements on both market and real economic variables. To calculate their score, they subtract the negative words from the positive words und divide this by the number of total words of the statement. A similar score is used by \citet{nyman_news_2018}, who measure the effect of narratives and sentiment of financial market text-based data on developments in the financial system. They count the number of occurrences of excitement words and anxiety words and then scale these numbers by the total text size as measured by the number of characters.

The present paper uses a dictionary that lists words associated with positive and negative polarity weighted within the interval of $[-1; 1]$. [SentimentWortschatz](http://wortschatz.uni-leipzig.de/de/download), is a publicly available German-language resource for sentiment analysis, opinion mining, etc.. The current version of SentiWS (v1.8b) contains 1,650 positive and 1,818 negative words, which sum up to 15,649 positive and 15,632 negative words including their inflections, respectively. Table \ref{t_sentdict} shows ten examples entries of the dictionary. To obtain a more reliable correlation between the "target" (a political party) and the word's polarity score, sentiment words are counted in a window of two sentences before and after the mention of a political party \citep{junque_de_fortuny_media_2012}. The tonality score of an article is then calculated from the sum of the these words divided by the total number of words in that article. Again, the tonality bias for is then computed as the deviation of each party's specific tonality from the average tonality of all other parties in that outlet and standardized to range from −1 to 1.

The score is then calculated from the sum of the words in a document (which can be assigned to a word from the dictionary) divided by the total number of words in that document.

```{r include=FALSE}
sent <- c(
  # positive Wörter
  readLines("dict/SentiWS_v1.8c_Negative.txt",
            encoding = "UTF-8"),
  # negative W??rter
  readLines("dict/SentiWS_v1.8c_Positive.txt",
            encoding = "UTF-8")
  ) %>%
  
  lapply(function(x) {
  # Extrahieren der einzelnen Spalten
  res <- strsplit(x, "\t", fixed = TRUE)[[1]]
  return(data.frame(words = res[1], value = res[2],
                    stringsAsFactors = FALSE))
  }) %>% 
  
  bind_rows %>%
  mutate(word = gsub("\\|.*", "", words) %>% 
           tolower, value = as.numeric(value),
         type = gsub(".*\\|", "", words)) %>%
  
  # nur adjektive oder adverben
  # filter(type == "ADJX" | type == "ADV") %>%
  # manche Wörter kommen doppelt vor, hier nehmen wir den mittleren Wert
  group_by(word) %>%
  dplyr::summarise(value = mean(value)) %>% ungroup %>%
  # Delete "Heil" (wegen Hubertus Heil)
  filter(!grepl('heil',word,ignore.case = T)) %>%
  # welcome to hell (g20)
  filter(!grepl('hell',word,ignore.case = T)) %>%
  # filter values that that score between -0.1 and +0.1 
  filter(!dplyr::between(value, -0.1,0.1))
```

```{r echo=FALSE}
sent %>% 
  sample_n(10) %>%
  htmlTable::htmlTable(align="l")
```

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
library(tidytext)

# Tokenize text to sentences
sentence <- btw %>%
  select(doc_index, text) %>%
  unnest_tokens(sentence, text, token = "sentences") 
  
# check if a party is mentioned in a sentence
sentence <- sentence %>%
  mutate(party = ifelse(grepl("SPD",sentence, ignore.case = T),"SPD",NA),
         party = ifelse(grepl("AfD",sentence, ignore.case = T),"AfD",party),
         party = ifelse(grepl("FDP",sentence, ignore.case = T),"FDP",party),
         party = ifelse(grepl("Grüne",sentence, ignore.case = T),"Grüne",party),
         party = ifelse(grepl("Linke",sentence, ignore.case = T),"Linke",party),
         party = ifelse(grepl(paste("CDU","CSU","Union",sep="|"),sentence, ignore.case = T),"Union",party)
         )
```

```{r eval=FALSE, include=FALSE}
token <- sentence %>%
    # Filter one article (doc_index)
    filter(doc_index == 10005) %>%
    mutate(sentence_index = as.numeric(rownames(.))) %>%
    # and tokenize into words
    unnest_tokens(word, sentence) %>%
    # Combine second word with sentiment values
    left_join(., sent, by = "word") 
```


```{r eval=FALSE, include=FALSE}
parties <- c("SPD","Union","FDP","Grüne","AfD","Linke")
sentDF <- data.frame() 

for (x in btw$doc_index) {
  
  token <- sentence %>%
    # Filter one article (doc_index)
    filter(doc_index == x) %>%
    mutate(sentence_index = as.numeric(rownames(.))) %>%
    # and tokenize into words
    unnest_tokens(word, sentence) %>%
    # Combine second word with sentiment values
    left_join(., sent, by = "word") 
  
  for (i in parties) {
    
    token %>% 
      # get all sentences where the party is mentioned
      filter(party == i) %>% 
      distinct(sentence_index) %>%
      # get the 2-sentence range
      mutate(lower1 = sentence_index - 1,
             # lower2 = sentence_index - 2,
             upper1 = sentence_index + 1
             # upper2 = sentence_index + 2
             )  %>%
      gather(index, sentence) %>% 
      distinct(sentence) %>%
      filter(sentence > 0) %>%
      select(sentence) -> tempdf
  
    token %>%
      filter(sentence_index %in% tempdf$sentence) -> tempdf2
    
    tempdf2 %>%
      mutate(value = ifelse(is.na(value),0,value)) %>%
      summarise(sum_value = sum(value)) -> sum
    
    # normalize the sentiment value by the number of words in the document
    sentiment <- sum/nrow(tempdf2)
    
    tempdf3 <- cbind(x,i,sentiment) 
    sentDF <- rbind(sentDF,tempdf3)
  } 
}
```

```{r eval=FALSE, include=FALSE}
sentDF <- sentDF %>%
  transmute(doc_index = x,
            party = as.character(i),
            sentiment_sum = sum_value
            ) %>%
  left_join(.,btw %>% select(doc_index,date, medium, text), by="doc_index") %>%
  mutate(
    year = lubridate::year(date),
    month = lubridate::month(date),
    date = as.Date(paste0(year,"/",month,"/1")),
    party = ifelse(grepl("Cdu",party, ignore.case = T),"Union",party),
    party = ifelse(grepl("Linke",party, ignore.case = T),"Linke",party)
    )

save(sentDF, file = "../output/sentiment.Rda")
```

## Sentiment value

```{r}
load("../output/sentiment.Rda")
```

```{r eval=FALSE, include=FALSE}
sent <- sentDF %>% 
  group_by(medium, party, year, month) %>%
  dplyr::summarise(sent = mean(sentiment_sum, na.rm = T)) %>%
  ungroup()
```

```{r echo=FALSE, fig.height=8, fig.width=10}
sent %>%
  mutate(
    sent_norm = normalize_data(sent),
    date =as.Date(paste("01",month,year, sep = "-"), format="%d-%m-%Y")
  ) %>%
  ggplot(aes(date, sent_norm, color = medium, group = medium)) +
  geom_line() +
  geom_hline(yintercept = 0, size = 0.3, color = "grey30", linetype = 2) +
  facet_wrap(~party) +
  labs(y = "sentiment", x =NULL) +
  guides(colour = guide_legend(nrow = 1)) +
  theme(legend.position = "bottom",
        legend.title = element_blank()) 
```

```{r eval=FALSE, include=FALSE}
save(sent, file = "../output/sentiment.Rda")
```