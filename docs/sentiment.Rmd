---
title: "Different types of media bias - tonality bias"
author: "Franziska Löw"
date: "`r format(Sys.Date())`"
output: html_document
---

```{r include=FALSE}
rm(list = ls())

library(tidyverse)
library(ggthemes)
source("func/functions.R")

load("../output/data_step2.Rda")

parties <- c("SPD",paste("CDU","CSU",sep = "|"),"FDP","Grüne","AfD","Linke")
btw %>% mutate(doc_index = as.numeric(rownames(.))) -> btw
```

To measure the tone (or sentiment) of a document adictionary-based method is applied. To conduct such an analysis, a lists of words (dictionary) associated with a given emotion, such as negativity is pre-defined by the analyst. The target text is then deconstructed into individual words (or tokens) and the frequencies of words contained in a given dictionary are then calculated. 

The present paper uses a dictionary that lists words associated with positive and negative polarity weighted within the interval of $[-1; 1]$. [SentimentWortschatz](http://wortschatz.uni-leipzig.de/de/download), or SentiWS for short, is a publicly available German-language resource for sentiment analysis, opinion mining etc. The current version of SentiWS (v1.8b) contains 1,650 positive and 1,818 negative words, which sum up to 15,649 positive and 15,632 negative word forms incl. their inflections, respectively. It not only contains adjectives and adverbs explicitly expressing a sentiment, but also nouns and verbs implicitly containing one.

The sentiment score for each party in an article is calculated from each word that occurs in a window of two sentences before and two sentences after the occurence of that political party. An article can mention several party names, or switch tone. The given interval ensures a more reliable correlation between the political party being mentioned (the "target") and the word's polarity score, contrary to measuring all adjectives in the article. A similar approach for target identification is used in de Fortuny et al. (2012) and in Balahur et al. (2010). They latter used a 10-word window and report improved accuracy when compared to measuring all words in the article. Furthermore adjectives that score between -0.1 and +0.1 are excluded to reduce noise. 

The score is then calculated from the sum of the words in a document (which can be assigned to a word from the dictionary) divided by the total number of words in that document.

```{r include=FALSE}
sent <- c(
  # positive Wörter
  readLines("dict/SentiWS_v1.8c_Negative.txt",
            encoding = "UTF-8"),
  # negative W??rter
  readLines("dict/SentiWS_v1.8c_Positive.txt",
            encoding = "UTF-8")
  ) %>%
  
  lapply(function(x) {
  # Extrahieren der einzelnen Spalten
  res <- strsplit(x, "\t", fixed = TRUE)[[1]]
  return(data.frame(words = res[1], value = res[2],
                    stringsAsFactors = FALSE))
  }) %>% 
  
  bind_rows %>%
  mutate(word = gsub("\\|.*", "", words) %>% 
           tolower, value = as.numeric(value),
         type = gsub(".*\\|", "", words)) %>%
  
  # nur adjektive oder adverben
  # filter(type == "ADJX" | type == "ADV") %>%
  # manche Wörter kommen doppelt vor, hier nehmen wir den mittleren Wert
  group_by(word) %>%
  dplyr::summarise(value = mean(value)) %>% ungroup %>%
  # Delete "Heil" (wegen Hubertus Heil)
  filter(!grepl('heil',word,ignore.case = T)) %>%
  # welcome to hell (g20)
  filter(!grepl('hell',word,ignore.case = T)) %>%
  # filter values that that score between -0.1 and +0.1 
  filter(!dplyr::between(value, -0.1,0.1))
```

```{r echo=FALSE}
sent %>% 
  sample_n(10) %>%
  htmlTable::htmlTable(align="l")
```

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
library(tidytext)

# Tokenize text to sentences
sentence <- btw %>%
  select(doc_index, text) %>%
  unnest_tokens(sentence, text, token = "sentences") 
  
# check if a party is mentioned in a sentence
sentence <- sentence %>%
  mutate(party = ifelse(grepl("SPD",sentence, ignore.case = T),"SPD",NA),
         party = ifelse(grepl("AfD",sentence, ignore.case = T),"AfD",party),
         party = ifelse(grepl("FDP",sentence, ignore.case = T),"FDP",party),
         party = ifelse(grepl("Grüne",sentence, ignore.case = T),"Grüne",party),
         party = ifelse(grepl("Linke",sentence, ignore.case = T),"Linke",party),
         party = ifelse(grepl(paste("CDU","CSU","Union",sep="|"),sentence, ignore.case = T),"Union",party)
         )
```

```{r eval=FALSE, include=FALSE}
token <- sentence %>%
    # Filter one article (doc_index)
    filter(doc_index == 10005) %>%
    mutate(sentence_index = as.numeric(rownames(.))) %>%
    # and tokenize into words
    unnest_tokens(word, sentence) %>%
    # Combine second word with sentiment values
    left_join(., sent, by = "word") 
```


```{r eval=FALSE, include=FALSE}
parties <- c("SPD","Union","FDP","Grüne","AfD","Linke")
sentDF <- data.frame() 

for (x in btw$doc_index) {
  
  token <- sentence %>%
    # Filter one article (doc_index)
    filter(doc_index == x) %>%
    mutate(sentence_index = as.numeric(rownames(.))) %>%
    # and tokenize into words
    unnest_tokens(word, sentence) %>%
    # Combine second word with sentiment values
    left_join(., sent, by = "word") 
  
  for (i in parties) {
    
    token %>% 
      # get all sentences where the party is mentioned
      filter(party == i) %>% 
      distinct(sentence_index) %>%
      # get the 2-sentence range
      mutate(lower1 = sentence_index - 1,
             # lower2 = sentence_index - 2,
             upper1 = sentence_index + 1
             # upper2 = sentence_index + 2
             )  %>%
      gather(index, sentence) %>% 
      distinct(sentence) %>%
      filter(sentence > 0) %>%
      select(sentence) -> tempdf
  
    token %>%
      filter(sentence_index %in% tempdf$sentence) -> tempdf2
    
    tempdf2 %>%
      mutate(value = ifelse(is.na(value),0,value)) %>%
      summarise(sum_value = sum(value)) -> sum
    
    # normalize the sentiment value by the number of words in the document
    sentiment <- sum/nrow(tempdf2)
    
    tempdf3 <- cbind(x,i,sentiment) 
    sentDF <- rbind(sentDF,tempdf3)
  } 
}
```

```{r eval=FALSE, include=FALSE}
sentDF <- sentDF %>%
  transmute(doc_index = x,
            party = as.character(i),
            sentiment_sum = sum_value
            ) %>%
  left_join(.,btw %>% select(doc_index,date, medium, text), by="doc_index") %>%
  mutate(
    year = lubridate::year(date),
    month = lubridate::month(date),
    date = as.Date(paste0(year,"/",month,"/1")),
    party = ifelse(grepl("Cdu",party, ignore.case = T),"Union",party),
    party = ifelse(grepl("Linke",party, ignore.case = T),"Linke",party)
    )

save(sentDF, file = "../output/sentiment.Rda")
```

## Sentiment value

```{r}
load("../output/sentiment.Rda")
```

```{r}
sent <- sentDF %>% 
  group_by(medium, party, year, month) %>%
  dplyr::summarise(sent = mean(sentiment_sum, na.rm = T)) %>%
  ungroup()
```

```{r echo=FALSE}
sent %>%
  group_by(medium,party) %>%
  summarize(avg_sent_p_s = mean(avg_sent_p_s, na.rm = T)) %>%
  mutate(avg_sent_p_s = normalize_data2(avg_sent_p_s)) %>%
  select(medium, party, avg_sent_p_s) %>%
  spread(key = party, value = avg_sent_p_s) %>%
  ggiraphExtra::ggRadar(aes(color = medium),
                        interactive = T,
                        alpha = 0,
                        rescale = F,
                        legend.position = "right") 
```

```{r}
save(sent, file = "../output/sentiment.Rda")
```