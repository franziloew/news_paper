---
title: "Topic Modeling of News"
author: "Franziska L??w"
date: "`r format(Sys.Date())`"
output: github_document
---

```{r, include=FALSE}
# load the packages
libs <- c("readr","lubridate","tm","stm","RPostgreSQL","tidyverse",
          "class","knitr","data.table","SnowballC","parallel","gridExtra","jsonlite","stringr","scales")
lapply(libs, library, character.only = TRUE)

rm(list=ls())
```

# Load and prepare Dataframes

```{r}
# Connect to postgres
pg = dbDriver("PostgreSQL")

con = dbConnect(pg, user="franzi", 
                password="", 
                host="localhost", 
                port=5432,
                dbname="news")

# load data from eventregistry
df1 <- dbGetQuery(con, 'SELECT * FROM er_scrapes')

# load data from webhose.io
load("../output/news_cleaned.Rda")
df2 <- df
```

## Check Dataframes
```{r}
# Eventregistry
str(df1)
```

```{r message=FALSE}
# Webhose.io
str(df2)
```

## Combine dataframes

### 1. clean Eventregistry Data (DF1)

```{r}
df1 %>% 
  mutate(site = str_extract(source,"(?<='uri': ')[A-z][^']*"),
         fullname = str_extract(source,"(?<='title': ')[A-z][^']*")
         ) %>%
  group_by(site, fullname, source) %>%
  tally(sort = T) %>% htmlTable::htmlTable(align = "l")
```

```{r}
df1 <- df1 %>% 
  mutate(site = str_extract(source,"(?<='uri': ')[A-z][^']*"),
         fullname = str_extract(source,"(?<='title': ')[A-z][^']*"),
         text = body,
         date = as.POSIXct(date)
         ) %>%
  select(date,fullname,site,title,text,url) 

str(df1)
```

### 2. clean Webhose.io Data (DF2)
```{r}
df2 <- df2 %>% 
  mutate(
         date = as.POSIXct(date)
         ) %>%
  select(date,site,title,text,url) 

str(df2)
```

### 3. Combine DF1 and DF2
```{r}
# combine
df_full <- bind_rows(df1,df2)
# save
save(df_full, file="../output/data_step1.Rda")
```

# Clean Data
```{r}
library(tidyverse)
rm(list=ls())
load(file="../output/data_step1.Rda")
```

## A. Select Sample

### 1. by date, medium & party ... 
```{r}
start <- "2017-06-01"
end <- "2018-03-01"
keeps <- c("bild.de","focus.de","spiegel.de", "stern.de","tagesschau.de", "welt.de", "zeit.de")
parties <- c("CDU","CSU","AfD","SPD","FDP","Grüne","Linke")

df <- df_full %>%
  filter(site %in% keeps) %>%
  mutate(medium = ifelse(site == "zeit.de","ZEIT ONLINE", site),
         medium = ifelse(site == "focus.de","FOCUS Online",medium),
         medium = ifelse(site == "spiegel.de","SPIEGEL ONLINE",medium),
         medium = ifelse(site == "welt.de", "DIE WELT",medium),
         medium = ifelse(site == "bild.de", "Bild.de",medium),
         medium = ifelse(site == "stern.de", "stern.de",medium),
         medium = ifelse(site == "tagesschau.de","tagesschau.de",medium)
         ) %>%
  select(-fullname) %>%
  filter(date > start & date < end) %>%
  filter(grepl(paste(parties, sep="|", collapse = "|"), text)) %>%
  mutate(title_text = paste(title, text, sep=" "),
         index = as.integer(rownames(.)),
         text_length = sapply(gregexpr("\\S+", text), length)
         )  
```

### 2. by website section...
```{r}
# split the url into several parts (separated by  '/') and save it into a list
url_pattern <- str_split(df$url,"[/]")

# convert the list into a dataframe
df_url <- do.call(rbind.data.frame, url_pattern)

# select columns 3,4,5 of that df and rename the columns accordingly
df_url <- df_url[,c(3,4,5)]
names(df_url) <- c("subsite","ressort1","ressort2")

# combine with df
df <- df %>% 
  left_join(df_url %>% 
              transmute(ressort1 = as.character(ressort1),
                        ressort2 = as.character(ressort2),
                        subsite = as.character(subsite),
                        index = as.integer(rownames(.))
                        ), by = "index")
```


```{r}
df %>% 
  filter(is.na(ressort1)) %>%
  sample_n(10) %>% select(site, title, subsite, url, ressort1) %>% 
  htmlTable::htmlTable(align="l")
```

```{r fig.height=6, fig.width=10}
df %>%
  group_by(site, ressort1) %>%
  tally(sort=T) %>%
  top_n(n = 10, wt = n) %>%
  ggplot(aes(reorder(ressort1,n),n)) +
  ggthemes::theme_hc() +
  #ggthemes::scale_fill_hc() +
  geom_col(show.legend = F, alpha=.6) +
  facet_wrap( ~ site, ncol =4, scales = "free") +
  coord_flip() +
  labs(x = NULL, y = NULL, title="Top 10 news section",
       subtitle = "Ressorts where selected from the URL of each article")
```

```{r}
df %>%
  filter(ressort1=="politik"|ressort1=="inland") %>% 
  group_by(medium, ressort2) %>%
  tally(sort=T) %>%
  top_n(n = 2, wt = n) %>%
  ggplot(aes(reorder(ressort2,n),n)) +
  ggthemes::theme_hc() +
  #ggthemes::scale_fill_hc() +
  geom_col(show.legend = F, alpha=.6) +
  facet_wrap( ~ medium, ncol =4, scales = "free") +
  coord_flip() 
```

```{r}
btw1 <- df %>% filter(ressort1=="politik") %>% filter(ressort2=="deutschland" | ressort2 == "inland")

# tagesschau 
btw2 <- df %>% filter(ressort1=="inland")

# combine 
btw <- bind_rows(btw1, btw2)
```

```{r eval=FALSE, include=FALSE}
# pat1 <- paste("sport", "kultur", "ausland", "kommentar", "multimedia", "eilmeldung","wetter","unwetter", 
#               "index", "100sekunden","fernsehen","international","german","arabisch","reaktionenspanien",
#               "video","praxistipps","diverses","ticker","stream","podcast","elbvertiefung","magazin", sep="|")
# 
# pat2 <- paste("grossbritannien", "jemen", "kabul", "teheran", "syrien", "saudi-arabien", "turku", "venezuela", 
#               "xiaobo", "zugunglueck", "korrespondenten", "lego", "unterhaltung", "scheichun", "parliament",
#               "airbus-enders", "alabama-moore", "olympiasperre", "las-vegas-angriff-109", "malta-journalistin",
#               "polen-eu-rechtsstaatverfahren", "radiopreis", sep="|")

# Select articles within the politics section
btw <- df %>% filter(ressort1=="politik"|ressort1=="inland") %>%
  filter(!grepl(paste(pat1,pat2,sep="|"),title_text, ignore.case = T))
```

```{r eval=FALSE, include=FALSE}
pat1 <- "\\w\\.de/politik"
pat2 <- "tagesschau.de"
pat3 <- paste("sport", "kultur", "ausland", "kommentar", "multimedia", "eilmeldung", "wirtschaft", "schlusslicht", "hintergrund", "tutorials", "wetter","index", "100sekunden", "app", "fernsehen", "grossbritannien", "jemen", "kabul", "teheran", "syrien", "saudi-arabien", "turku", "venezuela", "xiaobo", "zugunglueck", "korrespondenten", "lego", "unterhaltung", "scheichun", "parliament", "airbus-enders", "alabama-moore", "olympiasperre", "las-vegas-angriff-109", "malta-journalistin", "polen-eu-rechtsstaatverfahren", "radiopreis", "bild-international", "spiegel.de/international","german","arabisch", "reaktionenspanien", "wetter", "video", "praxistipps", "diverses", "ticker", "stream", "podcast", "elbvertiefung", "magazin", sep="|")

btw <- df %>% 
  filter(grepl(paste(pat1, pat2, sep="|"),url, perl =TRUE)) %>%
  filter(!grepl(pat3, url))
```

### 3. by more filters ...
```{r}
btw <- btw %>%
  # Filter Articles with less than 120 words
  filter(text_length > 120) %>%
  arrange(desc(text_length)) %>%
  
  # Filter Duplicates
  distinct(url, .keep_all = TRUE) %>%
  
  # remove articles that only contain video 
  filter(!grepl("Nutzungsbedingungen Embedding Tagesschau", 
                title_text,perl = TRUE, ignore.case = TRUE)) %>%
  filter(!grepl("video",url)) %>%
  filter(!grepl("</div>", title_text)) %>%
  
  # remove text that mostly contain user comments
  filter(!startsWith(text,"1.")) %>%
  
  # remove articles behind a pay-wall
  filter(!grepl("SPIEGEL-Plus-Artikel", text)) %>%

  # remove articles about Deutsche Bahn and weather
  filter(!grepl("wetter", url, ignore.case = TRUE)) %>%
  filter(!grepl("bahn", url, ignore.case = TRUE)) 
```

```{r}
save(df, btw, file="../output/data_step2.Rda")

rm(list=ls())
load("../output/data_step2.Rda")
```

## B. Text Pre-Processing

Check tokens, bigrams, ...
```{r}
library(tidytext)

btw %>%
  select(site, title_text) %>%
  unnest_tokens(bigram, title_text, token = "ngrams", n=2) -> bigrams

head(bigrams)
```

```{r}
bigrams %>%
  dplyr::count(site, bigram) %>%
  bind_tf_idf(bigram, site, n) %>%
  arrange(desc(tf_idf))
```

The following two chuncks of code are just to check right regex definitions to use it inthe clean.text function.
```{r}
pat <- "video konnte nicht abgespielt"
btw %>%
  filter(grepl(pat, title_text, perl = TRUE, ignore.case = TRUE)) %>%
  select(site, text) %>%
  #filter(site=="tagesschau.de") %>%
  sample_n(2) %>%
  htmlTable::htmlTable()
  #.[2,]
  # group_by(site) %>%
  # tally(sort = TRUE)
```

```{r}
pat <- "Video konnte nicht[^\n]*"
btw_test <- btw 

as.data.frame(str_match(btw_test$title_text , pat)) ->test

test %>%
  filter(!is.na(test)) 
```

### Domain specific cleaning
Apply cleaning function  
```{r, echo=FALSE, message=FALSE, warning=FALSE}
clean.text <- function(x)
  {
  # All
  x = gsub("Getty Images", "", x)
  x = gsub('Startseite[^\n]*', "", x, ignore.case = TRUE, perl = TRUE)
  x = gsub("deutsche presse agentur","", x, ignore.case = TRUE, perl = TRUE)
  x = gsub("Eine Kolumne von \\w{1,} \\w{1,}", "", x, ignore.case = T, perl = TRUE)

  # Bild.de
  x = gsub("Shopübersicht Top Gutscheine", "", x)
  x = gsub('Politik Inland[^\n]*', "", x, perl = TRUE)
  
  # welt.de
  x = gsub('Quelle: N24[^\n]*', "", x, perl = TRUE)
  x = gsub('infocom[^\n]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('Infografik[^\n]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('versuchen Sie es[^.]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('Video konnte nicht[^\n]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('Welt twitter', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('\\w{1,} für Abonnenten', "", x, perl = TRUE, ignore.case = TRUE)

  # FOCUS.de
  x = gsub("FOCUS Online", "", x)
  x = gsub('Wochit[^\n]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub("Vielen Dank! Ihr Kommentar wurde abgeschickt.", "", x)
  x = gsub('Im Interesse unserer User[^"]*', "", x, perl = TRUE)
  x = gsub('Sie haben noch 800[^"]*', "", x, perl = TRUE)
  x = gsub('Erzählen Sie auf FOCUS Online über Ihren Heimatort Teilen Sie Ihren Artikel und Ihr Foto', "", x, perl = TRUE)
  x = gsub("Bericht schreiben", "", x)
  x = gsub("Vielen Dank! Ihr Kommentar wurde abgeschickt.", "", x)
  x = gsub("Hier können Sie selbst Artikel verfassen:","", x)
  x = gsub("Live-Ticker", "", x)
  x = gsub('Aus unserem Netzwerk[^"]*', "", x, perl = TRUE)
  x = gsub("</div>[^*]*", "", x, perl = TRUE)


  # Spiegel.de
  x = gsub("7 mal 17", "", x)
  x = gsub("Zur Startseite Diesen Artikel... Drucken Feedback Nutzungsrechte", "", x)
  x = gsub('Liebe Leserin, lieber Leser,\num diesen[^"]*', "", x)
  x = gsub('Liebe Leserin, lieber Leser, um diesen[^"]*', "", x)
  x = gsub('ejf[^"]*', "", x)
  x = gsub('tjf[^"]*', "", x)
  x = gsub('Fotostrecke[^"]*', "", x, perl = TRUE, ignore.case = TRUE)
  x = gsub('Florian Gathmann[^\n]*', "", x, perl = TRUE)
  x = gsub('Eine Kolumne von Jan Fleischhauer', "", x, perl = TRUE)
  x = gsub("Wenig Zeit? Am Textende gibt's eine Zusammenfassung", "", x)
  x = gsub("Twitter: @\\w{1,} folgen Mehr Artikel von \\w{1,} \\w{1,}", "", x, perl = TRUE)

  # Zeit.de
  x = gsub("Inhalt Seite", "", x)
  x = gsub("\\w{1,} \\w{1,} zur Autorenseite", "", x, perl = TRUE)
  x = gsub('Seitennavig[^"]*',"", x, perl=TRUE)
  x = gsub('Kartengeschichte[^"]*', "", x, perl = TRUE, ignore.case = TRUE)
  
  # Stern.de
  x = gsub('Fullscreen[^\n]*', "", x, perl = TRUE)

  # Tagesschau.de
  x = gsub("Hinweis: Falls die [^\\.]*", "", x, perl=TRUE)
  x = gsub("auswählen", "", x, perl = TRUE)
  x = gsub("Dieser Artikel wurde ausgedruckt unter der Adresse: [^\\s]*", "", x, perl = TRUE)
  x = gsub("faktenfinder.tagesschau.de", "", x, perl=TRUE)
  
  return(x)
}

# apply function to dataframe
btw$text_cleaned <- clean.text(btw$title_text)

btw$text_cleaned <- gsub("[[:punct:]]", " ", btw$text_cleaned)
btw$text_cleaned <- gsub("[[:cntrl:]]", " ", btw$text_cleaned)
btw$text_cleaned <- gsub("[[:digit:]]", " ", btw$text_cleaned)
btw$text_cleaned <- gsub("^[[:space:]]+", " ", btw$text_cleaned)
btw$text_cleaned <- gsub("[[:space:]]+$", " ", btw$text_cleaned)
btw$text_cleaned <- tolower(btw$text_cleaned)

## Remove stopwords
# 1
german_stopwords_full <- read.table("dict/german_stopwords_full.txt", stringsAsFactors = F)
german_stopwords_full <- german_stopwords_full$V1

# 2
mystopwords <- c("focus","online","spiegel","stern",".de","bild","bildplus","n-tv.de", "zeit", "ersten","ard", "tagesschau","müssen","sagen","faktenfinder", "zeitmagazin","seitenanfang","ja","mal","heute","ich","sie","passwort","kommentar","wurde","ihr","der","im","artikel","mehr","ihren","foto","e","seien","comment","ticker","live","laif","uhr","videolänge","dass","mindestens","das","mail","die","schon","neuer abschnitt", "login", "loggen", "inaktiv","nwmi","wäre","viele","nwnoa","morgenkolumne","beim","dpa","video","quelle","afp","witters","fotogalerie","wurden","worden","wegen","sagt","immer","gibt","geht","spon","registrierter","als","spiegel","vielen","in","es","bitte","dank","unserer","nutzer","sei","beitrag","user","seit","zeichen","tba","datenschutzerklärung","premium","nutzungsbedingungen","nutzungsrechte","pflichtfelder","registrierung","anzeige","großbuchstaben","sonderzeichen","html","seitennavigation","fullscreen","statista","club","sagte","borenda","spreepicture","shopübersicht","herr","imago","dobovisek","barenberg","heinlein","armbrüster","kaess","tsereteli","konietzny","klenkes","hauptstadtstudio","newsletter","premiumbereich","nachrichtenpodcast","karrierespiegel","picture alliance","appnutzer","civey","abo")

stopwords <- c(german_stopwords_full, mystopwords)
stopwords <- unique(stopwords)

# 3
btw$text_cleaned<- tm::removeWords(btw$text_cleaned, stopwords)
```

### Stemming
```{r }
stem_text<- function(text, language = "porter", mc.cores = 1) {
  # stem each word in a block of text
  stem_string <- function(str, language) {
    str <- strsplit(x = str, split = "\\s")
    str <- SnowballC::wordStem(unlist(str), language = language)
    str <- paste(str, collapse = " ")
    return(str)
  }
   
  # stem each text block in turn
  x <- parallel::mclapply(X = text, FUN = stem_string, language, mc.cores = mc.cores)
   
  # return stemed text blocks
  return(unlist(x))
}

btw$text_cleaned1 <- stem_text(btw$text_cleaned)
```

```{r}
btw %>%
  sample_n(1) %>%
  select(title_text, text_cleaned1) %>%
  htmlTable::htmlTable()
```

```{r}
save(btw, df, file="../output/data_step2.Rda")
```




