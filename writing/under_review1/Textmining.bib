
@article{armstrong_competition_2006,
	title = {Competition in two-sided markets},
	volume = {37},
	issn = {1756-2171},
	url = {http://dx.doi.org/10.1111/j.1756-2171.2006.tb00037.x},
	doi = {10.1111/j.1756-2171.2006.tb00037.x},
	number = {3},
	journal = {The RAND Journal of Economics},
	author = {Armstrong, Mark},
	year = {2006},
	pages = {668--691}
}

@article{rochet_platform_2003,
	title = {Platform {Competition} in {Two}-{Sided} {Markets}},
	volume = {1},
	issn = {1542-4774},
	url = {http://onlinelibrary.wiley.com/doi/10.1162/154247603322493212/abstract},
	doi = {10.1162/154247603322493212},
	abstract = {Many if not most markets with network externalities are two-sided. To succeed, platforms in industries such as software, portals and media, payment systems and the Internet, must “get both sides of the market on board.” Accordingly, platforms devote much attention to their business model, that is, to how they court each side while making money overall. This paper builds a model of platform competition with two-sided markets. It unveils the determinants of price allocation and end-user surplus for different governance structures (profit-maximizing platforms and not-for-profit joint undertakings), and compares the outcomes with those under an integrated monopolist and a Ramsey planner. (JEL: L5, L82, L86, L96)},
	language = {en},
	number = {4},
	urldate = {2016-08-04},
	journal = {Journal of the European Economic Association},
	author = {Rochet, Jean-Charles and Tirole, Jean},
	month = jun,
	year = {2003},
	pages = {990--1029},
	file = {Snapshot:/Users/Franzi/Zotero/storage/WJI5PS42/abstract.html:text/html}
}

@article{caillaud_chicken_2003,
	title = {Chicken \& {Egg}: {Competition} among {Intermediation} {Service} {Providers}},
	volume = {34},
	issn = {0741-6261},
	shorttitle = {Chicken \& {Egg}},
	url = {http://econpapers.repec.org/article/rjerandje/v_3a34_3ay_3a2003_3ai_3a2_3ap_3a309-28.htm},
	abstract = {We analyze a model of imperfect price competition between intermediation service providers. We insist on features that are relevant for informational intermediation via the Internet: the presence of indirect network externalities, the possibility of using the nonexclusive services of several intermediaries, and the widespread practice of price discrimination based on users' identity and on usage. Efficient market structures emerge in equilibrium, as well as some specific form of inefficient structures. Intermediaries have incentives to propose non-exclusive services, as this moderates competition and allows them to exert market power. We analyze in detail the pricing and business strategies followed by intermediation services providers. Copyright 2003 by the RAND Corporation.},
	number = {2},
	urldate = {2016-10-18},
	journal = {RAND Journal of Economics},
	author = {Caillaud, Bernard and Jullien, Bruno},
	year = {2003},
	pages = {309--28},
	file = {RePEc Snapshot:/Users/Franzi/Zotero/storage/GD5R58G2/v_3a34_3ay_3a2003_3ai_3a2_3ap_3a309-28.html:text/html}
}

@book{box_time_2008,
	address = {Hoboken, NJ},
	edition = {4},
	series = {Wiley series in probability and statistics},
	title = {Time series analysis: forecasting and control},
	isbn = {978-0-470-27284-8},
	shorttitle = {Time series analysis},
	publisher = {Wiley},
	author = {Box, George E. P. and Jenkins, Gwilym M. and Reinsel, Gregory C.},
	year = {2008},
	keywords = {*Time-series analysis / Prediction theory / Transfer functions / Feedback control systems -- Mathematical models, *Zeitreihenanalyse / Kontrolltheorie / Rückkopplung / Mathematisches Modell}
}

@article{evans_economics_2008,
	title = {The {Economics} of the {Online} {Advertising} {Industry}},
	volume = {7},
	url = {https://ideas.repec.org/a/bpj/rneart/v7y2008i3n2.html},
	abstract = {Internet-based technologies are revolutionizing the stodgy \$625 billion global advertising industry. There are a number of public policy issues to consider. Will a single ad platform emerge or will several remain viable? What are the consequences of alternative market structures for a web economy that is increasingly based on selling eyeballs to advertisers? This article describes the online advertising industry. The industry is populated by a number of multi-sided platforms that facilitate connecting advertisers to viewers. Search-based advertising platforms, the most developed of these, have interesting economic features that result from the combination of keyword bidding by advertisers and single-homing.},
	number = {3},
	urldate = {2016-08-11},
	journal = {Review of Network Economics},
	author = {Evans, David S.},
	year = {2008},
	pages = {1--33},
	file = {RePEc Snapshot:/Users/Franzi/Zotero/storage/Z6B7DBRS/v7y2008i3n2.html:text/html}
}

@inproceedings{korencic_getting_2015,
	address = {New York, NY, USA},
	series = {{TM} '15},
	title = {Getting the {Agenda} {Right}: {Measuring} {Media} {Agenda} {Using} {Topic} {Models}},
	isbn = {978-1-4503-3784-7},
	shorttitle = {Getting the {Agenda} {Right}},
	url = {http://doi.acm.org/10.1145/2809936.2809942},
	doi = {10.1145/2809936.2809942},
	abstract = {Agenda setting is the theory of how issue salience is transferred from the media to media audience. An agenda-setting study requires one to define a set of issues and to measure their salience. We propose a semi-supervised approach based on topic modeling for exploring a news corpus and measuring the media agenda by tagging news articles with issues. The approach relies on an off-the-shelf Latent Dirichlet Allocation topic model, manual labeling of topics, and topic model customization. In preliminary evaluation, the tagger achieves a micro F1-score of 0.85 and outperforms the supervised baselines, suggesting that it could be successfully used for agenda-setting studies.},
	booktitle = {Proceedings of the 2015 {Workshop} on {Topic} {Models}: {Post}-{Processing} and {Applications}},
	publisher = {ACM},
	author = {Korenčić, Damir and Ristov, Strahil and Šnajder, Jan},
	year = {2015},
	keywords = {topic modeling, agenda measuring, agenda setting, document tagging, multilabel classification, news media analysis},
	pages = {61--66}
}

@article{tetlock_giving_2007,
	title = {Giving {Content} to {Investor} {Sentiment}: {The} {Role} of {Media} in the {Stock} {Market}},
	volume = {62},
	issn = {1540-6261},
	shorttitle = {Giving {Content} to {Investor} {Sentiment}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2007.01232.x/abstract},
	doi = {10.1111/j.1540-6261.2007.01232.x},
	abstract = {I quantitatively measure the interactions between the media and the stock market using daily content from a popular Wall Street Journal column. I find that high media pessimism predicts downward pressure on market prices followed by a reversion to fundamentals, and unusually high or low pessimism predicts high market trading volume. These and similar results are consistent with theoretical models of noise and liquidity traders, and are inconsistent with theories of media content as a proxy for new information about fundamental asset values, as a proxy for market volatility, or as a sideshow with no relationship to asset markets.},
	language = {en},
	number = {3},
	journal = {The Journal of Finance},
	author = {Tetlock, Paul C.},
	month = jun,
	year = {2007},
	pages = {1139--1168},
	file = {Snapshot:/Users/Franzi/Zotero/storage/R4FBIJ83/abstract.html:text/html}
}

@article{baker_measuring_2016,
	title = {Measuring {Economic} {Policy} {Uncertainty}},
	url = {http://www.nber.org/papers/w21633},
	abstract = {We develop a new index of economic policy uncertainty (EPU) based on newspaper coverage frequency. Several types of evidence – including human readings of 12,000 newspaper articles – indicate that our index proxies for movements in policy-related economic uncertainty. Our US index spikes near tight presidential elections, Gulf Wars I and II, the 9/11 attacks, the failure of Lehman Brothers, the 2011 debt-ceiling dispute and other major battles over fiscal policy. Using firm-level data, we find that policy uncertainty raises stock price volatility and reduces investment and employment in policy-sensitive sectors like defense, healthcare, and infrastructure construction. At the macro level, policy uncertainty innovations foreshadow declines in investment, output, and employment in the United States and, in a panel VAR setting, for 12 major economies. Extending our US index back to 1900, EPU rose dramatically in the 1930s (from late 1931) and has drifted upwards since the 1960s.},
	number = {4},
	journal = {Quarterly Journal of Economics},
	author = {Baker, Scott R. and Bloom, Nicholas and Davis, Steven J.},
	year = {2016},
	doi = {10.3386/w21633},
	pages = {1593--1636}
}

@techreport{gentzkow_text_2017,
	type = {Working {Paper}},
	title = {Text as {Data}},
	url = {http://www.nber.org/papers/w23276},
	abstract = {An ever increasing share of human interaction, communication, and culture is recorded as digital text. We provide an introduction to the use of text as an input to economic research. We discuss the features that make text different from other forms of data, offer a practical overview of relevant statistical methods, and survey a variety of applications.},
	number = {23276},
	institution = {National Bureau of Economic Research},
	author = {Gentzkow, Matthew and Kelly, Bryan T. and Taddy, Matt},
	month = mar,
	year = {2017},
	doi = {10.3386/w23276}
}

@article{blei_probabilistic_2012,
	title = {Probabilistic {Topic} {Models}},
	volume = {55},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/2133806.2133826},
	doi = {10.1145/2133806.2133826},
	abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
	number = {4},
	journal = {Commun. ACM},
	author = {Blei, David M.},
	month = apr,
	year = {2012},
	pages = {77--84},
	file = {ACM Full Text PDF:/Users/Franzi/Zotero/storage/GWUDPWQT/Blei - 2012 - Probabilistic Topic Models.pdf:application/pdf}
}

@article{blei_latent_2003,
	title = {Latent dirichlet allocation},
	volume = {3},
	journal = {Journal of machine Learning research},
	author = {Blei, David M. and Ng, Andrew Y and Jordan, Michael I},
	month = jan,
	year = {2003},
	pages = {993--1022}
}

@incollection{mcauliffe_supervised_2008,
	title = {Supervised {Topic} {Models}},
	url = {http://papers.nips.cc/paper/3328-supervised-topic-models.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 20},
	publisher = {Curran Associates, Inc.},
	author = {Mcauliffe, Jon D. and Blei, David M.},
	editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
	year = {2008},
	pages = {121--128},
	file = {NIPS Full Text PDF:/Users/Franzi/Zotero/storage/AKKVXDQ9/Mcauliffe und Blei - 2008 - Supervised Topic Models.pdf:application/pdf;NIPS Snapshort:/Users/Franzi/Zotero/storage/KQV8JHW8/3328-supervised-topic-models.html:text/html}
}

@article{grimmer_bayesian_2010,
	title = {A {Bayesian} {Hierarchical} {Topic} {Model} for {Political} {Texts}: {Measuring} {Expressed} {Agendas} in {Senate} {Press} {Releases}},
	volume = {18},
	shorttitle = {A {Bayesian} {Hierarchical} {Topic} {Model} for {Political} {Texts}},
	url = {https://papers.ssrn.com/abstract=1541022},
	abstract = {Political scientists lack methods to efficiently measure the priorities political actors emphasize in statements. To address this limitation, I introduce a statistical model that attends to the structure of political rhetoric when measuring expressed priorities: statements are naturally organized by author. The expressed agenda model exploits this structure to simultaneously estimate the topics in the texts, as well as the attention political actors allocate to the estimated topics. I apply the method to a collection of over 24,000 press releases from senators from 2007, which I demonstrate is an ideal medium to measure how senators explain their work in Washington to constituents. A set of examples validates the estimated priorities and demonstrates their usefulness for testing theories of how members of Congress communicate with constituents. The statistical model and its extensions will be made available in a forthcoming free software package for the R computing language.},
	number = {1},
	urldate = {2017-10-07},
	journal = {Political Analysis},
	author = {Grimmer, Justin},
	year = {2010},
	keywords = {SSRN, A Bayesian Hierarchical Topic Model for Political Texts: Measuring Expressed Agendas in Senate Press Releases, Justin  Grimmer},
	pages = {1--35},
	file = {Snapshot:/Users/Franzi/Zotero/storage/3QRF5PHE/papers.html:text/html}
}

@article{pritchard_inference_2000,
	title = {Inference of population structure using multilocus genotype data},
	volume = {155},
	issn = {0016-6731},
	abstract = {We describe a model-based clustering method for using multilocus genotype data to infer population structure and assign individuals to populations. We assume a model in which there are K populations (where K may be unknown), each of which is characterized by a set of allele frequencies at each locus. Individuals in the sample are assigned (probabilistically) to populations, or jointly to two or more populations if their genotypes indicate that they are admixed. Our model does not assume a particular mutation process, and it can be applied to most of the commonly used genetic markers, provided that they are not closely linked. Applications of our method include demonstrating the presence of population structure, assigning individuals to populations, studying hybrid zones, and identifying migrants and admixed individuals. We show that the method can produce highly accurate assignments using modest numbers of loci-e.g. , seven microsatellite loci in an example using genotype data from an endangered bird species. The software used for this article is available from http://www.stats.ox.ac.uk/ approximately pritch/home. html.},
	language = {eng},
	number = {2},
	journal = {Genetics},
	author = {Pritchard, J. K. and Stephens, M. and Donnelly, P.},
	month = jun,
	year = {2000},
	pmid = {10835412},
	pmcid = {PMC1461096},
	keywords = {Algorithms, Cluster Analysis, Genetics, Population, Genotype, Humans, Models, Genetic},
	pages = {945--959}
}

@inproceedings{taddy_estimation_2012,
	title = {On estimation and selection for topic models},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Taddy, Matt},
	year = {2012}
}

@article{airoldi_reconceptualizing_2010,
	title = {Reconceptualizing the classification of {PNAS} articles},
	volume = {107},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/107/49/20899},
	doi = {10.1073/pnas.1013452107},
	abstract = {PNAS article classification is rooted in long-standing disciplinary divisions that do not necessarily reflect the structure of modern scientific research. We reevaluate that structure using latent pattern models from statistical machine learning, also known as mixed-membership models, that identify semantic structure in co-occurrence of words in the abstracts and references. Our findings suggest that the latent dimensionality of patterns underlying PNAS research articles in the Biological Sciences is only slightly larger than the number of categories currently in use, but it differs substantially in the content of the categories. Further, the number of articles that are listed under multiple categories is only a small fraction of what it should be. These findings together with the sensitivity analyses suggest ways to reconceptualize the organization of papers published in PNAS.},
	language = {en},
	number = {49},
	urldate = {2017-10-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Airoldi, Edoardo M. and Erosheva, Elena A. and Fienberg, Stephen E. and Joutard, Cyrille and Love, Tanzy and Shringarpure, Suyash},
	month = jul,
	year = {2010},
	pmid = {21078953},
	keywords = {text analysis, hierarchical modeling, Monte Carlo Markov chain, variational inference, Dirichlet process},
	pages = {20899--20904},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/3HPSSBQ2/Airoldi et al. - 2010 - Reconceptualizing the classification of PNAS artic.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/3SR8QVPZ/20899.html:text/html}
}

@article{teh_hierarchical_2006,
	title = {Hierarchical {Dirichlet} {Processes}},
	volume = {101},
	issn = {0162-1459},
	url = {http://amstat.tandfonline.com/doi/abs/10.1198/016214506000000302},
	doi = {10.1198/016214506000000302},
	abstract = {We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the “Chinese restaurant franchise.” We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling.},
	number = {476},
	journal = {Journal of the American Statistical Association},
	author = {Teh, Yee Whye and Jordan, Michael I and Beal, Matthew J and Blei, David M},
	month = dec,
	year = {2006},
	pages = {1566--1581},
	file = {Snapshot:/Users/Franzi/Zotero/storage/67GVBDZ8/016214506000000302.html:text/html}
}

@inproceedings{blei_dynamic_2006,
	address = {New York, NY, USA},
	series = {{ICML} '06},
	title = {Dynamic {Topic} {Models}},
	isbn = {978-1-59593-383-6},
	url = {http://doi.acm.org/10.1145/1143844.1143859},
	doi = {10.1145/1143844.1143859},
	abstract = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR'ed archives of the journal Science from 1880 through 2000.},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Blei, David M. and Lafferty, John D.},
	year = {2006},
	pages = {113--120}
}

@article{airoldi_improving_2016,
	title = {Improving and {Evaluating} {Topic} {Models} and {Other} {Models} of {Text}},
	volume = {111},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1080/01621459.2015.1051182},
	doi = {10.1080/01621459.2015.1051182},
	abstract = {An ongoing challenge in the analysis of document collections is how to summarize content in terms of a set of inferred themes that can be interpreted substantively in terms of topics. The current practice of parameterizing the themes in terms of most frequent words limits interpretability by ignoring the differential use of words across topics. Here, we show that words that are both frequent and exclusive to a theme are more effective at characterizing topical content, and we propose a regularization scheme that leads to better estimates of these quantities. We consider a supervised setting where professional editors have annotated documents to topic categories, organized into a tree, in which leaf-nodes correspond to more specific topics. Each document is annotated to multiple categories, at different levels of the tree. We introduce a hierarchical Poisson convolution model to analyze these annotated documents. A parallelized Hamiltonian Monte Carlo sampler allows the inference to scale to millions of documents. The model leverages the structure among categories defined by professional editors to infer a clear semantic description for each topic in terms of words that are both frequent and exclusive. In this supervised setting, we validate the efficacy of word frequency and exclusivity at characterizing topical content on two very large collections of documents, from Reuters and the New York Times. In an unsupervised setting, we then consider a simplified version of the model that shares the same regularization scheme with the previous model. We carry out a large randomized experiment on Amazon Mechanical Turk to demonstrate that topic summaries based on frequency and exclusivity, estimated using the proposed regularization scheme, are more interpretable than currently established frequency-based summaries, and that the proposed model produces more efficient estimates of exclusivity than the currently established models.},
	number = {516},
	journal = {Journal of the American Statistical Association},
	author = {Airoldi, Edoardo M. and Bischof, Jonathan M.},
	month = oct,
	year = {2016},
	keywords = {text analysis, Categorical data, Hamiltonian Monte Carlo, High-dimensional data, Parallel inference},
	pages = {1381--1403},
	file = {Snapshot:/Users/Franzi/Zotero/storage/HWNC8RRH/01621459.2015.html:text/html}
}

@article{grimmer_text_2013,
	title = {Text as {Data}: {The} {Promise} and {Pitfalls} of {Automatic} {Content} {Analysis} {Methods} for {Political} {Texts}},
	volume = {21},
	shorttitle = {Text as {Data}},
	journal = {Political Analysis},
	author = {Grimmer, Justin and Stewart, Brandon},
	year = {2013},
	pages = {267--297},
	file = {Text as Data\: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts | Brandon Stewart:/Users/Franzi/Zotero/storage/FF9QRDJ4/text-data-promise-and-pitfalls-automatic-content-analysis-methods-political.html:text/html}
}

@inproceedings{roberts_structural_2013,
	title = {The {Structural} {Topic} {Model} and {Applied} {Social} {Science}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} {Workshop} on {Topic} {Models}: {Computation}, {Application}, and {Evaluation}.},
	author = {Roberts, Margaret and Stewart, Brandon and Tingley, Dustin and Airoldi, Edoardo},
	year = {2013},
	file = {The Structural Topic Model and Applied Social Science | Brandon Stewart:/Users/Franzi/Zotero/storage/69G2KU2F/structural-topic-model-and-applied-social-science.html:text/html}
}

@incollection{socher_bayesian_2009,
	title = {A {Bayesian} {Analysis} of {Dynamics} in {Free} {Recall}},
	url = {http://papers.nips.cc/paper/3720-a-bayesian-analysis-of-dynamics-in-free-recall.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 22},
	publisher = {Curran Associates, Inc.},
	author = {Socher, Richard and Gershman, Samuel and Sederberg, Per and Norman, Kenneth and Perotte, Adler J. and Blei, David M.},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
	year = {2009},
	pages = {1714--1722},
	file = {NIPS Full Text PDF:/Users/Franzi/Zotero/storage/JQWKTPGH/Socher et al. - 2009 - A Bayesian Analysis of Dynamics in Free Recall.pdf:application/pdf;NIPS Snapshort:/Users/Franzi/Zotero/storage/ZFW3QA7C/3720-a-bayesian-analysis-of-dynamics-in-free-recall.html:text/html}
}

@article{quinn_how_2010,
	title = {How to {Analyze} {Political} {Attention} with {Minimal} {Assumptions} and {Costs}},
	volume = {54},
	issn = {1540-5907},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1540-5907.2009.00427.x/abstract},
	doi = {10.1111/j.1540-5907.2009.00427.x},
	abstract = {Previous methods of analyzing the substance of political attention have had to make several restrictive assumptions or been prohibitively costly when applied to large-scale political texts. Here, we describe a topic model for legislative speech, a statistical learning model that uses word choices to infer topical categories covered in a set of speeches and to identify the topic of specific speeches. Our method estimates, rather than assumes, the substance of topics, the keywords that identify topics, and the hierarchical nesting of topics. We use the topic model to examine the agenda in the U.S. Senate from 1997 to 2004. Using a new database of over 118,000 speeches (70,000,000 words) from the Congressional Record, our model reveals speech topic categories that are both distinctive and meaningfully interrelated and a richer view of democratic agenda dynamics than had previously been possible.},
	language = {en},
	number = {1},
	journal = {American Journal of Political Science},
	author = {Quinn, Kevin M. and Monroe, Burt L. and Colaresi, Michael and Crespin, Michael H. and Radev, Dragomir R.},
	month = jan,
	year = {2010},
	pages = {209--228},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/59JK6JEX/Quinn et al. - 2010 - How to Analyze Political Attention with Minimal As.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/QSVKCVD5/abstract.html:text/html}
}

@article{roberts_model_2016,
	title = {A {Model} of {Text} for {Experimentation} in the {Social} {Sciences}},
	volume = {111},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1080/01621459.2016.1141684},
	doi = {10.1080/01621459.2016.1141684},
	abstract = {Statistical models of text have become increasingly popular in statistics and computer science as a method of exploring large document collections. Social scientists often want to move beyond exploration, to measurement and experimentation, and make inference about social and political processes that drive discourse and content. In this article, we develop a model of text data that supports this type of substantive research. Our approach is to posit a hierarchical mixed membership model for analyzing topical content of documents, in which mixing weights are parameterized by observed covariates. In this model, topical prevalence and topical content are specified as a simple generalized linear model on an arbitrary number of document-level covariates, such as news source and time of release, enabling researchers to introduce elements of the experimental design that informed document collection into the model, within a generally applicable framework. We demonstrate the proposed methodology by analyzing a collection of news reports about China, where we allow the prevalence of topics to evolve over time and vary across newswire services. Our methods quantify the effect of news wire source on both the frequency and nature of topic coverage. Supplementary materials for this article are available online.},
	number = {515},
	journal = {Journal of the American Statistical Association},
	author = {Roberts, Margaret E. and Stewart, Brandon M. and Airoldi, Edoardo M.},
	month = jul,
	year = {2016},
	keywords = {text analysis, Causal inference, Experimentation, High-dimensional inference, Social sciences, Variational approximation},
	pages = {988--1003},
	file = {Snapshot:/Users/Franzi/Zotero/storage/ZMEDWA9E/01621459.2016.html:text/html}
}

@book{airoldi_handbook_2014,
	title = {Handbook of mixed membership models and their applications},
	isbn = {978-1-4665-0408-0},
	url = {http://cds.cern.ch/record/1974849},
	abstract = {In response to scientific needs for more diverse and structured explanations of statistical data, researchers have discovered how to model individual data points as belonging to multiple groups. Handbook of Mixed Membership Models and Their Applications shows you how to use these flexible modeling tools to uncover hidden patterns in modern high-dimensional multivariate data. It explores the use of the models in various application settings, including survey data, population genetics, text analysis, image processing and annotation, and molecular biology.Through examples using real data sets, yo},
	urldate = {2017-10-12},
	publisher = {Taylor and Francis},
	author = {Airoldi, Edoardo M. and Erosheva, Elena A. and Fienberg, Stephen E. and Blei, David},
	year = {2014},
	file = {Snapshot:/Users/Franzi/Zotero/storage/P7XE3MF4/1974849.html:text/html}
}

@inproceedings{mishler_using_2015,
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Using {Structural} {Topic} {Modeling} to {Detect} {Events} and {Cluster} {Twitter} {Users} in the {Ukrainian} {Crisis}},
	isbn = {978-3-319-21379-8 978-3-319-21380-4},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-21380-4_108},
	doi = {10.1007/978-3-319-21380-4_108},
	abstract = {Structural topic modeling (STM) is a recently introduced technique to model how the content of a collection of documents changes as a function of variables such as author identity or time of writing. We present two proof-of-concept applications of STM using Russian social media data. In our first study, we model how topics change over time, showing that STM can be used to detect significant events such as the downing of Malaysia Air Flight 17. In our second study, we model how topical content varies across a set of authors, showing that STM can be used to cluster Twitter users who are sympathetic to Ukraine versus Russia as well as to cluster accounts that are suspected to belong to the same individual (so-called “sockpuppets”). Structural topic modeling shows promise as a tool for analyzing social media data, a domain that has been largely ignored in the topic modeling literature.},
	language = {en},
	urldate = {2017-10-12},
	booktitle = {{HCI} {International} 2015 - {Posters}’ {Extended} {Abstracts}},
	publisher = {Springer, Cham},
	author = {Mishler, Alan and Crabb, Erin Smith and Paletz, Susannah and Hefright, Brook and Golonka, Ewa},
	month = aug,
	year = {2015},
	pages = {639--644},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/ACUCPA8U/Mishler et al. - 2015 - Using Structural Topic Modeling to Detect Events a.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/NG9CQ4T9/978-3-319-21380-4_108.html:text/html}
}

@article{griffiths_finding_2004,
	title = {Finding scientific topics},
	volume = {101},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/101/suppl_1/5228},
	doi = {10.1073/pnas.0307752101},
	abstract = {A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. \& Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying “hot topics” by examining temporal dynamics and tagging abstracts to illustrate semantic content.},
	language = {en},
	number = {suppl 1},
	urldate = {2017-10-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Griffiths, Thomas L. and Steyvers, Mark},
	month = jun,
	year = {2004},
	pmid = {14872004},
	pages = {5228--5235},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/9P87NGIJ/Griffiths und Steyvers - 2004 - Finding scientific topics.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/TX4TP3NE/5228.html:text/html}
}

@article{erosheva_mixed-membership_2004,
	title = {Mixed-membership models of scientific publications},
	volume = {101},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/101/suppl_1/5220},
	doi = {10.1073/pnas.0307760101},
	abstract = {PNAS is one of world's most cited multidisciplinary scientific journals. The PNAS official classification structure of subjects is reflected in topic labels submitted by the authors of articles, largely related to traditionally established disciplines. These include broad field classifications into physical sciences, biological sciences, social sciences, and further subtopic classifications within the fields. Focusing on biological sciences, we explore an internal soft-classification structure of articles based only on semantic decompositions of abstracts and bibliographies and compare it with the formal discipline classifications. Our model assumes that there is a fixed number of internal categories, each characterized by multinomial distributions over words (in abstracts) and references (in bibliographies). Soft classification for each article is based on proportions of the article's content coming from each category. We discuss the appropriateness of the model for the PNAS database as well as other features of the data relevant to soft classification.},
	language = {en},
	number = {suppl 1},
	urldate = {2017-10-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Erosheva, Elena and Fienberg, Stephen and Lafferty, John},
	month = jun,
	year = {2004},
	pmid = {15020766},
	pages = {5220--5227},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/QX4H27E9/Erosheva et al. - 2004 - Mixed-membership models of scientific publications.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/NVF8AVJ7/5220.html:text/html}
}

@article{genkin_large-scale_2007,
	title = {Large-{Scale} {Bayesian} {Logistic} {Regression} for {Text} {Categorization}},
	volume = {49},
	issn = {0040-1706},
	url = {http://dx.doi.org/10.1198/004017007000000245},
	doi = {10.1198/004017007000000245},
	abstract = {Logistic regression analysis of high-dimensional data, such as natural language text, poses computational and statistical challenges. Maximum likelihood estimation often fails in these applications. We present a simple Bayesian logistic regression approach that uses a Laplace prior to avoid overfitting and produces sparse predictive models for text data. We apply this approach to a range of document classification problems and show that it produces compact predictive models at least as effective as those produced by support vector machine classifiers or ridge logistic regression combined with feature selection. We describe our model fitting algorithm, our open source implementations (BBR and BMR), and experimental results.},
	number = {3},
	journal = {Technometrics},
	author = {Genkin, Alexander and Lewis, David D. and Madigan, David},
	month = aug,
	year = {2007},
	keywords = {Information retrieval, Lasso, Penalization, Ridge regression, Support vector classifier, Variable selection},
	pages = {291--304},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/9JI658VW/Genkin et al. - 2007 - Large-Scale Bayesian Logistic Regression for Text .pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/9XV4KG9V/004017007000000245.html:text/html}
}

@phdthesis{ponweiser_latent_2012,
	type = {Theses / {Institute} for {Statistics} and {Mathematics}},
	title = {Latent {Dirichlet} {Allocation} in {R}},
	url = {http://epub.wu.ac.at/3558/},
	abstract = {Topic models are a new research field within the computer sciences information retrieval and text mining. They are generative probabilistic models of text corpora inferred by machine learning and they can be used for retrieval and text mining tasks. The most prominent topic model is latent Dirichlet allocation (LDA), which was introduced in 2003 by Blei et al. and has since then sparked off the development of other topic models for domain-specific purposes.
This thesis focuses on LDA's practical application. Its main goal is the replication of the data analyses from the 2004 LDA paper ``Finding scientific topics'' by Thomas Griffiths and Mark Steyvers within the framework of the R statistical programming language and the R{\textasciitilde}package topicmodels by Bettina Grün and Kurt Hornik. The complete process, including extraction of a text corpus from the PNAS journal's website, data preprocessing, transformation into a document-term matrix, model selection, model estimation, as well as presentation of the results, is fully documented and commented. The outcome closely matches the analyses of the original paper, therefore the research by Griffiths/Steyvers can be reproduced. Furthermore, this thesis proves the suitability of the R environment for text mining with LDA. (author's abstract)},
	language = {en},
	urldate = {2017-10-13},
	school = {WU Vienna University of Economics and Business},
	author = {Ponweiser, Martin},
	month = may,
	year = {2012},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/AKASJPTP/Ponweiser - 2012 - Latent Dirichlet Allocation in R.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/E5J47GZP/3558.html:text/html}
}

@article{silge_tidytext:_2016,
	title = {tidytext: {Text} {Mining} and {Analysis} {Using} {Tidy} {Data} {Principles} in {R}},
	shorttitle = {tidytext},
	doi = {10.21105/joss.00037},
	journal = {The Journal of Open Source Software},
	author = {Silge, Julia and Robinson, David},
	month = jul,
	year = {2016},
	file = {Snapshot:/Users/Franzi/Zotero/storage/KBIXHDAH/305219559_tidytext_Text_Mining_and_Analysis_Using_Tidy_Data_Principles_in_R.pdf:application/pdf}
}

@inproceedings{phan_learning_2008,
	address = {Beijing, China},
	title = {Learning to {Classify} {Short} and {Sparse} {Text} \& {Web} with {Hidden} {Topics} from {Large}-{Scale} {Data} {Collections}},
	doi = {10.1145/1367497.1367510},
	abstract = {This paper presents a general framework for building classi- fiers that deal with short and sparse text \& Web segments by making the most of hidden topics discovered from large- scale data collections. The main motivation of this work is that many classification tasks working with short segments of text \& Web, such as search snippets, forum \& chat mes- sages, blog \& news feeds, product reviews, and book \& movie summaries, fail to achieve high accuracy due to the data sparseness. We, therefore, come up with an idea of gaining external knowledge to make the data more related as well as expand the coverage of classifiers to handle future data bet- ter. The underlying idea of the framework is that for each classification task, we collect a large-scale external data col- lection called "universal dataset", and then build a classifier on both a (small) set of labeled training data and a rich set of hidden topics discovered from that data collection. The framework is general enough to be applied to different data domains and genres ranging from Web search results to medical text. We did a careful evaluation on several hundred megabytes of Wikipedia (30M words) and MEDLINE (18M words) with two tasks: "Web search domain disambiguation" and "disease categorization for medical text", and achieved significant quality enhancement.},
	booktitle = {Proceedings of the 17th {International} {World} {Wide} {Web} {Conference} ({WWW} 2008)},
	author = {Phan, Xuan-Hieu and Nguyen, Le and Horiguchi, Susumu},
	month = jan,
	year = {2008},
	pages = {91--100},
	file = {Snapshot:/Users/Franzi/Zotero/storage/SP33XWPF/221023426_Learning_to_Classify_Short_and_Sparse_Text_Web_with_Hidden_Topics_from_Large-Scale_Dat.pdf:application/pdf}
}

@inproceedings{rabinovich_inverse_2014,
	address = {Beijing, China},
	series = {{ICML}'14},
	title = {The {Inverse} {Regression} {Topic} {Model}},
	url = {http://dl.acm.org/citation.cfm?id=3044805.3044829},
	abstract = {Taddy (2013) proposed multinomial inverse regression (MNIR) as a new model of annotated text based on the influence of metadata and response variables on the distribution of words in a document. While effective, MNIR has no way to exploit structure in the corpus to improve its predictions or facilitate exploratory data analysis. On the other hand, traditional probabilistic topic models (like latent Dirichlet allocation) capture natural heterogeneity in a collection but do not account for external variables. In this paper, we introduce the inverse regression topic model (IRTM), a mixed-membership extension of MNIR that combines the strengths of both methodologies. We present two inference algorithms for the IRTM: an efficient batch estimation algorithm and an online variant, which is suitable for large corpora. We apply these methods to a corpus of 73K Congressional press releases and another of 150K Yelp reviews, demonstrating that the IRTM outperforms both MNIR and supervised topic models on the prediction task. Further, we give examples showing that the IRTM enables systematic discovery of in-topic lexical variation, which is not possible with previous supervised topic models.},
	booktitle = {Proceedings of the 31st {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 32},
	publisher = {JMLR.org},
	author = {Rabinovich, Maxim and Blei, David M.},
	year = {2014},
	pages = {I--199--I--207}
}

@article{schiller_development_2016,
	title = {Development of the {Social} {Network} {Usage} in {Germany} since 2012},
	journal = {Working Paper TU Darmstadt},
	author = {Schiller, Benjamin and Heimbach, Irina and Strufe, Thorsten and Hinz, Oliver},
	year = {2016}
}

@article{bholat_text_2015,
	title = {Text {Mining} for {Central} {Banks}},
	issn = {1556-5068},
	url = {http://www.academia.edu/13430482/Text_mining_for_central_banks},
	abstract = {Although often applied in other social sciences, text mining has been less frequently used in economics and in policy circles, particularly inside central banks. This Handbook is a brief introduction to the field, discussing how text mining is useful},
	urldate = {2017-11-06},
	journal = {SSRN Electronic Journal},
	author = {Bholat, David M. and Hansen, Stephen and Santos, Pedro M. and Schonhardt-Bailey, Cheryl},
	month = jun,
	year = {2015},
	file = {Snapshot:/Users/Franzi/Zotero/storage/J462FFQU/Text_mining_for_central_banks.html:text/html}
}

@inproceedings{minka_expectation-propagation_2002,
	address = {San Francisco, CA, USA},
	series = {{UAI}'02},
	title = {Expectation-propagation for the {Generative} {Aspect} {Model}},
	isbn = {978-1-55860-897-9},
	url = {http://dl.acm.org/citation.cfm?id=2073876.2073918},
	abstract = {The generative aspect model is an extension of the multinomial model for text that allows word probabilities to vary stochastically across documents. Previous results with aspect models have been promising, but hindered by the computational difficulty of carrying out inference and learning. This paper demonstrates that the simple variational methods of Blei et al. (2001) can lead to inaccurate inferences and biased learning for the generative aspect model. We develop an alternative approach that leads to higher accuracy at comparable cost. An extension of Expectation-Propagation is used for inference and then embedded in an EM algorithm for learning. Experimental results are presented for both synthetic and real data sets.},
	booktitle = {Proceedings of the {Eighteenth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Minka, Thomas and Lafferty, John},
	year = {2002},
	pages = {352--359}
}

@article{roberts_stm:_2016,
	title = {stm: {R} {Package} for {Structural} {Topic} {Models}},
	volume = {forthcoming},
	shorttitle = {stm},
	journal = {Journal of Statistical Software},
	author = {Roberts, Margaret and Stewart, Brandon and Tingley, Dustin},
	month = jan,
	year = {2016},
	file = {stm\: R Package for Structural Topic Models | Dustin Tingley:/Users/Franzi/Zotero/storage/FAWQJE6X/stm-r-package-structural-topic-models.html:text/html}
}

@incollection{roberts_navigating_2016,
	address = {New York},
	title = {Navigating the {Local} {Modes} of {Big} {Data}: {The} {Case} of {Topic} {Models}.},
	booktitle = {Computational {Social} {Science}: {Discovery} and {Prediction}},
	publisher = {Cambridge University Press},
	author = {Roberts, Margaret and Stewart, Brandon and Tingley, Dustin},
	year = {2016},
	file = {Navigating the Local Modes of Big Data\: The Case of Topic Models | Dustin Tingley:/Users/Franzi/Zotero/storage/TW4DVST8/navigating-local-modes-big-data-case-topic-models.html:text/html}
}

@techreport{grajzl_structural_2017,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {A {Structural} {Topic} {Model} of the {Features} and the {Cultural} {Origins} of {Bacon}'s {Ideas}},
	url = {https://papers.ssrn.com/abstract=2944816},
	abstract = {We use machine-learning methods to study the features and origins of the thought of Francis Bacon, a key figure in the development of a cultural paradigm that provided intellectual roots for modern economic development. We estimate a structural topic model, a state-of-the-art methodology for analysis of text corpora. The estimates uncover sixteen topics prominent in Bacon's opus. Two are central in the ideas usually associated with Bacon: inductive epistemology and fact-seeking. While Bacon's epistemology is strongly connected with his jurisprudence, fact-seeking is more isolated from Bacon's other intellectual pursuits. The utilitarian promise of science and the central organization of the scientific quest, embraced by Bacon's followers, were not emphasized by him, a finding suggesting that these aspects of the 'Baconian' culture owed little to Bacon's own contributions. Bacon's use of different topics varies notably with intended audience and chosen medium.},
	number = {ID 2944816},
	urldate = {2017-11-07},
	institution = {Social Science Research Network},
	author = {Grajzl, Peter and Murrell, Peter},
	month = oct,
	year = {2017},
	keywords = {Francis Bacon, culture, law, knowledge, natural philosophy, politics, religion},
	file = {Snapshot:/Users/Franzi/Zotero/storage/B862TXNA/papers.html:text/html}
}

@article{baturo_what_2017,
	title = {What {Drives} the {International} {Development} {Agenda}? {An} {NLP} {Analysis} of the {United} {Nations} {General} {Debate} 1970-2016},
	shorttitle = {What {Drives} the {International} {Development} {Agenda}?},
	url = {http://arxiv.org/abs/1708.05873},
	abstract = {There is surprisingly little known about agenda setting for international development in the United Nations (UN) despite it having a significant influence on the process and outcomes of development efforts. This paper addresses this shortcoming using a novel approach that applies natural language processing techniques to countries' annual statements in the UN General Debate. Every year UN member states deliver statements during the General Debate on their governments' perspective on major issues in world politics. These speeches provide invaluable information on state preferences on a wide range of issues, including international development, but have largely been overlooked in the study of global politics. This paper identifies the main international development topics that states raise in these speeches between 1970 and 2016, and examine the country-specific drivers of international development rhetoric.},
	journal = {arXiv:1708.05873 [cs]},
	author = {Baturo, Alexander and Dasandi, Niheer and Mikhaylov, Slava J.},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.05873},
	keywords = {Computer Science - Computation and Language, Computer Science - Social and Information Networks},
	file = {arXiv\:1708.05873 PDF:/Users/Franzi/Zotero/storage/FSRGQVHA/Baturo et al. - 2017 - What Drives the International Development Agenda .pdf:application/pdf;arXiv.org Snapshot:/Users/Franzi/Zotero/storage/H2JRUWQF/1708.html:text/html}
}

@incollection{hagen_data_2018,
	series = {Public {Administration} and {Information} {Technology}},
	title = {Data {Analytics} for {Policy} {Informatics}: {The} {Case} of {E}-{Petitioning}},
	isbn = {978-3-319-61761-9 978-3-319-61762-6},
	shorttitle = {Data {Analytics} for {Policy} {Informatics}},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-61762-6_9},
	abstract = {To contribute to the development of policy informatics, we discuss the benefits of analyzing electronic petitions (e-petitions), a form of citizen-government discourse with deep historic roots that has recently transitioned into a technologically-enabled and novel form of political communication. We begin by presenting a rationale for the analysis of e-petitions as a type of e-participation that can contribute to the development of public policy, provided that it is possible to analyze the large volumes of data produced in petitioning processes. From there we consider two data analytic strategies that offer promising approaches to the analysis of e-petitions and that lend themselves to the future creation of policy informatics tools. We discuss the application of topic modeling to the analysis of e-petition textual data to identify emergent topics of substantial concern to the public. We further propose the application of social network analysis to data related to the dynamics of petitioning processes, such as the social connections between petition initiators and signers, and tweets that solicit petition signatures in petitioning campaigns; both may be useful in revealing patterns of collective action. The paper concludes by reflecting on issues that should be brought to bear on the construction of policy informatics tools that make use of e-petitioning data.},
	language = {en},
	urldate = {2017-11-09},
	booktitle = {Policy {Analytics}, {Modelling}, and {Informatics}},
	publisher = {Springer, Cham},
	author = {Hagen, Loni and Harrison, Teresa M. and Dumas, Catherine L.},
	year = {2018},
	doi = {10.1007/978-3-319-61762-6_9},
	pages = {205--224},
	file = {Snapshot:/Users/Franzi/Zotero/storage/QBNNBEIJ/978-3-319-61762-6_9.html:text/html}
}

@article{haixia_extracting_2016,
	title = {Extracting {Topics} of {Computer} {Science} {Literature} with {LDA} {Model}, {Extracting} {Topics} of {Computer} {Science} {Literature} with {LDA} {Model}},
	volume = {32},
	issn = {2096-3467},
	url = {http://manu44.magtech.com.cn/Jwk_infotech_wk3/EN/abstract/abstract4288.shtml},
	doi = {10.11925/infotech.1003-3513.2016.11.03},
	language = {cn},
	number = {11},
	urldate = {2017-11-09},
	journal = {Data Analysis and Knowledge Discovery},
	author = {Haixia, Yang and Baojun, Gao and Hanlin, Sun and Haixia, Yang and Baojun, Gao and Hanlin, Sun},
	month = dec,
	year = {2016},
	pages = {20--26},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/PR58B4CK/Haixia et al. - 2016 - Extracting Topics of Computer Science Literature w.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/DDFB9U5P/abstract4288.html:text/html}
}

@article{das_trends_2017,
	title = {Trends in {Transportation} {Research}},
	volume = {2614},
	issn = {0361-1981},
	url = {http://trrjournalonline.trb.org/doi/abs/10.3141/2614-04},
	doi = {10.3141/2614-04},
	abstract = {Proceedings of journal and conference papers are good sources of big textual data to examine research trends in various branches of science. The contents, usually unstructured in nature, require fast machine-learning algorithms to be deciphered. Exploratory analysis through text mining usually provides the descriptive nature of the contents but lacks quantification of the topics and their correlations. Topic models are algorithms designed to discover the main theme or trend in massive collections of unstructured documents. Through the use of a structural topic model, an extension of latent Dirichlet allocation, this study introduced distinct topic models on the basis of the relative frequencies of the words used in the abstracts of 15,357 TRB compendium papers. With data from 7 years (2008 through 2014) of TRB annual meeting compendium papers, the 20 most dominant topics emerged from a bag of 4 million words. The findings of this study contributed to the understanding of topical trends in the complex and evolving field of transportation engineering research.},
	journal = {Transportation Research Record: Journal of the Transportation Research Board},
	author = {Das, Subasish and Dixon, Karen and Sun, Xiaoduan and Dutta, Anandi and Zupancich, Michelle},
	month = jan,
	year = {2017},
	pages = {27--38},
	file = {Snapshot:/Users/Franzi/Zotero/storage/FTII7NXR/2614-04.html:text/html}
}

@article{roberts_structural_2014,
	title = {Structural {Topic} {Models} for {Open}-{Ended} {Survey} {Responses}},
	volume = {58},
	issn = {1540-5907},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/ajps.12103/abstract},
	doi = {10.1111/ajps.12103},
	abstract = {Collection and especially analysis of open-ended survey responses are relatively rare in the discipline and when conducted are almost exclusively done through human coding. We present an alternative, semiautomated approach, the structural topic model (STM) (Roberts, Stewart, and Airoldi 2013; Roberts et al. 2013), that draws on recent developments in machine learning based analysis of textual data. A crucial contribution of the method is that it incorporates information about the document, such as the author's gender, political affiliation, and treatment assignment (if an experimental study). This article focuses on how the STM is helpful for survey researchers and experimentalists. The STM makes analyzing open-ended responses easier, more revealing, and capable of being used to estimate treatment effects. We illustrate these innovations with analysis of text from surveys and experiments.},
	language = {en},
	number = {4},
	journal = {American Journal of Political Science},
	author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin and Lucas, Christopher and Leder-Luis, Jetson and Gadarian, Shana Kushner and Albertson, Bethany and Rand, David G.},
	month = oct,
	year = {2014},
	pages = {1064--1082},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/FD9RK878/Roberts et al. - 2014 - Structural Topic Models for Open-Ended Survey Resp.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/6GGMT3JM/abstract.html:text/html}
}

@article{lucas_computer_2015,
	title = {Computer assisted text analysis for comparative politics},
	volume = {23},
	number = {2},
	journal = {Political Analysis},
	author = {Lucas, Christopher and Nielsen, Richard and Roberts, Margaret and Stewart, Brandon and Storer, Alex and Tingley, Dustin},
	year = {2015},
	pages = {254--277},
	file = {Computer assisted text analysis for comparative politics | Dustin Tingley:/Users/Franzi/Zotero/storage/N5EV4K52/computer-assisted-text-analysis-comparative-politics.html:text/html}
}

@techreport{mueller_reading_2016,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Reading between the {Lines}: {Prediction} of {Political} {Violence} {Using} {Newspaper} {Text}},
	shorttitle = {Reading between the {Lines}},
	url = {https://papers.ssrn.com/abstract=2843535},
	abstract = {This article provides a new methodology to predict armed conflict by using newspaper text. Through machine learning, vast quantities of newspaper text are reduced to interpretable topics. We propose the use of the within-country variation of these topics to predict the timing of conflict. This allows us to avoid the tendency of predicting conflict only in countries where it occurred before. We show that the within-country variation of topics is an extremely robust predictor of conflict and becomes particularly useful when new conflict risks arise. Two aspects seem to be responsible for these features. Topics provide depth because they consist of changing, long lists of terms which makes them able to capture the changing context of conflict. At the same time topics provide width because they summarize all text, including coverage of stabilizing factors.},
	number = {ID 2843535},
	urldate = {2017-11-09},
	institution = {Social Science Research Network},
	author = {Mueller, Hannes Felix and Rauh, Christopher},
	month = sep,
	year = {2016},
	keywords = {Civil War, conflict, Forecasting, Latent Dirichlet Allocation., Machine Learning, panel data, Topic Models},
	file = {Snapshot:/Users/Franzi/Zotero/storage/R7FGX7V9/papers.html:text/html}
}

@techreport{law_constitutional_2016,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Constitutional {Archetypes}},
	url = {https://papers.ssrn.com/abstract=2732519},
	abstract = {It is a core function of constitutions to justify the existence and organization of the state. The ideological narratives embedded in constitutions are not fundamentally unique, however, but instead derive from a limited number of competing models. Each model is defined by a particular type of justification for the existence and organization of the state, and by a symbiotic relationship with a particular legal tradition. These models are so ubiquitous and elemental that they amount to constitutional archetypes.This Article contends as an empirical matter that constitutional narratives of the state boil down to a combination of three basic archetypes–namely, a liberal archetype, a statist archetype, and a universalist archetype. The liberal archetype is closely identified with the common law tradition and views the state as a potentially oppressive concentration of authority in need of regulation and restraint. In keeping with this conception of the state, liberal constitutions emphasize the imposition of limits upon government in the form of negative and procedural rights, as well as a strong and independent judiciary to make these limits effective. The legitimacy of the state is contingent upon adherence to constitutional limits. Constitutions in this vein are largely agnostic as to what goals, if any, society as a whole should pursue through the mechanism of the state.The statist archetype, in contrast, is associated with the civil law tradition and hails the state as the embodiment of a distinctive community and the vehicle for the achievement of the community’s goals. The legitimacy of the state rests upon the strength of the state’s claim to represent the will of a community. Consequently, constitutions in this vein are attentive to the identity, membership, and symbols of the state. Other characteristics of a statist constitution include an emphasis on the articulation of collective goals and positive rights that contemplate an active role for the state, and an obligation on the part of citizens to cooperate with the state in the pursuit of shared goals.The universalist archetype, the newest and most prevalent of the three, is symbiotically intertwined with a post-World War II, post-Westphalian paradigm of international law that rests the legitimacy of the state upon the normative force of a global legal order that encompasses both constitutional law and international law. Characteristics of this archetype include explicit commitment to supranational institutions and supranational law and reliance on generic terms and concepts that can be found not only in a variety of national constitutions, but also in international legal instruments.Empirical evidence of the prevalence and content of these three basic archetypes can be found in the unlikeliest of places – namely, constitutional preambles. Preambles enjoy a reputation for expressing uniquely national values, identities, and narratives. If there is any part of a constitution that ought not to be reducible to a handful of recurring patterns, it is surely the preamble. Yet analysis of the world’s constitutional preambles using methods from computational linguistics suggests that they consist of a combination of the three archetypes. Estimation of a structural topic model yields a quantitative measure of the extent to which each preamble draws upon each archetype.The empirical analysis also highlights the growing commingling and interdependence of constitutional law and international law. The semantic patterns that characterize universalist preambles mirrors those found in leading international human rights instruments. The adoption of the same conceptual and normative vocabulary by both universalist constitutions and key international legal instruments signals the emergence of a globalized ideological dialect common to both domestic constitutional law and public international law. The rising use of this common language by constitutional drafters since World War II is a quantitative indicator of the growing extent to which constitutional law and public international law influence each other.},
	number = {ID 2732519},
	urldate = {2017-11-09},
	institution = {Social Science Research Network},
	author = {Law, David S.},
	month = dec,
	year = {2016},
	keywords = {text analysis, constitution, archetype, ideology, international law, constitutional law, preamble, statism, liberalism, universalism, automated, content analysis, topic model, legal traditions, civil law, common law, empirical, constitutional drafting, constitution-making, constitution-writing},
	file = {Snapshot:/Users/Franzi/Zotero/storage/IMHWX425/papers.html:text/html}
}

@article{farrell_corporate_2016,
	title = {Corporate funding and ideological polarization about climate change},
	volume = {113},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/113/1/92},
	doi = {10.1073/pnas.1509433112},
	abstract = {Drawing on large-scale computational data and methods, this research demonstrates how polarization efforts are influenced by a patterned network of political and financial actors. These dynamics, which have been notoriously difficult to quantify, are illustrated here with a computational analysis of climate change politics in the United States. The comprehensive data include all individual and organizational actors in the climate change countermovement (164 organizations), as well as all written and verbal texts produced by this network between 1993–2013 (40,785 texts, more than 39 million words). Two main findings emerge. First, that organizations with corporate funding were more likely to have written and disseminated texts meant to polarize the climate change issue. Second, and more importantly, that corporate funding influences the actual thematic content of these polarization efforts, and the discursive prevalence of that thematic content over time. These findings provide new, and comprehensive, confirmation of dynamics long thought to be at the root of climate change politics and discourse. Beyond the specifics of climate change, this paper has important implications for understanding ideological polarization more generally, and the increasing role of private funding in determining why certain polarizing themes are created and amplified. Lastly, the paper suggests that future studies build on the novel approach taken here that integrates large-scale textual analysis with social networks.},
	language = {en},
	number = {1},
	urldate = {2017-11-09},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Farrell, Justin},
	month = may,
	year = {2016},
	pmid = {26598653},
	keywords = {politics, funding, polarization, computational social science, climate change},
	pages = {92--97},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/8M3IS9E4/Farrell - 2016 - Corporate funding and ideological polarization abo.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/9R5TQ8PC/92.html:text/html}
}

@article{reich_computer-assisted_2014,
	title = {Computer-{Assisted} {Reading} and {Discovery} for {Student} {Generated} {Text} in {Massive} {Open} {Online} {Courses}},
	volume = {2},
	copyright = {Copyright (c)},
	issn = {1929-7750},
	url = {http://www.learning-analytics.info/journals/index.php/JLA/article/view/4138},
	doi = {10.18608/jla.2015.21.8},
	abstract = {Dealing with the vast quantities of text that students generate in Massive Open Online Courses (MOOCs) and other large-scale online learning environments is a daunting challenge. Computational tools are needed to help instructional teams uncover themes and patterns as students write in forums, assignments, and surveys. This paper introduces to the learning analytics community the Structural Topic Model, an approach to language processing that can 1) ﬁnd syntactic patterns with semantic meaning in unstructured text, 2) identify variation in those patterns across covariates, and 3) uncover archetypal texts that exemplify the documents within a topical pattern. We show examples of computationally aided discovery and reading in three MOOC settings: mapping students’ self-reported motivations, identifying themes in discussion forums, and uncovering patterns of feedback in course evaluations.},
	language = {en},
	number = {1},
	urldate = {2017-11-09},
	journal = {Journal of Learning Analytics},
	author = {Reich, Justin and Tingley, Dustin and Leder-Luis, Jetson and Roberts, Margaret E. and Stewart, Brandon},
	month = nov,
	year = {2014},
	keywords = {text analysis, Massive Open Online Courses, topic modelling, computer‐assisted reading},
	pages = {156--184},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/BPWP7FIR/Reich et al. - 2014 - Computer-Assisted Reading and Discovery for Studen.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/52JEF8DU/4138.html:text/html}
}

@incollection{gabszewicz_media_2015,
	series = {Handbook on the economics of the media. - {Cheltenham}, {UK} : {Edward} {Elgar} {Publishing}, {ISBN} 978-0-85793-888-6. - 2015, p. 3-35},
	title = {Media as multi-sided platforms},
	language = {eng},
	booktitle = {Handbook on the economics of the media},
	author = {Gabszewicz, Jean Jaskold and Resende, Joana and Sonnac, Nathalie},
	year = {2015},
	file = {Media as multi-sided platforms - EconBiz:/Users/Franzi/Zotero/storage/MS8D937F/10011339834.html:text/html}
}

@inproceedings{mimno_optimizing_2011,
	address = {Stroudsburg, PA, USA},
	series = {{EMNLP} '11},
	title = {Optimizing {Semantic} {Coherence} in {Topic} {Models}},
	isbn = {978-1-937284-11-4},
	url = {http://dl.acm.org/citation.cfm?id=2145432.2145462},
	abstract = {Latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces. In order for people to use such models, however, they must trust them. Unfortunately, typical dimensionality reduction methods for text, such as latent Dirichlet allocation, often produce low-dimensional subspaces (topics) that are obviously flawed to human domain experts. The contributions of this paper are threefold: (1) An analysis of the ways in which topics can be flawed; (2) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data; (3) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the National Institutes of Health (NIH).},
	booktitle = {Proceedings of the {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Mimno, David and Wallach, Hanna M. and Talley, Edmund and Leenders, Miriam and McCallum, Andrew},
	year = {2011},
	pages = {262--272},
	file = {ACM Full Text PDF:/Users/Franzi/Zotero/storage/758M9D8V/Mimno et al. - 2011 - Optimizing Semantic Coherence in Topic Models.pdf:application/pdf}
}

@inproceedings{wei_lda-based_2006,
	address = {New York, NY, USA},
	series = {{SIGIR} '06},
	title = {{LDA}-based {Document} {Models} for {Ad}-hoc {Retrieval}},
	isbn = {978-1-59593-369-0},
	url = {http://doi.acm.org/10.1145/1148170.1148204},
	doi = {10.1145/1148170.1148204},
	abstract = {Search algorithms incorporating some form of topic model have a long history in information retrieval. For example, cluster-based retrieval has been studied since the 60s and has recently produced good results in the language model framework. An approach to building topic models based on a formal generative model of documents, Latent Dirichlet Allocation (LDA), is heavily cited in the machine learning literature, but its feasibility and effectiveness in information retrieval is mostly unknown. In this paper, we study how to efficiently use LDA to improve ad-hoc retrieval. We propose an LDA-based document model within the language modeling framework, and evaluate it on several TREC collections. Gibbs sampling is employed to conduct approximate inference in LDA and the computational complexity is analyzed. We show that improvements over retrieval using cluster-based models can be obtained with reasonable efficiency.},
	booktitle = {Proceedings of the 29th {Annual} {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Wei, Xing and Croft, W. Bruce},
	year = {2006},
	keywords = {Information retrieval, topic model, document model, language model, latent dirichlet allocation (LDA)},
	pages = {178--185}
}

@inproceedings{wallach_evaluation_2009,
	address = {New York, NY, USA},
	series = {{ICML} '09},
	title = {Evaluation {Methods} for {Topic} {Models}},
	isbn = {978-1-60558-516-1},
	url = {http://doi.acm.org/10.1145/1553374.1553515},
	doi = {10.1145/1553374.1553515},
	abstract = {A natural evaluation metric for statistical topic models is the probability of held-out documents given a trained model. While exact computation of this probability is intractable, several estimators for this probability have been used in the topic modeling literature, including the harmonic mean method and empirical likelihood method. In this paper, we demonstrate experimentally that commonly-used methods are unlikely to accurately estimate the probability of held-out documents, and propose two alternative methods that are both accurate and efficient.},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Wallach, Hanna M. and Murray, Iain and Salakhutdinov, Ruslan and Mimno, David},
	year = {2009},
	pages = {1105--1112}
}

@incollection{chang_reading_2009,
	title = {Reading {Tea} {Leaves}: {How} {Humans} {Interpret} {Topic} {Models}},
	shorttitle = {Reading {Tea} {Leaves}},
	url = {http://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 22},
	publisher = {Curran Associates, Inc.},
	author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Boyd-graber, Jordan L. and Blei, David M.},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
	year = {2009},
	pages = {288--296},
	file = {NIPS Full Text PDF:/Users/Franzi/Zotero/storage/UUPBMREK/Chang et al. - 2009 - Reading Tea Leaves How Humans Interpret Topic Mod.pdf:application/pdf;NIPS Snapshort:/Users/Franzi/Zotero/storage/68XT476P/3700-reading-tea-leaves-how-humans-interpret-topic-models.html:text/html}
}

@inproceedings{blei_latent_2001,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'01},
	title = {Latent {Dirichlet} {Allocation}},
	url = {http://dl.acm.org/citation.cfm?id=2980539.2980618},
	abstract = {We propose a generative model for text and other collections of discrete data that generalizes or improves on several previous models including naive Bayes/unigram, mixture of unigrams [6], and Hof-mann's aspect model, also known as probabilistic latent semantic indexing (pLSI) [3]. In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable. Inference and learning are carried out efficiently via variational algorithms. We present empirical results on applications of this model to problems in text modeling, collaborative filtering, and text classification.},
	booktitle = {Proceedings of the 14th {International} {Conference} on {Neural} {Information} {Processing} {Systems}: {Natural} and {Synthetic}},
	publisher = {MIT Press},
	author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
	year = {2001},
	pages = {601--608}
}

@incollection{griffiths_hierarchical_2004,
	title = {Hierarchical {Topic} {Models} and the {Nested} {Chinese} {Restaurant} {Process}},
	url = {http://papers.nips.cc/paper/2466-hierarchical-topic-models-and-the-nested-chinese-restaurant-process.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 16},
	publisher = {MIT Press},
	author = {Griffiths, Thomas L. and Jordan, Michael I. and Tenenbaum, Joshua B. and Blei, David M.},
	editor = {Thrun, S. and Saul, L. K. and Schölkopf, B.},
	year = {2004},
	pages = {17--24},
	file = {NIPS Full Text PDF:/Users/Franzi/Zotero/storage/RC2JTMS3/Griffiths et al. - 2004 - Hierarchical Topic Models and the Nested Chinese R.pdf:application/pdf;NIPS Snapshort:/Users/Franzi/Zotero/storage/H6Z7JS5C/2466-hierarchical-topic-models-and-the-nested-chinese-restaurant-process.html:text/html}
}

@article{blei_nested_2007,
	title = {The nested {Chinese} restaurant process and {Bayesian} nonparametric inference of topic hierarchies},
	url = {http://arxiv.org/abs/0710.0845},
	abstract = {We present the nested Chinese restaurant process (nCRP), a stochastic process which assigns probability distributions to infinitely-deep, infinitely-branching trees. We show how this stochastic process can be used as a prior distribution in a Bayesian nonparametric model of document collections. Specifically, we present an application to information retrieval in which documents are modeled as paths down a random tree, and the preferential attachment dynamics of the nCRP leads to clustering of documents according to sharing of topics at multiple levels of abstraction. Given a corpus of documents, a posterior inference algorithm finds an approximation to a posterior distribution over trees, topics and allocations of words to levels of the tree. We demonstrate this algorithm on collections of scientific abstracts from several journals. This model exemplifies a recent trend in statistical machine learning--the use of Bayesian nonparametric methods to infer distributions on flexible data structures.},
	journal = {arXiv:0710.0845 [stat]},
	author = {Blei, David M. and Griffiths, Thomas L. and Jordan, Michael I.},
	month = oct,
	year = {2007},
	note = {arXiv: 0710.0845},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:0710.0845 PDF:/Users/Franzi/Zotero/storage/QF33HDD7/Blei et al. - 2007 - The nested Chinese restaurant process and Bayesian.pdf:application/pdf;arXiv.org Snapshot:/Users/Franzi/Zotero/storage/GC56ZTK4/0710.html:text/html}
}

@article{griffiths_probabilistic_2002,
	title = {A probabilistic approach to semantic representation},
	volume = {24},
	url = {https://escholarship.org/uc/item/44x9v7m7},
	number = {24},
	urldate = {2017-11-16},
	journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
	author = {Griffiths, Thomas L. and Steyvers, Mark},
	month = jan,
	year = {2002},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/6EQ8VAJR/Griffiths und Steyvers - 2002 - A probabilistic approach to semantic representatio.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/76HFDXBQ/44x9v7m7.pdf:application/pdf}
}

@techreport{heinrich_parameter_2004,
	title = {Parameter estimation for text analysis},
	abstract = {Abstract. Presents parameter estimation methods common with discrete probability distributions, which is of particular interest in text modeling. Starting with maximum likelihood, a posteriori and Bayesian estimation, central concepts like conjugate distributions and Bayesian networks are reviewed. As an application, the model of latent Dirichlet allocation (LDA) is explained in detail with a full derivation of an approximate inference algorithm based on Gibbs sampling, including a discussion of Dirichlet hyperparameter estimation. Finally, analysis methods of LDA models are discussed.},
	author = {Heinrich, Gregor},
	year = {2004},
	file = {Citeseer - Full Text PDF:/Users/Franzi/Zotero/storage/PTHWFSPM/Heinrich - 2004 - Parameter estimation for text analysis.pdf:application/pdf;Citeseer - Snapshot:/Users/Franzi/Zotero/storage/JZDEF9EX/summary.html:text/html}
}

@incollection{steyvers_probabilistic_2006,
	title = {Probabilistic {Topic} {Models}},
	booktitle = {Latent {Semantic} {Analysis}: {A} {Road} to {Meaning}.},
	publisher = {Laurence Erlbaum},
	author = {Steyvers, Mark and Griffiths, Thomas L.},
	editor = {Landauer, L. and Mcnamara, D. and Dennis, S. and Kintsch, W.},
	year = {2006}
}

@incollection{hoffman_online_2010,
	title = {Online {Learning} for {Latent} {Dirichlet} {Allocation}},
	url = {http://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 23},
	publisher = {Curran Associates, Inc.},
	author = {Hoffman, Matthew and Bach, Francis R. and Blei, David M.},
	editor = {Lafferty, J. D. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel, R. S. and Culotta, A.},
	year = {2010},
	pages = {856--864},
	file = {NIPS Full Text PDF:/Users/Franzi/Zotero/storage/E3N5REJ7/Hoffman et al. - 2010 - Online Learning for Latent Dirichlet Allocation.pdf:application/pdf;NIPS Snapshort:/Users/Franzi/Zotero/storage/NE5ZUD83/3902-online-learning-for-latent-dirichlet-allocation.html:text/html}
}

@inproceedings{buntine_estimating_2009,
	address = {Berlin, Heidelberg},
	series = {{ACML} '09},
	title = {Estimating {Likelihoods} for {Topic} {Models}},
	isbn = {978-3-642-05223-1},
	url = {http://dx.doi.org/10.1007/978-3-642-05224-8_6},
	doi = {10.1007/978-3-642-05224-8_6},
	abstract = {Topic models are a discrete analogue to principle component analysis and independent component analysis that model {\textless}em{\textgreater}topic{\textless}/em{\textgreater} at the word level within a document. They have many variants such as NMF, PLSI and LDA, and are used in many fields such as genetics, text and the web, image analysis and recommender systems. However, only recently have reasonable methods for estimating the likelihood of unseen documents, for instance to perform testing or model comparison, become available. This paper explores a number of recent methods, and improves their theory, performance, and testing.},
	booktitle = {Proceedings of the 1st {Asian} {Conference} on {Machine} {Learning}: {Advances} in {Machine} {Learning}},
	publisher = {Springer-Verlag},
	author = {Buntine, Wray},
	year = {2009},
	pages = {51--64}
}

@inproceedings{alsumait_topic_2009,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Topic {Significance} {Ranking} of {LDA} {Generative} {Models}},
	isbn = {978-3-642-04179-2 978-3-642-04180-8},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-04180-8_22},
	doi = {10.1007/978-3-642-04180-8_22},
	abstract = {Topic models, like Latent Dirichlet Allocation (LDA), have been recently used to automatically generate text corpora topics, and to subdivide the corpus words among those topics. However, not all the estimated topics are of equal importance or correspond to genuine themes of the domain. Some of the topics can be a collection of irrelevant words, or represent insignificant themes. Current approaches to topic modeling perform manual examination to find meaningful topics. This paper presents the first automated unsupervised analysis of LDA models to identify junk topics from legitimate ones, and to rank the topic significance. Basically, the distance between a topic distribution and three definitions of “junk distribution” is computed using a variety of measures, from which an expressive figure of the topic significance is implemented using 4-phase Weighted Combination approach. Our experiments on synthetic and benchmark datasets show the effectiveness of the proposed approach in ranking the topic significance.},
	language = {en},
	urldate = {2017-11-16},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {AlSumait, Loulwah and Barbará, Daniel and Gentle, James and Domeniconi, Carlotta},
	month = sep,
	year = {2009},
	pages = {67--82},
	file = {Snapshot:/Users/Franzi/Zotero/storage/WCPH8XR4/10.html:text/html}
}

@inproceedings{mimno_optimizing_2011-1,
	address = {Stroudsburg, PA, USA},
	series = {{EMNLP} '11},
	title = {Optimizing {Semantic} {Coherence} in {Topic} {Models}},
	isbn = {978-1-937284-11-4},
	url = {http://dl.acm.org/citation.cfm?id=2145432.2145462},
	abstract = {Latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces. In order for people to use such models, however, they must trust them. Unfortunately, typical dimensionality reduction methods for text, such as latent Dirichlet allocation, often produce low-dimensional subspaces (topics) that are obviously flawed to human domain experts. The contributions of this paper are threefold: (1) An analysis of the ways in which topics can be flawed; (2) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data; (3) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the National Institutes of Health (NIH).},
	booktitle = {Proceedings of the {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Mimno, David and Wallach, Hanna M. and Talley, Edmund and Leenders, Miriam and McCallum, Andrew},
	year = {2011},
	pages = {262--272},
	file = {ACM Full Text PDF:/Users/Franzi/Zotero/storage/8ATB9444/Mimno et al. - 2011 - Optimizing Semantic Coherence in Topic Models.pdf:application/pdf}
}

@inproceedings{mimno_bayesian_2011,
	address = {Stroudsburg, PA, USA},
	series = {{EMNLP} '11},
	title = {Bayesian {Checking} for {Topic} {Models}},
	isbn = {978-1-937284-11-4},
	url = {http://dl.acm.org/citation.cfm?id=2145432.2145459},
	abstract = {Real document collections do not fit the independence assumptions asserted by most statistical topic models, but how badly do they violate them? We present a Bayesian method for measuring how well a topic model fits a corpus. Our approach is based on posterior predictive checking, a method for diagnosing Bayesian models in user-defined ways. Our method can identify where a topic model fits the data, where it falls short, and in which directions it might be improved.},
	booktitle = {Proceedings of the {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Mimno, David and Blei, David},
	year = {2011},
	pages = {227--237},
	file = {ACM Full Text PDF:/Users/Franzi/Zotero/storage/MFUGSEJM/Mimno und Blei - 2011 - Bayesian Checking for Topic Models.pdf:application/pdf}
}

@inproceedings{newman_automatic_2010,
	address = {Stroudsburg, PA, USA},
	series = {{HLT} '10},
	title = {Automatic {Evaluation} of {Topic} {Coherence}},
	isbn = {978-1-932432-65-7},
	url = {http://dl.acm.org/citation.cfm?id=1857999.1858011},
	abstract = {This paper introduces the novel task of topic coherence evaluation, whereby a set of words, as generated by a topic model, is rated for coherence or interpretability. We apply a range of topic scoring models to the evaluation task, drawing on WordNet, Wikipedia and the Google search engine, and existing research on lexical similarity/relatedness. In comparison with human scores for a set of learned topics over two distinct datasets, we show a simple co-occurrence measure based on pointwise mutual information over Wikipedia data is able to achieve results for the task at or nearing the level of inter-annotator correlation, and that other Wikipedia-based lexical relatedness methods also achieve strong results. Google produces strong, if less consistent, results, while our results over WordNet are patchy at best.},
	booktitle = {Human {Language} {Technologies}: {The} 2010 {Annual} {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Newman, David and Lau, Jey Han and Grieser, Karl and Baldwin, Timothy},
	year = {2010},
	pages = {100--108},
	file = {ACM Full Text PDF:/Users/Franzi/Zotero/storage/QJIP655P/Newman et al. - 2010 - Automatic Evaluation of Topic Coherence.pdf:application/pdf}
}

@incollection{wallach_rethinking_2009,
	title = {Rethinking {LDA}: {Why} {Priors} {Matter}},
	shorttitle = {Rethinking {LDA}},
	url = {http://papers.nips.cc/paper/3854-rethinking-lda-why-priors-matter.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 22},
	publisher = {Curran Associates, Inc.},
	author = {Wallach, Hanna M. and Mimno, David M. and McCallum, Andrew},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
	year = {2009},
	pages = {1973--1981},
	file = {NIPS Full Text PDF:/Users/Franzi/Zotero/storage/TPHKVI25/Wallach et al. - 2009 - Rethinking LDA Why Priors Matter.pdf:application/pdf;NIPS Snapshort:/Users/Franzi/Zotero/storage/6FVP9RXD/3854-rethinking-lda-why-priors-matter.html:text/html}
}

@article{asuncion_smoothing_2012,
	title = {On {Smoothing} and {Inference} for {Topic} {Models}},
	url = {http://arxiv.org/abs/1205.2662},
	abstract = {Latent Dirichlet analysis, or topic modeling, is a flexible latent variable framework for modeling high-dimensional sparse count data. Various learning algorithms have been developed in recent years, including collapsed Gibbs sampling, variational inference, and maximum a posteriori estimation, and this variety motivates the need for careful empirical comparisons. In this paper, we highlight the close connections between these approaches. We find that the main differences are attributable to the amount of smoothing applied to the counts. When the hyperparameters are optimized, the differences in performance among the algorithms diminish significantly. The ability of these algorithms to achieve solutions of comparable accuracy gives us the freedom to select computationally efficient approaches. Using the insights gained from this comparative study, we show how accurate topic models can be learned in several seconds on text corpora with thousands of documents.},
	journal = {arXiv:1205.2662 [cs, stat]},
	author = {Asuncion, Arthur and Welling, Max and Smyth, Padhraic and Teh, Yee Whye},
	month = may,
	year = {2012},
	note = {arXiv: 1205.2662},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv\:1205.2662 PDF:/Users/Franzi/Zotero/storage/MRRAS29S/Asuncion et al. - 2012 - On Smoothing and Inference for Topic Models.pdf:application/pdf;arXiv.org Snapshot:/Users/Franzi/Zotero/storage/UHH4ICFA/1205.html:text/html}
}

@inproceedings{hofmann_probabilistic_1999,
	address = {New York, NY, USA},
	series = {{SIGIR} '99},
	title = {Probabilistic {Latent} {Semantic} {Indexing}},
	isbn = {978-1-58113-096-6},
	url = {http://doi.acm.org/10.1145/312624.312649},
	doi = {10.1145/312624.312649},
	booktitle = {Proceedings of the 22Nd {Annual} {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {ACM},
	author = {Hofmann, Thomas},
	year = {1999},
	pages = {50--57}
}

@article{deerwester_indexing_1990,
	title = {Indexing by latent semantic analysis},
	volume = {41},
	abstract = {A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 or-thogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are re-turned. initial tests find this completely automatic method for retrieval to be promising.},
	number = {6},
	journal = {Journal of the American Society for Information Science},
	author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
	year = {1990},
	pages = {391--407},
	file = {Citeseer - Full Text PDF:/Users/Franzi/Zotero/storage/EVGTHKF3/Deerwester et al. - 1990 - Indexing by latent semantic analysis.pdf:application/pdf;Citeseer - Snapshot:/Users/Franzi/Zotero/storage/5KT8CZXC/summary.html:text/html}
}

@article{corden_maximisation_1952,
	title = {The {Maximisation} of {Profit} by a {Newspaper}},
	volume = {20},
	issn = {0034-6527},
	url = {http://www.jstor.org/stable/2295888},
	doi = {10.2307/2295888},
	number = {3},
	journal = {The Review of Economic Studies},
	author = {Corden, W. M.},
	year = {1952},
	pages = {181--190}
}

@article{gustafsson_circulation_1978,
	title = {The circulation spiral and the principle of household coverage},
	volume = {26},
	issn = {0358-5522},
	url = {https://doi.org/10.1080/03585522.1978.10407893},
	doi = {10.1080/03585522.1978.10407893},
	abstract = {The growth of oligopoly within the newspaper industry is a widespread phenomenon which has been examined by both researchers into the mass media and public enquiries into the press in a number of countries. Politicians recognise the development and want to modify the process of concentration, prevent newspaper closures, and even promote new ventures. Many western countries have taken measures to try to control forces bearing toward concentration in the newspaper industry. Such efforts, however, require a thorough knowledge of the market and its mechanism.},
	number = {1},
	journal = {Scandinavian Economic History Review},
	author = {Gustafsson, Karl Erik},
	month = jan,
	year = {1978},
	pages = {1--14},
	file = {Snapshot:/Users/Franzi/Zotero/storage/D8TJWJWK/03585522.1978.html:text/html}
}

@article{blair_pricing_1993,
	title = {Pricing {Decisions} of the {Newspaper} {Monopolist}},
	volume = {59},
	issn = {0038-4038},
	url = {http://www.jstor.org/stable/1059734},
	doi = {10.2307/1059734},
	number = {4},
	journal = {Southern Economic Journal},
	author = {Blair, Roger D. and Romano, Richard E.},
	year = {1993},
	pages = {721--732}
}

@article{evans_empirical_2003,
	title = {Some {Empirical} {Aspects} of {Multi}-sided {Platform} {Industries}},
	volume = {2},
	issn = {1446-9022},
	url = {https://www.degruyter.com/view/j/rne.2003.2.issue-3/rne.2003.2.3.1026/rne.2003.2.3.1026.xml},
	doi = {10.2202/1446-9022.1026},
	abstract = {Multi-sided platform markets have two or more different groups of customers that businesses have to get and keep on board to succeed. These industries range from dating clubs (men and women), to video game consoles (game developers and users), to payment cards (cardholders and merchants), to operating system software (application developers and users). They include some of the most important industries in the economy. A survey of businesses in these industries shows that multi-sided platform businesses devise entry strategies to get multiple sides of the market on board and devise pricing, product, and other competitive strategies to keep multiple customer groups on a common platform that internalizes externalities across members of these groups.},
	number = {3},
	journal = {Review of Network Economics},
	author = {Evans, David S.},
	year = {2003},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/JXJD8ZCR/Evans - 2003 - Some Empirical Aspects of Multi-sided Platform Ind.pdf:application/pdf}
}

@article{ellman_what_2009,
	title = {What do the {Papers} {Sell}? {A} {Model} of {Advertising} and {Media} {Bias}*},
	volume = {119},
	issn = {1468-0297},
	shorttitle = {What do the {Papers} {Sell}?},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1468-0297.2009.02218.x/abstract},
	doi = {10.1111/j.1468-0297.2009.02218.x},
	abstract = {We model the market for news as a two-sided market where newspapers sell news to readers who value accuracy and sell space to advertisers who value advert-receptive readers. In this setting, monopolistic newspapers under-report or bias news that sufficiently reduces advertiser profits. Paradoxically, increasing the size of advertising eventually leads competing newspapers to reduce advertiser bias. Nonetheless, advertisers can counter this effect if able to commit to news-sensitive cut-off strategies, potentially inducing as much bias as in the monopoly case. We use these results to explain contrasting historical and recent evidence on commercial bias and influence in the media.},
	language = {en},
	number = {537},
	journal = {The Economic Journal},
	author = {Ellman, Matthew and Germano, Fabrizio},
	month = apr,
	year = {2009},
	pages = {680--704},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/ESUGMQCA/Ellman und Germano - 2009 - What do the Papers Sell A Model of Advertising an.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/E87BHVVH/abstract.html:text/html}
}

@book{wiedmann_text_2016,
	address = {Wiesbaden},
	edition = {1},
	title = {Text {Mining} for {Qualitative} {Data} {Analysis} in the {Social} {Sciences}},
	url = {//www.springer.com/de/book/9783658153083},
	urldate = {2017-11-26},
	publisher = {VS Verlag für Sozialwissenschaften},
	author = {Wiedmann, Gregor},
	year = {2016},
	file = {Snapshot:/Users/Franzi/Zotero/storage/7P4K4CSX/9783658153083.html:text/html}
}

@incollection{boogaart_linear_2013,
	series = {Use {R}!},
	title = {Linear {Models} for {Compositions}},
	isbn = {978-3-642-36808-0 978-3-642-36809-7},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-36809-7_5},
	abstract = {Compositions can play the role of dependent and independent variables in linear models. In both cases, the parameters of the linear models are again compositions of the same simplex as the data. Most methods for classical linear models have a close analog in these compositional linear models. This chapter addresses several questions on this subject. What are compositional linear models? How to visualize the dependence of compositions, already multivariable, with further external covariables? How to model and check such dependence with compositional linear models? What are the underlying assumptions? How can we check these assumptions? What is the compositional interpretation of the results? How to use linear models to provide statistical evidence with tests, confidence intervals, and predictive regions? How to visualize model results and model parameters? How to compare compositional linear models and how to find the most appropriate one?},
	language = {en},
	urldate = {2017-11-29},
	booktitle = {Analyzing {Compositional} {Data} with {R}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Boogaart, K. Gerald van den and Tolosana-Delgado, Raimon},
	year = {2013},
	doi = {10.1007/978-3-642-36809-7_5},
	pages = {95--175},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/XXRU4BZK/Boogaart und Tolosana-Delgado - 2013 - Linear Models for Compositions.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/66BBBZV2/978-3-642-36809-7_5.html:text/html}
}

@techreport{evans_industrial_2005,
	type = {Working {Paper}},
	title = {The {Industrial} {Organization} of {Markets} with {Two}-{Sided} {Platforms}},
	url = {http://www.nber.org/papers/w11603},
	abstract = {Two-sided platforms (2SPs) cater to two or more distinct groups of customers, facilitating value-creating interactions between them. The village market and the village matchmaker were 2SPs; eBay and Match.com are more recent examples. Other examples include payment card systems, magazines, shopping malls, and personal computer operating systems. Building on the seminal work of Rochet and Tirole (2003), a rapidly growing literature has illuminated the economic principles that apply to 2SPs generally. One key result is that 2SPs may find it profit-maximizing to charge prices for one customer group that are below marginal cost or even negative, and such skewed pricing pattern is prevalent, although not universal, in industries that appear to be based on 2SPs. Over the years, courts have also recognized that certain industries, notably payment card systems and newspapers, now understood to be based on 2SPs, are governed by unusual economic relationships. This chapter provides an introduction to the economics of 2SPs and its application to several competition policy issues.},
	number = {11603},
	institution = {National Bureau of Economic Research},
	author = {Evans, David S. and Schmalensee, Richard},
	month = sep,
	year = {2005},
	doi = {10.3386/w11603},
	file = {NBER Full Text PDF:/Users/Franzi/Zotero/storage/EWJINGI8/Evans und Schmalensee - 2005 - The Industrial Organization of Markets with Two-Si.pdf:application/pdf}
}

@article{arora_practical_2012,
	title = {A {Practical} {Algorithm} for {Topic} {Modeling} with {Provable} {Guarantees}},
	url = {https://arxiv.org/abs/1212.4777},
	urldate = {2017-12-07},
	author = {Arora, Sanjeev and Ge, Rong and Halpern, Yoni and Mimno, David and Moitra, Ankur and Sontag, David and Wu, Yichen and Zhu, Michael},
	month = dec,
	year = {2012},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/KR8KRPIX/Arora et al. - 2012 - A Practical Algorithm for Topic Modeling with Prov.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/P63RKV2V/1212.html:text/html}
}

@article{steiner_program_1952,
	title = {Program {Patterns} and {Preferences}, and the {Workability} of {Competition} in {Radio} {Broadcasting}},
	volume = {66},
	issn = {0033-5533},
	url = {http://www.jstor.org/stable/1882942},
	doi = {10.2307/1882942},
	abstract = {I. Criteria for the appraisal of workability, 195.--II. The one period model, 197.--III. The model over time, 207.--IV. Relevance of the model to the market structure of the industry, 217.--V. Some suggestions for further analysis, 222.},
	number = {2},
	journal = {The Quarterly Journal of Economics},
	author = {Steiner, Peter O.},
	year = {1952},
	pages = {194--223}
}

@article{gabszewicz_press_2001,
	series = {15th {Annual} {Congress} of the {European} {Economic} {Association}},
	title = {Press advertising and the ascent of the ‘{Pensée} {Unique}’},
	volume = {45},
	issn = {0014-2921},
	url = {http://www.sciencedirect.com/science/article/pii/S0014292101001398},
	doi = {10.1016/S0014-2921(01)00139-8},
	abstract = {The press industry depends in a crucial way on the possibility of financing an important fraction of its activities by advertising receipts. We show that this induces the editors of newspapers to moderate, in several cases, the political message they display to their readers, compared with the political opinions they would have expressed otherwise. To this end, we consider a three-stage game in which editors select sequentially their political image, the price of their newspaper and the advertising tariff they oppose to the advertisers. The intuition of the result lies in the fact that editors have to sell tasteless political messages to their readers in order to sell a larger audience to the advertisers.},
	number = {4},
	journal = {European Economic Review},
	author = {Gabszewicz, Jean J. and Laussel, Dider and Sonnac, Nathalie},
	month = may,
	year = {2001},
	keywords = {advertising, TV-broadcasting, Channels-program diversity},
	pages = {641--651},
	file = {ScienceDirect Full Text PDF:/Users/Franzi/Zotero/storage/8RHUZD5J/Gabszewicz et al. - 2001 - Press advertising and the ascent of the ‘Pensée Un.pdf:application/pdf;ScienceDirect Snapshot:/Users/Franzi/Zotero/storage/JH6EJC28/S0014292101001398.html:text/html}
}

@article{samuelson_aspects_1958,
	title = {Aspects of {Public} {Expenditure} {Theories}},
	volume = {40},
	issn = {0034-6535},
	url = {http://www.jstor.org/stable/1926336},
	doi = {10.2307/1926336},
	number = {4},
	journal = {The Review of Economics and Statistics},
	author = {Samuelson, Paul A.},
	year = {1958},
	pages = {332--338}
}

@article{loughran_when_2011,
	title = {When {Is} a {Liability} {Not} a {Liability}? {Textual} {Analysis}, {Dictionaries}, and 10-{Ks}},
	volume = {66},
	issn = {1540-6261},
	shorttitle = {When {Is} a {Liability} {Not} a {Liability}?},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2010.01625.x/abstract},
	doi = {10.1111/j.1540-6261.2010.01625.x},
	abstract = {Previous research uses negative word counts to measure the tone of a text. We show that word lists developed for other disciplines misclassify common words in financial text. In a large sample of 10-Ks during 1994 to 2008, almost three-fourths of the words identified as negative by the widely used Harvard Dictionary are words typically not considered negative in financial contexts. We develop an alternative negative word list, along with five other word lists, that better reflect tone in financial text. We link the word lists to 10-K filing returns, trading volume, return volatility, fraud, material weakness, and unexpected earnings.},
	language = {en},
	number = {1},
	urldate = {2018-01-11},
	journal = {The Journal of Finance},
	author = {Loughran, Tim and Mcdonald, Bill},
	month = feb,
	year = {2011},
	pages = {35--65},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/ZFIUBAP9/Loughran und Mcdonald - 2011 - When Is a Liability Not a Liability Textual Analy.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/TE73EQLZ/abstract.html:text/html}
}

@article{shapiro_measuring_2017,
	title = {Measuring {News} {Sentiment}},
	url = {https://econpapers.repec.org/paper/fipfedfwp/2017-01.htm},
	abstract = {We develop and assess new time series measures of economic sentiment based on computational text analysis of economic and financial newspaper articles from January 1980 to April 2015. The text analysis is based on predictive models estimated using machine learning techniques from Kanjoya. We analyze four alternative news sentiment indexes. We find that the news sentiment indexes correlate strongly with contemporaneous business cycle indicators. We also find that innovations to news sentiment predict future economic activity. Furthermore, in most cases, the news sentiment measures outperform the University of Michigan and Conference board measures in predicting the federal funds rate, consumption, employment, inflation, industrial production, and the S\&P500. For some of these economic outcomes, there is evidence that the news sentiment measures have significant predictive power even after conditioning on these survey-based measures.},
	number = {2017-01},
	urldate = {2018-01-11},
	journal = {Federal Reserve Bank of San Francisco Working Paper},
	author = {Shapiro, Adam and Sudhof, Moritz and Wilson, Daniel},
	month = jan,
	year = {2017},
	file = {RePEc PDF:/Users/Franzi/Zotero/storage/TB95H8YV/Shapiro et al. - 2017 - Measuring News Sentiment.pdf:application/pdf;RePEc Snapshot:/Users/Franzi/Zotero/storage/CJ57433M/2017-01.html:text/html}
}

@article{pritchard_association_2000,
	title = {Association {Mapping} in {Structured} {Populations}},
	volume = {67},
	issn = {0002-9297},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1287075/},
	abstract = {The use, in association studies, of the forthcoming dense genomewide collection of single-nucleotide polymorphisms (SNPs) has been heralded as a potential breakthrough in the study of the genetic basis of common complex disorders. A serious problem with association mapping is that population structure can lead to spurious associations between a candidate marker and a phenotype. One common solution has been to abandon case-control studies in favor of family-based tests of association, such as the transmission/disequilibrium test (TDT), but this comes at a considerable cost in the need to collect DNA from close relatives of affected individuals. In this article we describe a novel, statistically valid, method for case-control association studies in structured populations. Our method uses a set of unlinked genetic markers to infer details of population structure, and to estimate the ancestry of sampled individuals, before using this information to test for associations within subpopulations. It provides power comparable with the TDT in many settings and may substantially outperform it if there are conflicting associations in different subpopulations.},
	number = {1},
	urldate = {2018-01-19},
	journal = {American Journal of Human Genetics},
	author = {Pritchard, Jonathan K. and Stephens, Matthew and Rosenberg, Noah A. and Donnelly, Peter},
	month = jul,
	year = {2000},
	pmid = {10827107},
	pmcid = {PMC1287075},
	pages = {170--181},
	file = {PubMed Central Full Text PDF:/Users/Franzi/Zotero/storage/5C5QX5GU/Pritchard et al. - 2000 - Association Mapping in Structured Populations.pdf:application/pdf}
}

@article{erosheva_mixed-membership_2004-1,
	title = {Mixed-membership models of scientific publications},
	volume = {101},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/101/suppl_1/5220},
	doi = {10.1073/pnas.0307760101},
	abstract = {PNAS is one of world's most cited multidisciplinary scientific journals. The PNAS official classification structure of subjects is reflected in topic labels submitted by the authors of articles, largely related to traditionally established disciplines. These include broad field classifications into physical sciences, biological sciences, social sciences, and further subtopic classifications within the fields. Focusing on biological sciences, we explore an internal soft-classification structure of articles based only on semantic decompositions of abstracts and bibliographies and compare it with the formal discipline classifications. Our model assumes that there is a fixed number of internal categories, each characterized by multinomial distributions over words (in abstracts) and references (in bibliographies). Soft classification for each article is based on proportions of the article's content coming from each category. We discuss the appropriateness of the model for the PNAS database as well as other features of the data relevant to soft classification.},
	language = {en},
	number = {suppl 1},
	urldate = {2018-01-19},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Erosheva, Elena and Fienberg, Stephen and Lafferty, John},
	month = jun,
	year = {2004},
	pmid = {15020766},
	pages = {5220--5227},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/EXNI8WAK/Erosheva et al. - 2004 - Mixed-membership models of scientific publications.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/CXECBUCC/5220.html:text/html}
}

@article{braun_variational_2010,
	title = {Variational inference for large-scale models of discrete choice},
	volume = {105},
	issn = {0162-1459, 1537-274X},
	url = {http://arxiv.org/abs/0712.2526},
	doi = {10.1198/jasa.2009.tm08030},
	abstract = {Discrete choice models are commonly used by applied statisticians in numerous fields, such as marketing, economics, finance, and operations research. When agents in discrete choice models are assumed to have differing preferences, exact inference is often intractable. Markov chain Monte Carlo techniques make approximate inference possible, but the computational cost is prohibitive on the large data sets now becoming routinely available. Variational methods provide a deterministic alternative for approximation of the posterior distribution. We derive variational procedures for empirical Bayes and fully Bayesian inference in the mixed multinomial logit model of discrete choice. The algorithms require only that we solve a sequence of unconstrained optimization problems, which are shown to be convex. Extensive simulations demonstrate that variational methods achieve accuracy competitive with Markov chain Monte Carlo, at a small fraction of the computational cost. Thus, variational methods permit inferences on data sets that otherwise could not be analyzed without bias-inducing modifications to the underlying model.},
	number = {489},
	urldate = {2018-01-19},
	journal = {Journal of the American Statistical Association},
	author = {Braun, Michael and McAuliffe, Jon},
	month = mar,
	year = {2010},
	note = {arXiv: 0712.2526},
	keywords = {Statistics - Machine Learning, Statistics - Computation, Statistics - Methodology},
	pages = {324--335},
	file = {arXiv\:0712.2526 PDF:/Users/Franzi/Zotero/storage/NWRMRPXH/Braun und McAuliffe - 2010 - Variational inference for large-scale models of di.pdf:application/pdf;arXiv.org Snapshot:/Users/Franzi/Zotero/storage/V373IACQ/0712.html:text/html}
}

@inproceedings{bischof_summarizing_2012,
	address = {USA},
	series = {{ICML}'12},
	title = {Summarizing {Topical} {Content} with {Word} {Frequency} and {Exclusivity}},
	isbn = {978-1-4503-1285-1},
	url = {http://dl.acm.org/citation.cfm?id=3042573.3042578},
	abstract = {Recent work in text analysis commonly describes topics in terms of their most frequent words, but the exclusivity of words to topics is equally important for communicating content. We introduce Hierarchical Poisson Convolution (HPC), a model which infers regularized estimates of the differential use of words across topics as well as their frequency within topics. HPC uses known hierarchical structure on human-labeled topics to make focused comparisons of differential usage within each branch of the hierarchy of labels. We then infer a summary for each topic in terms of words that are both frequent and exclusive. We develop a parallelized Hamiltonian Monte Carlo sampler that allows for fast and scalable computation.},
	urldate = {2018-01-19},
	booktitle = {Proceedings of the 29th {International} {Coference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Bischof, Jonathan M. and Airoldi, Edoardo M.},
	year = {2012},
	pages = {9--16}
}

@article{endres_new_2003,
	title = {A new metric for probability distributions},
	volume = {49},
	issn = {0018-9448},
	doi = {10.1109/TIT.2003.813506},
	abstract = {We introduce a metric for probability distributions, which is bounded, information-theoretically motivated, and has a natural Bayesian interpretation. The square root of the well-known χ2 distance is an asymptotic approximation to it. Moreover, it is a close relative of the capacitory discrimination and Jensen-Shannon divergence.},
	number = {7},
	journal = {IEEE Transactions on Information Theory},
	author = {Endres, D. M. and Schindelin, J. E.},
	month = jul,
	year = {2003},
	keywords = {Adaptive estimation, Algorithm design and analysis, asymptotic approximation, Bayes methods, Bayesian interpretation, Bayesian methods, bounded information-theoretically motivated metric, capacitory discrimination, Convergence, Gaussian noise, information theory, Iterative algorithms, Jensen-Shannon divergence, probability, Probability distribution, probability distributions, square root, Wavelet analysis, White noise, Writing, χ2 distance},
	pages = {1858--1860},
	file = {IEEE Xplore Abstract Record:/Users/Franzi/Zotero/storage/H6L3EIL4/1207388.html:text/html}
}

@inproceedings{he_detecting_2009,
	address = {New York, NY, USA},
	series = {{CIKM} '09},
	title = {Detecting {Topic} {Evolution} in {Scientific} {Literature}: {How} {Can} {Citations} {Help}?},
	isbn = {978-1-60558-512-3},
	shorttitle = {Detecting {Topic} {Evolution} in {Scientific} {Literature}},
	url = {http://doi.acm.org/10.1145/1645953.1646076},
	doi = {10.1145/1645953.1646076},
	abstract = {Understanding how topics in scientific literature evolve is an interesting and important problem. Previous work simply models each paper as a bag of words and also considers the impact of authors. However, the impact of one document on another as captured by citations, one important inherent element in scientific literature, has not been considered. In this paper, we address the problem of understanding topic evolution by leveraging citations, and develop citation-aware approaches. We propose an iterative topic evolution learning framework by adapting the Latent Dirichlet Allocation model to the citation network and develop a novel inheritance topic model. We evaluate the effectiveness and efficiency of our approaches and compare with the state of the art approaches on a large collection of more than 650,000 research papers in the last 16 years and the citation network enabled by CiteSeerX. The results clearly show that citations can help to understand topic evolution better.},
	urldate = {2018-01-23},
	booktitle = {Proceedings of the 18th {ACM} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {ACM},
	author = {He, Qi and Chen, Bi and Pei, Jian and Qiu, Baojun and Mitra, Prasenjit and Giles, Lee},
	year = {2009},
	keywords = {citations, inheritance topic model, topic evolution},
	pages = {957--966}
}

@article{newman_distributed_2009,
	title = {Distributed {Algorithms} for {Topic} {Models}},
	volume = {10},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=1577069.1755845},
	abstract = {We describe distributed algorithms for two widely-used topic models, namely the Latent Dirichlet Allocation (LDA) model, and the Hierarchical Dirichet Process (HDP) model. In our distributed algorithms the data is partitioned across separate processors and inference is done in a parallel, distributed fashion. We propose two distributed algorithms for LDA. The first algorithm is a straightforward mapping of LDA to a distributed processor setting. In this algorithm processors concurrently perform Gibbs sampling over local data followed by a global update of topic counts. The algorithm is simple to implement and can be viewed as an approximation to Gibbs-sampled LDA. The second version is a model that uses a hierarchical Bayesian extension of LDA to directly account for distributed data. This model has a theoretical guarantee of convergence but is more complex to implement than the first algorithm. Our distributed algorithm for HDP takes the straightforward mapping approach, and merges newly-created topics either by matching or by topic-id. Using five real-world text corpora we show that distributed learning works well in practice. For both LDA and HDP, we show that the converged test-data log probability for distributed learning is indistinguishable from that obtained with single-processor learning. Our extensive experimental results include learning topic models for two multi-million document collections using a 1024-processor parallel computer.},
	urldate = {2018-01-23},
	journal = {J. Mach. Learn. Res.},
	author = {Newman, David and Asuncion, Arthur and Smyth, Padhraic and Welling, Max},
	month = dec,
	year = {2009},
	pages = {1801--1828},
	file = {ACM Full Text PDF:/Users/Franzi/Zotero/storage/8WPWT44A/Newman et al. - 2009 - Distributed Algorithms for Topic Models.pdf:application/pdf}
}

@inproceedings{kim_topic_2011,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Topic {Chains} for {Understanding} a {News} {Corpus}},
	isbn = {978-3-642-19436-8 978-3-642-19437-5},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-19437-5_13},
	doi = {10.1007/978-3-642-19437-5_13},
	abstract = {The Web is a great resource and archive of news articles for the world. We present a framework, based on probabilistic topic modeling, for uncovering the meaningful structure and trends of important topics and issues hidden within the news archives on the Web. Central in the framework is a topic chain, a temporal organization of similar topics. We experimented with various topic similarity metrics and present our insights on how best to construct topic chains. We discuss how to interpret the topic chains to understand the news corpus by looking at long-term topics, temporary issues, and shifts of focus in the topic chains. We applied our framework to nine months of Korean Web news corpus and present our findings.},
	language = {en},
	urldate = {2018-01-23},
	booktitle = {Computational {Linguistics} and {Intelligent} {Text} {Processing}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Kim, Dongwoo and Oh, Alice},
	month = feb,
	year = {2011},
	pages = {163--176}
}

@inproceedings{wang_mining_2009,
	address = {New York, NY, USA},
	series = {{WSDM} '09},
	title = {Mining {Common} {Topics} from {Multiple} {Asynchronous} {Text} {Streams}},
	isbn = {978-1-60558-390-7},
	url = {http://doi.acm.org/10.1145/1498759.1498826},
	doi = {10.1145/1498759.1498826},
	abstract = {Text streams are becoming more and more ubiquitous, in the forms of news feeds, weblog archives and so on, which result in a large volume of data. An effective way to explore the semantic as well as temporal information in text streams is topic mining, which can further facilitate other knowledge discovery procedures. In many applications, we are facing multiple text streams which are related to each other and share common topics. The correlation among these streams can provide more meaningful and comprehensive clues for topic mining than those from each individual stream. However, it is nontrivial to explore the correlation with the existence of asynchronism among multiple streams, i.e. documents from different streams about the same topic may have different timestamps, which remains unsolved in the context of topic mining. In this paper, we formally address this problem and put forward a novel algorithm based on the generative topic model. Our algorithm consists of two alternate steps: the first step extracts common topics from multiple streams based on the adjusted timestamps by the second step; the second step adjusts the timestamps of the documents according to the time distribution of the discovered topics by the first step. We perform these two steps alternately and a monotone convergence of our objective function is guaranteed. The effectiveness and advantage of our approach were justified by extensive empirical studies on two real data sets consisting of six research paper streams and two news article streams, respectively.},
	urldate = {2018-01-23},
	booktitle = {Proceedings of the {Second} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {ACM},
	author = {Wang, Xiang and Zhang, Kai and Jin, Xiaoming and Shen, Dou},
	year = {2009},
	keywords = {topic model, asynchronous streams, temporal text mining},
	pages = {192--201}
}

@book{ramage_labeled_2009,
	title = {Labeled {LDA}: {A} supervised topic model for credit attribution in multi-labeled corpora},
	shorttitle = {Labeled {LDA}},
	abstract = {A significant portion of the world’s text is tagged by readers on social bookmarking websites. Credit attribution is an inherent problem in these corpora because most pages have multiple tags, but the tags do not always apply with equal specificity across the whole document. Solving the credit attribution problem requires associating each word in a document with the most appropriate tags and vice versa. This paper introduces Labeled LDA, a topic model that constrains Latent Dirichlet Allocation by defining a one-to-one correspondence between LDA’s latent topics and user tags. This allows Labeled LDA to directly learn word-tag correspondences. We demonstrate Labeled LDA’s improved expressiveness over traditional LDA with visualizations of a corpus of tagged web pages from del.icio.us. Labeled LDA outperforms SVMs by more than 3 to 1 when extracting tag-specific document snippets. As a multi-label text classifier, our model is competitive with a discriminative baseline on a variety of datasets. 1},
	author = {Ramage, Daniel and Hall, David and Nallapati, Ramesh and Manning, Christopher D.},
	year = {2009},
	file = {Citeseer - Full Text PDF:/Users/Franzi/Zotero/storage/WD2TZ58J/Ramage et al. - Labeled LDA A supervised topic model for credit a.pdf:application/pdf;Citeseer - Snapshot:/Users/Franzi/Zotero/storage/W5DGAHT5/summary.html:text/html}
}

@inproceedings{chaney_visualizing_2012,
	title = {Visualizing {Topic} {Models}},
	copyright = {Authors who publish a paper in this conference agree to the following terms:    1. Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.    2. The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.    3. The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.    4. Author(s) retain all proprietary rights other than copyright (such as patent rights).    5. Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.    6. Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without AAAI’s written permission.    7. Author(s) may make limited distribution of all or portions of their article/paper prior to publication.    8. In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.    9. In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/ICWSM/ICWSM12/paper/view/4645},
	abstract = {Managing large collections of documents is an important problem for many areas of science, industry, and culture. Probabilistic topic modeling offers a promising solution. Topic modeling is an unsupervised machine learning method that learns the underlying themes in a large collection of otherwise unorganized documents. This discovered structure summarizes and organizes the documents. However, topic models are high-level statistical tools\&mdash;a user must scrutinize numerical distributions to understand and explore their results. In this paper, we present a method for visualizing topic models. Our method creates a navigator of the documents, allowing users to explore the hidden structure that a topic model discovers. These browsing interfaces reveal meaningful patterns in a collection, helping end-users explore and understand its contents in new ways. We provide open source software of our method.},
	language = {en},
	urldate = {2018-01-23},
	booktitle = {Sixth {International} {AAAI} {Conference} on {Weblogs} and {Social} {Media}},
	author = {Chaney, Allison June-Barlow and Blei, David M.},
	month = may,
	year = {2012},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/968W9M2F/Chaney und Blei - 2012 - Visualizing Topic Models.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/DXRG63IG/4645.html:text/html}
}

@article{egami_how_2017,
	title = {How to {Make} {Causal} {Inferences} {Using} {Texts}},
	journal = {Working Paper},
	author = {Egami, Naoki and Fong, Christian J. and Grimmer, Justin and Roberts, Margaret E. and Stewart, Brandon M.},
	month = dec,
	year = {2017},
	file = {How to Make Causal Inferences Using Texts | Naoki Egami:/Users/Franzi/Zotero/storage/LVA68CVK/how-make-causal-inferences-using-texts.html:text/html}
}

@mastersthesis{paul_cross-collection_2009,
	title = {Cross-{Collection} {Topic} {Models}: {Automatically} {Comparing} and {Contrasting} {Text}},
	shorttitle = {Cross-{Collection} {Topic} {Models}},
	abstract = {This paper describes cross-collection latent Dirichlet allocation (ccLDA), a probabilistic topic model that captures meaningful word co-occurrences across multiple text collections. The model is applied to three different applications: discovering cultural differences in blogs and forums from different countries, discovering research topics across multiple scientific disciplines, and comparing editorial differences between multiple media sources. A variety of qualitative and quantitative evaluations of ccLDA are performed, including log-likelihood measurements and performance measurements of the model used as a generative classifier. Improvements over previous work are demonstrated. Finally, possible extensions and modifications to the model are presented with promising results. 1},
	school = {University of Illinois at Urbana-Champaign},
	author = {Paul, Michael},
	year = {2009},
	file = {Citeseer - Full Text PDF:/Users/Franzi/Zotero/storage/H2I347RL/Paul - Cross-Collection Topic Models Automatically Compa.pdf:application/pdf;Citeseer - Snapshot:/Users/Franzi/Zotero/storage/KGFW8BSS/summary.html:text/html}
}

@article{nyman_news_2018,
	title = {News and narratives in financial systems: exploiting big data for systemic risk assessment {\textbar} {Bank} of {England}},
	volume = {704},
	url = {https://www.bankofengland.co.uk/working-paper/2018/news-and-narratives-in-financial-systems},
	urldate = {2018-02-21},
	journal = {Bank of England Working Paper},
	author = {Nyman, Rickard and Kapadia, Sujit and Tuckett, David and Gregory, David and Ormerod, Paul and Smith, Robert},
	month = jan,
	year = {2018},
	file = {News and narratives in financial systems\: exploiting big data for systemic risk assessment | Bank of England:/Users/Franzi/Zotero/storage/BK83G72M/news-and-narratives-in-financial-systems.html:text/html}
}

@book{cage_information_2017,
	title = {L'information à tout prix},
	language = {fr},
	publisher = {Éditions de l’INA},
	author = {Cagé, Julia and Hervé, Nicolas and Viaud, Marie-Luce},
	year = {2017},
	file = {Snapshot:/Users/Franzi/Zotero/storage/E5Z7FALC/Lire-L-information-a-tout-prix-de-Julia-Cage.html:text/html}
}

@article{paul_qualitative_2017,
	title = {Qualitative and quantitative central bank communication and inflation expectations},
	volume = {17},
	url = {https://ideas.repec.org/a/bpj/bejmac/v17y2017i1p41n7.html},
	abstract = {We aim to investigate the simultaneous and interacted effects of central bank qualitative and quantitative communication on private inflation expectations, measured with survey and market-based measures. The effects of ECB inflation projections and Governing Council members’ speeches are identified through an instrumental-variables estimation using a principal component analysis to generate relevant instruments. We find that ECB projections have a positive effect on current-year forecasts, and that ECB projections and speeches are substitutes at longer horizons. Moreover, ECB speeches and the ECB rate reinforce the effect of ECB projections when they are consistent, and convey the same signal about inflationary pressures.},
	language = {en},
	number = {1},
	urldate = {2018-03-07},
	journal = {The B.E. Journal of Macroeconomics},
	author = {Paul, Hubert},
	year = {2017},
	keywords = {central bank communication, European central bank, monetary policy, principal component analysis},
	pages = {1--41},
	file = {Snapshot:/Users/Franzi/Zotero/storage/55VNARXK/v17y2017i1p41n7.html:text/html}
}

@article{tetlock_giving_2007-1,
	title = {Giving {Content} to {Investor} {Sentiment}: {The} {Role} of {Media} in the {Stock} {Market}},
	volume = {62},
	issn = {1540-6261},
	shorttitle = {Giving {Content} to {Investor} {Sentiment}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2007.01232.x/abstract},
	doi = {10.1111/j.1540-6261.2007.01232.x},
	abstract = {I quantitatively measure the interactions between the media and the stock market using daily content from a popular Wall Street Journal column. I find that high media pessimism predicts downward pressure on market prices followed by a reversion to fundamentals, and unusually high or low pessimism predicts high market trading volume. These and similar results are consistent with theoretical models of noise and liquidity traders, and are inconsistent with theories of media content as a proxy for new information about fundamental asset values, as a proxy for market volatility, or as a sideshow with no relationship to asset markets.},
	language = {en},
	number = {3},
	urldate = {2018-03-07},
	journal = {The Journal of Finance},
	author = {Tetlock, Paul C.},
	month = jun,
	year = {2007},
	pages = {1139--1168},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/RHRD6AQT/Tetlock - 2007 - Giving Content to Investor Sentiment The Role of .pdf:application/pdf}
}

@article{hansen_shocking_2016,
	series = {38th {Annual} {NBER} {International} {Seminar} on {Macroeconomics}},
	title = {Shocking language: {Understanding} the macroeconomic effects of central bank communication},
	volume = {99},
	issn = {0022-1996},
	shorttitle = {Shocking language},
	url = {http://www.sciencedirect.com/science/article/pii/S0022199615001828},
	doi = {10.1016/j.jinteco.2015.12.008},
	abstract = {We explore how the multi-dimensional aspects of information released by the FOMC has effects on both market and real economic variables. Using tools from computational linguistics, we measure the information released by the FOMC on the state of economic conditions, as well as the guidance the FOMC provides about future monetary policy decisions. Employing these measures within a FAVAR framework, we find that shocks to forward guidance are more important than the FOMC communication of current economic conditions in terms of their effects on market and real variables. Nonetheless, neither communication has particularly strong effects on real economic variables.},
	urldate = {2018-03-07},
	journal = {Journal of International Economics},
	author = {Hansen, Stephen and McMahon, Michael},
	month = mar,
	year = {2016},
	keywords = {Communication, Monetary policy, Vector autoregression},
	pages = {S114--S133},
	file = {ScienceDirect Full Text PDF:/Users/Franzi/Zotero/storage/AINER6IR/Hansen und McMahon - 2016 - Shocking language Understanding the macroeconomic.pdf:application/pdf;ScienceDirect Snapshot:/Users/Franzi/Zotero/storage/39TT2SW9/S0022199615001828.html:text/html}
}

@article{tetlock_more_2008,
	title = {More {Than} {Words}: {Quantifying} {Language} to {Measure} {Firms}' {Fundamentals}},
	volume = {63},
	issn = {1540-6261},
	shorttitle = {More {Than} {Words}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2008.01362.x/abstract},
	doi = {10.1111/j.1540-6261.2008.01362.x},
	abstract = {We examine whether a simple quantitative measure of language can be used to predict individual firms' accounting earnings and stock returns. Our three main findings are: (1) the fraction of negative words in firm-specific news stories forecasts low firm earnings; (2) firms' stock prices briefly underreact to the information embedded in negative words; and (3) the earnings and return predictability from negative words is largest for the stories that focus on fundamentals. Together these findings suggest that linguistic media content captures otherwise hard-to-quantify aspects of firms' fundamentals, which investors quickly incorporate into stock prices.},
	language = {en},
	number = {3},
	urldate = {2018-03-07},
	journal = {The Journal of Finance},
	author = {Tetlock, Paul C. and Saar-Tsechansky, Maytal and Macskassy, Sofus},
	month = jun,
	year = {2008},
	pages = {1437--1467},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/NTZB3TUK/Tetlock et al. - 2008 - More Than Words Quantifying Language to Measure F.pdf:application/pdf}
}

@misc{noauthor_media_2014,
	title = {Media {Freedom} and {Pluralism}},
	url = {https://ec.europa.eu/digital-single-market/en/policies/media-freedom-and-pluralism},
	abstract = {The importance of transparency, freedom and diversity in Europe's media landscape. The European Commission commits to respect freedom and pluralism of media. In this page you can find several acts, documents and studies on the subject.},
	language = {en},
	urldate = {2018-03-14},
	journal = {Digital Single Market},
	month = feb,
	year = {2014},
	file = {Snapshot:/Users/Franzi/Zotero/storage/23P77YVX/media-freedom-and-pluralism.html:text/html}
}

@inproceedings{tumasjan_predicting_2010,
	address = {Washington},
	title = {Predicting {Elections} with {Twitter}: {What} 140 {Characters} {Reveal} about {Political} {Sentiment}},
	shorttitle = {Predicting {Elections} with {Twitter}},
	url = {https://www.researchgate.net/publication/215776042_Predicting_Elections_with_Twitter_What_140_Characters_Reveal_about_Political_Sentiment},
	abstract = {Twitter is a microblogging website where users read and write millions of short messages on a variety of topics every day. This study uses the context of the German federal election to investigate whether Twitter is used as a forum for political deliberation and whether online messages on Twitter validly mirror offline political sentiment. Using LIWC text analysis software, we conducted a contentanalysis of over 100,000 messages containing a reference to either a political party or a politician. Our results show that Twitter is indeed used extensively for political deliberation. We find that the mere number of messages mentioning a party reflects the election result. Moreover, joint mentions of two parties are in line with real world political ties and coalitions. An analysis of the tweets' political sentiment demonstrates close correspondence to the parties' and politicians' political positions indicating that the content of Twitter messages plausibly reflects the offline political landscape. We discuss the use of microblogging message content as a valid indicator of political sentiment and derive suggestions for further research.},
	language = {en},
	urldate = {2018-03-17},
	booktitle = {Proceedings of the {Fourth} {International} {Conference} on {Weblogs} and {Social} {Media}},
	author = {Tumasjan, Andranik and Sprenger, Timm Oliver and Sandner, Philipp and Welpe, Isabell},
	year = {2010},
	file = {Snapshot:/Users/Franzi/Zotero/storage/JHIPIJAW/215776042_Predicting_Elections_with_Twitter_What_140_Characters_Reveal_about_Political_Sentimen.html:text/html}
}

@article{fu_analyzing_2013,
	title = {Analyzing {Online} {Sentiment} to {Predict} {Telephone} {Poll} {Results}},
	volume = {16},
	issn = {2152-2715, 2152-2723},
	url = {http://online.liebertpub.com/doi/abs/10.1089/cyber.2012.0375},
	doi = {10.1089/cyber.2012.0375},
	abstract = {The telephone survey is a common social science research method for capturing public opinion, for example, an individual’s values or attitudes, or the government’s approval rating. However, reducing domestic landline usage, increasing nonresponse rate, and suffering from response bias of the interviewee’s self-reported data pose methodological challenges to such an approach. Because of the labor cost of administration, a phone survey is often conducted on a biweekly or monthly basis, and therefore a daily reﬂection of public opinion is usually not available. Recently, online sentiment analysis of user-generated content has been deployed to predict public opinion and human behavior. However, its overall effectiveness remains uncertain. This study seeks to examine the temporal association between online sentiment reﬂected in social media content and phone survey poll results in Hong Kong. Speciﬁcally, it aims to ﬁnd the extent to which online sentiment can predict phone survey results. Using autoregressive integrated moving average time-series analysis, this study suggested that online sentiment scores can lead phone survey results by about 8–15 days, and their correlation coefﬁcients were about 0.16. The ﬁnding is signiﬁcant to the study of social media in social science research, because it supports the conclusion that daily sentiment observed in social media content can serve as a leading predictor for phone survey results, keeping as much as 2 weeks ahead of the monthly announcement of opinion polls. We also discuss the practical and theoretical implications of this study.},
	language = {en},
	number = {9},
	urldate = {2018-03-19},
	journal = {Cyberpsychology, Behavior, and Social Networking},
	author = {Fu, King-wa and Chan, Chee-hon},
	month = sep,
	year = {2013},
	pages = {702--707},
	file = {Fu und Chan - 2013 - Analyzing Online Sentiment to Predict Telephone Po.pdf:/Users/Franzi/Zotero/storage/MPTLP2XU/Fu und Chan - 2013 - Analyzing Online Sentiment to Predict Telephone Po.pdf:application/pdf}
}

@inproceedings{godbole_large-scale_2007,
	title = {Large-{Scale} {Sentiment} {Analysis} for {News} and {Blogs} ({System} {Demonstration})},
	author = {Godbole, Namrata and Srinivasaiah, Manjunath and Skiena, Steven},
	month = jan,
	year = {2007},
	pages = {2},
	file = {Godbole et al. - Large-Scale Sentiment Analysis for News and Blogs .pdf:/Users/Franzi/Zotero/storage/AJPISQHX/Godbole et al. - Large-Scale Sentiment Analysis for News and Blogs .pdf:application/pdf}
}

@article{padmaja_evaluating_2014,
	title = {Evaluating {Sentiment} {Analysis} {Methods} and {Identifying} {Scope} of {Negation} in {Newspaper} {Articles}},
	volume = {3},
	url = {http://thesai.org/Publications/ViewPaper?Volume=3&Issue=11&Code=IJARAI&SerialNo=1},
	doi = {10.14569/IJARAI.2014.031101},
	abstract = {Automatic detection of linguistic negation in free text is a demanding need for many text processing applications including Sentiment Analysis. Our system uses online news archives from two different resources namely NDTV and The Hindu. While dealing with news articles, we performed three subtasks namely identifying the target; separation of good and bad news content from the good and bad sentiment expressed on the target and analysis of clearly marked opinion that is expressed explicitly, not needing interpretation or the use of world knowledge. In this paper, our main focus was on evaluating and comparing three sentiment analysis methods (two machine learning based and one lexical based) and also identifying the scope of negation in news articles for two political parties namely BJP and UPA by using three existing methodologies. They were Rest of the Sentence (RoS), Fixed Window Length (FWL) and Dependency Analysis (DA). Among the sentiment methods the best F-measure was SVM with the values 0.688 and 0.657 for BJP and UPA respectively. On the other hand, the F measures for RoS, FWL and DA were 0.58, 0.69 and 0.75 respectively. We observed that DA was performing better than the other two. Among 1675 sentences in the corpus, according to annotator I, 1,137 were positive and 538 were negative whereas according to annotator II, 1,130 were positive and 545 were negative. Further we also identified the score of each sentence and calculated the accuracy on the basis of average score of both the annotators.},
	language = {en},
	number = {11},
	urldate = {2018-03-19},
	journal = {International Journal of Advanced Research in Artificial Intelligence (IJARAI)},
	author = {Padmaja, S. and Fatima, Prof S. Sameen and Bandu, Sasidhar},
	year = {2014},
	file = {Full Text PDF:/Users/Franzi/Zotero/storage/3NRWA2F2/Padmaja et al. - 2014 - Evaluating Sentiment Analysis Methods and Identify.pdf:application/pdf;Snapshot:/Users/Franzi/Zotero/storage/RY3NFDRX/ViewPaper.html:text/html}
}

@article{luoma_development_2010,
	title = {The development and psychometric properties of a new measure of perceived stigma toward substance users},
	volume = {45},
	issn = {1532-2491},
	doi = {10.3109/10826080902864712},
	abstract = {A self-report measure of perceived stigma toward substance users was developed and studied. An initial measure was created based on a previously developed scale that was rated by experts for content validity and quality of items. The scale, along with other measures, was administered to 252 people in treatment for substance problems in the United States during 2006-2007. Refinement efforts resulted in an eight-item scale with good face validity, construct validity, and adequate levels of internal consistency. Most relationships with other constructs were as expected. Findings suggest that perceived stigma is distinct from other forms of stigma.},
	language = {eng},
	number = {1-2},
	journal = {Substance Use \& Misuse},
	author = {Luoma, Jason B. and O'Hair, Alyssa K. and Kohlenberg, Barbara S. and Hayes, Steven C. and Fletcher, Lindsay},
	year = {2010},
	pmid = {20025438},
	pmcid = {PMC5067154},
	keywords = {Humans, Adolescent, Adult, Drug Users, Factor Analysis, Statistical, Female, Male, Middle Aged, Psychometrics, Reproducibility of Results, Social Perception, Stereotyping, Substance-Related Disorders},
	pages = {47--57}
}

@article{dewenter_can_2018,
	title = {Can {Media} {Drive} the {Electorate}? {The} {Impact} of {Media} {Coverage} on {Party} {Affiliation} and {Voting} {Intentions}},
	volume = {179},
	journal = {Working Paper Series, Helmut Schmidt University Hamburg, Department of Economics},
	author = {Dewenter, Ralf and Linder, Melissa and Thomas, Tobias},
	month = apr,
	year = {2018}
}