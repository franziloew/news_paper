
@article{armstrong_competition_2006,
	title = {Competition in two-sided markets},
	volume = {37},
	issn = {1756-2171},
	url = {http://dx.doi.org/10.1111/j.1756-2171.2006.tb00037.x},
	doi = {10.1111/j.1756-2171.2006.tb00037.x},
	pages = {668--691},
	number = {3},
	journaltitle = {The {RAND} Journal of Economics},
	author = {Armstrong, Mark},
	date = {2006}
}

@article{pritchard_inference_2000,
	title = {Inference of population structure using multilocus genotype data},
	volume = {155},
	issn = {0016-6731},
	abstract = {We describe a model-based clustering method for using multilocus genotype data to infer population structure and assign individuals to populations. We assume a model in which there are K populations (where K may be unknown), each of which is characterized by a set of allele frequencies at each locus. Individuals in the sample are assigned (probabilistically) to populations, or jointly to two or more populations if their genotypes indicate that they are admixed. Our model does not assume a particular mutation process, and it can be applied to most of the commonly used genetic markers, provided that they are not closely linked. Applications of our method include demonstrating the presence of population structure, assigning individuals to populations, studying hybrid zones, and identifying migrants and admixed individuals. We show that the method can produce highly accurate assignments using modest numbers of loci-e.g. , seven microsatellite loci in an example using genotype data from an endangered bird species. The software used for this article is available from http://www.stats.ox.ac.uk/ approximately pritch/home. html.},
	pages = {945--959},
	number = {2},
	journaltitle = {Genetics},
	shortjournal = {Genetics},
	author = {Pritchard, J. K. and Stephens, M. and Donnelly, P.},
	date = {2000-06},
	pmid = {10835412},
	pmcid = {PMC1461096},
	keywords = {Algorithms, Cluster Analysis, Genetics, Population, Genotype, Humans, Models, Genetic}
}

@article{grimmer_bayesian_2010,
	title = {A Bayesian Hierarchical Topic Model for Political Texts: Measuring Expressed Agendas in Senate Press Releases},
	volume = {18},
	url = {https://papers.ssrn.com/abstract=1541022},
	shorttitle = {A Bayesian Hierarchical Topic Model for Political Texts},
	abstract = {Political scientists lack methods to efficiently measure the priorities political actors emphasize in statements. To address this limitation, I introduce a statistical model that attends to the structure of political rhetoric when measuring expressed priorities: statements are naturally organized by author. The expressed agenda model exploits this structure to simultaneously estimate the topics in the texts, as well as the attention political actors allocate to the estimated topics. I apply the method to a collection of over 24,000 press releases from senators from 2007, which I demonstrate is an ideal medium to measure how senators explain their work in Washington to constituents. A set of examples validates the estimated priorities and demonstrates their usefulness for testing theories of how members of Congress communicate with constituents. The statistical model and its extensions will be made available in a forthcoming free software package for the R computing language.},
	pages = {1--35},
	number = {1},
	journaltitle = {Political Analysis},
	author = {Grimmer, Justin},
	urldate = {2017-10-07},
	date = {2010},
	keywords = {A Bayesian Hierarchical Topic Model for Political Texts: Measuring Expressed Agendas in Senate Press Releases, Justin  Grimmer, {SSRN}},
	file = {Snapshot:/Users/franzi/Zotero/storage/3QRF5PHE/papers.html:text/html}
}

@incollection{mcauliffe_supervised_2008,
	title = {Supervised Topic Models},
	url = {http://papers.nips.cc/paper/3328-supervised-topic-models.pdf},
	pages = {121--128},
	booktitle = {Advances in Neural Information Processing Systems 20},
	publisher = {Curran Associates, Inc.},
	author = {Mcauliffe, Jon D. and Blei, David M.},
	editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
	date = {2008},
	file = {NIPS Full Text PDF:/Users/franzi/Zotero/storage/AKKVXDQ9/Mcauliffe und Blei - 2008 - Supervised Topic Models.pdf:application/pdf;NIPS Snapshort:/Users/franzi/Zotero/storage/KQV8JHW8/3328-supervised-topic-models.html:text/html}
}

@article{blei_latent_2003,
	title = {Latent dirichlet allocation},
	volume = {3},
	pages = {993--1022},
	journaltitle = {Journal of machine Learning research},
	author = {Blei, David M. and Ng, Andrew Y and Jordan, Michael I},
	date = {2003-01}
}

@article{blei_probabilistic_2012,
	title = {Probabilistic Topic Models},
	volume = {55},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/2133806.2133826},
	doi = {10.1145/2133806.2133826},
	abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
	pages = {77--84},
	number = {4},
	journaltitle = {Commun. {ACM}},
	author = {Blei, David M.},
	date = {2012-04},
	file = {ACM Full Text PDF:/Users/franzi/Zotero/storage/GWUDPWQT/Blei - 2012 - Probabilistic Topic Models.pdf:application/pdf}
}

@report{gentzkow_text_2017,
	title = {Text as Data},
	url = {http://www.nber.org/papers/w23276},
	abstract = {An ever increasing share of human interaction, communication, and culture is recorded as digital text. We provide an introduction to the use of text as an input to economic research. We discuss the features that make text different from other forms of data, offer a practical overview of relevant statistical methods, and survey a variety of applications.},
	number = {23276},
	institution = {National Bureau of Economic Research},
	type = {Working Paper},
	author = {Gentzkow, Matthew and Kelly, Bryan T. and Taddy, Matt},
	date = {2017-03},
	doi = {10.3386/w23276}
}

@article{baker_measuring_2016,
	title = {Measuring Economic Policy Uncertainty},
	url = {http://www.nber.org/papers/w21633},
	abstract = {We develop a new index of economic policy uncertainty ({EPU}) based on newspaper coverage frequency. Several types of evidence – including human readings of 12,000 newspaper articles – indicate that our index proxies for movements in policy-related economic uncertainty. Our {US} index spikes near tight presidential elections, Gulf Wars I and {II}, the 9/11 attacks, the failure of Lehman Brothers, the 2011 debt-ceiling dispute and other major battles over fiscal policy. Using firm-level data, we find that policy uncertainty raises stock price volatility and reduces investment and employment in policy-sensitive sectors like defense, healthcare, and infrastructure construction. At the macro level, policy uncertainty innovations foreshadow declines in investment, output, and employment in the United States and, in a panel {VAR} setting, for 12 major economies. Extending our {US} index back to 1900, {EPU} rose dramatically in the 1930s (from late 1931) and has drifted upwards since the 1960s.},
	pages = {1593--1636},
	number = {4},
	journaltitle = {Quarterly Journal of Economics},
	author = {Baker, Scott R. and Bloom, Nicholas and Davis, Steven J.},
	date = {2016},
	doi = {10.3386/w21633}
}

@article{tetlock_giving_2007,
	title = {Giving Content to Investor Sentiment: The Role of Media in the Stock Market},
	volume = {62},
	issn = {1540-6261},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2007.01232.x/abstract},
	doi = {10.1111/j.1540-6261.2007.01232.x},
	shorttitle = {Giving Content to Investor Sentiment},
	abstract = {I quantitatively measure the interactions between the media and the stock market using daily content from a popular Wall Street Journal column. I find that high media pessimism predicts downward pressure on market prices followed by a reversion to fundamentals, and unusually high or low pessimism predicts high market trading volume. These and similar results are consistent with theoretical models of noise and liquidity traders, and are inconsistent with theories of media content as a proxy for new information about fundamental asset values, as a proxy for market volatility, or as a sideshow with no relationship to asset markets.},
	pages = {1139--1168},
	number = {3},
	journaltitle = {The Journal of Finance},
	author = {Tetlock, Paul C.},
	date = {2007-06-01},
	langid = {english},
	file = {Snapshot:/Users/franzi/Zotero/storage/R4FBIJ83/abstract.html:text/html}
}

@inproceedings{korencic_getting_2015,
	location = {New York, {NY}, {USA}},
	title = {Getting the Agenda Right: Measuring Media Agenda Using Topic Models},
	isbn = {978-1-4503-3784-7},
	url = {http://doi.acm.org/10.1145/2809936.2809942},
	doi = {10.1145/2809936.2809942},
	series = {{TM} '15},
	shorttitle = {Getting the Agenda Right},
	abstract = {Agenda setting is the theory of how issue salience is transferred from the media to media audience. An agenda-setting study requires one to define a set of issues and to measure their salience. We propose a semi-supervised approach based on topic modeling for exploring a news corpus and measuring the media agenda by tagging news articles with issues. The approach relies on an off-the-shelf Latent Dirichlet Allocation topic model, manual labeling of topics, and topic model customization. In preliminary evaluation, the tagger achieves a micro F1-score of 0.85 and outperforms the supervised baselines, suggesting that it could be successfully used for agenda-setting studies.},
	pages = {61--66},
	booktitle = {Proceedings of the 2015 Workshop on Topic Models: Post-Processing and Applications},
	publisher = {{ACM}},
	author = {Korenčić, Damir and Ristov, Strahil and Šnajder, Jan},
	date = {2015},
	keywords = {agenda measuring, agenda setting, document tagging, multilabel classification, news media analysis, topic modeling}
}

@article{evans_economics_2008,
	title = {The Economics of the Online Advertising Industry},
	volume = {7},
	url = {https://ideas.repec.org/a/bpj/rneart/v7y2008i3n2.html},
	abstract = {Internet-based technologies are revolutionizing the stodgy \$625 billion global advertising industry. There are a number of public policy issues to consider. Will a single ad platform emerge or will several remain viable? What are the consequences of alternative market structures for a web economy that is increasingly based on selling eyeballs to advertisers? This article describes the online advertising industry. The industry is populated by a number of multi-sided platforms that facilitate connecting advertisers to viewers. Search-based advertising platforms, the most developed of these, have interesting economic features that result from the combination of keyword bidding by advertisers and single-homing.},
	pages = {1--33},
	number = {3},
	journaltitle = {Review of Network Economics},
	author = {Evans, David S.},
	urldate = {2016-08-11},
	date = {2008},
	file = {RePEc Snapshot:/Users/franzi/Zotero/storage/Z6B7DBRS/v7y2008i3n2.html:text/html}
}

@article{caillaud_chicken_2003,
	title = {Chicken \& Egg: Competition among Intermediation Service Providers},
	volume = {34},
	issn = {0741-6261},
	url = {http://econpapers.repec.org/article/rjerandje/v_3a34_3ay_3a2003_3ai_3a2_3ap_3a309-28.htm},
	shorttitle = {Chicken \& Egg},
	abstract = {We analyze a model of imperfect price competition between intermediation service providers. We insist on features that are relevant for informational intermediation via the Internet: the presence of indirect network externalities, the possibility of using the nonexclusive services of several intermediaries, and the widespread practice of price discrimination based on users' identity and on usage. Efficient market structures emerge in equilibrium, as well as some specific form of inefficient structures. Intermediaries have incentives to propose non-exclusive services, as this moderates competition and allows them to exert market power. We analyze in detail the pricing and business strategies followed by intermediation services providers. Copyright 2003 by the {RAND} Corporation.},
	pages = {309--28},
	number = {2},
	journaltitle = {{RAND} Journal of Economics},
	author = {Caillaud, Bernard and Jullien, Bruno},
	urldate = {2016-10-18},
	date = {2003},
	file = {RePEc Snapshot:/Users/franzi/Zotero/storage/GD5R58G2/v_3a34_3ay_3a2003_3ai_3a2_3ap_3a309-28.html:text/html}
}

@book{box_time_2008,
	location = {Hoboken, {NJ}},
	edition = {4},
	title = {Time series analysis: forecasting and control},
	isbn = {978-0-470-27284-8},
	series = {Wiley series in probability and statistics},
	shorttitle = {Time series analysis},
	pagetotal = {xxiv+746},
	publisher = {Wiley},
	author = {Box, George E. P. and Jenkins, Gwilym M. and Reinsel, Gregory C.},
	date = {2008},
	keywords = {*Time-series analysis / Prediction theory / Transfer functions / Feedback control systems -- Mathematical models, *Zeitreihenanalyse / Kontrolltheorie / Rückkopplung / Mathematisches Modell},
	annotation = {Literaturverz. S. 685 - 6995. Aufl. u.d.T.: Time series analysis / George E.P. Box; Gwilym M. Jenkins; Gregory C. Reinsel; Greta M. Ljung}
}

@thesis{paul_cross-collection_2009,
	title = {Cross-Collection Topic Models: Automatically Comparing and Contrasting Text},
	shorttitle = {Cross-Collection Topic Models},
	abstract = {This paper describes cross-collection latent Dirichlet allocation ({ccLDA}), a probabilistic topic model that captures meaningful word co-occurrences across multiple text collections. The model is applied to three different applications: discovering cultural differences in blogs and forums from different countries, discovering research topics across multiple scientific disciplines, and comparing editorial differences between multiple media sources. A variety of qualitative and quantitative evaluations of {ccLDA} are performed, including log-likelihood measurements and performance measurements of the model used as a generative classifier. Improvements over previous work are demonstrated. Finally, possible extensions and modifications to the model are presented with promising results. 1},
	institution = {University of Illinois at Urbana-Champaign},
	type = {Master Thesis},
	author = {Paul, Michael},
	date = {2009},
	file = {Citeseer - Full Text PDF:/Users/franzi/Zotero/storage/H2I347RL/Paul - Cross-Collection Topic Models Automatically Compa.pdf:application/pdf;Citeseer - Snapshot:/Users/franzi/Zotero/storage/KGFW8BSS/summary.html:text/html}
}

@article{dewenter_can_2018,
	title = {Can Media Drive the Electorate? The Impact of Media Coverage on Party Affiliation and Voting Intentions},
	volume = {179},
	journaltitle = {Working Paper Series, Helmut Schmidt University Hamburg, Department of Economics},
	author = {Dewenter, Ralf and Linder, Melissa and Thomas, Tobias},
	date = {2018-04}
}

@article{luoma_development_2010,
	title = {The development and psychometric properties of a new measure of perceived stigma toward substance users},
	volume = {45},
	issn = {1532-2491},
	doi = {10.3109/10826080902864712},
	abstract = {A self-report measure of perceived stigma toward substance users was developed and studied. An initial measure was created based on a previously developed scale that was rated by experts for content validity and quality of items. The scale, along with other measures, was administered to 252 people in treatment for substance problems in the United States during 2006-2007. Refinement efforts resulted in an eight-item scale with good face validity, construct validity, and adequate levels of internal consistency. Most relationships with other constructs were as expected. Findings suggest that perceived stigma is distinct from other forms of stigma.},
	pages = {47--57},
	number = {1},
	journaltitle = {Substance Use \& Misuse},
	shortjournal = {Subst Use Misuse},
	author = {Luoma, Jason B. and O'Hair, Alyssa K. and Kohlenberg, Barbara S. and Hayes, Steven C. and Fletcher, Lindsay},
	date = {2010},
	pmid = {20025438},
	pmcid = {PMC5067154},
	keywords = {Humans, Adolescent, Adult, Drug Users, Factor Analysis, Statistical, Female, Male, Middle Aged, Psychometrics, Reproducibility of Results, Social Perception, Stereotyping, Substance-Related Disorders}
}

@article{padmaja_evaluating_2014,
	title = {Evaluating Sentiment Analysis Methods and Identifying Scope of Negation in Newspaper Articles},
	volume = {3},
	url = {http://thesai.org/Publications/ViewPaper?Volume=3&Issue=11&Code=IJARAI&SerialNo=1},
	doi = {10.14569/IJARAI.2014.031101},
	abstract = {Automatic detection of linguistic negation in free text is a demanding need for many text processing applications including Sentiment Analysis. Our system uses online news archives from two different resources namely {NDTV} and The Hindu. While dealing with news articles, we performed three subtasks namely identifying the target; separation of good and bad news content from the good and bad sentiment expressed on the target and analysis of clearly marked opinion that is expressed explicitly, not needing interpretation or the use of world knowledge. In this paper, our main focus was on evaluating and comparing three sentiment analysis methods (two machine learning based and one lexical based) and also identifying the scope of negation in news articles for two political parties namely {BJP} and {UPA} by using three existing methodologies. They were Rest of the Sentence ({RoS}), Fixed Window Length ({FWL}) and Dependency Analysis ({DA}). Among the sentiment methods the best F-measure was {SVM} with the values 0.688 and 0.657 for {BJP} and {UPA} respectively. On the other hand, the F measures for {RoS}, {FWL} and {DA} were 0.58, 0.69 and 0.75 respectively. We observed that {DA} was performing better than the other two. Among 1675 sentences in the corpus, according to annotator I, 1,137 were positive and 538 were negative whereas according to annotator {II}, 1,130 were positive and 545 were negative. Further we also identified the score of each sentence and calculated the accuracy on the basis of average score of both the annotators.},
	number = {11},
	journaltitle = {International Journal of Advanced Research in Artificial Intelligence ({IJARAI})},
	author = {Padmaja, S. and Fatima, Prof S. Sameen and Bandu, Sasidhar},
	urldate = {2018-03-19},
	date = {2014},
	langid = {english},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/3NRWA2F2/Padmaja et al. - 2014 - Evaluating Sentiment Analysis Methods and Identify.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/RY3NFDRX/ViewPaper.html:text/html}
}

@inproceedings{godbole_large-scale_2007,
	title = {Large-Scale Sentiment Analysis for News and Blogs (System Demonstration)},
	eventtitle = {Proceedings of the International Conference on Weblogs and Social Media},
	pages = {2},
	author = {Godbole, Namrata and Srinivasaiah, Manjunath and Skiena, Steven},
	date = {2007-01},
	file = {Godbole et al. - Large-Scale Sentiment Analysis for News and Blogs .pdf:/Users/franzi/Zotero/storage/AJPISQHX/Godbole et al. - Large-Scale Sentiment Analysis for News and Blogs .pdf:application/pdf}
}

@article{fu_analyzing_2013,
	title = {Analyzing Online Sentiment to Predict Telephone Poll Results},
	volume = {16},
	issn = {2152-2715, 2152-2723},
	url = {http://online.liebertpub.com/doi/abs/10.1089/cyber.2012.0375},
	doi = {10.1089/cyber.2012.0375},
	abstract = {The telephone survey is a common social science research method for capturing public opinion, for example, an individual’s values or attitudes, or the government’s approval rating. However, reducing domestic landline usage, increasing nonresponse rate, and suffering from response bias of the interviewee’s self-reported data pose methodological challenges to such an approach. Because of the labor cost of administration, a phone survey is often conducted on a biweekly or monthly basis, and therefore a daily reﬂection of public opinion is usually not available. Recently, online sentiment analysis of user-generated content has been deployed to predict public opinion and human behavior. However, its overall effectiveness remains uncertain. This study seeks to examine the temporal association between online sentiment reﬂected in social media content and phone survey poll results in Hong Kong. Speciﬁcally, it aims to ﬁnd the extent to which online sentiment can predict phone survey results. Using autoregressive integrated moving average time-series analysis, this study suggested that online sentiment scores can lead phone survey results by about 8–15 days, and their correlation coefﬁcients were about 0.16. The ﬁnding is signiﬁcant to the study of social media in social science research, because it supports the conclusion that daily sentiment observed in social media content can serve as a leading predictor for phone survey results, keeping as much as 2 weeks ahead of the monthly announcement of opinion polls. We also discuss the practical and theoretical implications of this study.},
	pages = {702--707},
	number = {9},
	journaltitle = {Cyberpsychology, Behavior, and Social Networking},
	author = {Fu, King-wa and Chan, Chee-hon},
	urldate = {2018-03-19},
	date = {2013-09},
	langid = {english},
	file = {Fu und Chan - 2013 - Analyzing Online Sentiment to Predict Telephone Po.pdf:/Users/franzi/Zotero/storage/MPTLP2XU/Fu und Chan - 2013 - Analyzing Online Sentiment to Predict Telephone Po.pdf:application/pdf}
}

@inproceedings{tumasjan_predicting_2010,
	location = {Washington},
	title = {Predicting Elections with Twitter: What 140 Characters Reveal about Political Sentiment},
	url = {https://www.researchgate.net/publication/215776042_Predicting_Elections_with_Twitter_What_140_Characters_Reveal_about_Political_Sentiment},
	shorttitle = {Predicting Elections with Twitter},
	abstract = {Twitter is a microblogging website where users read and write millions of short messages on a variety of topics every day. This study uses the context of the German federal election to investigate whether Twitter is used as a forum for political deliberation and whether online messages on Twitter validly mirror offline political sentiment. Using {LIWC} text analysis software, we conducted a contentanalysis of over 100,000 messages containing a reference to either a political party or a politician. Our results show that Twitter is indeed used extensively for political deliberation. We find that the mere number of messages mentioning a party reflects the election result. Moreover, joint mentions of two parties are in line with real world political ties and coalitions. An analysis of the tweets' political sentiment demonstrates close correspondence to the parties' and politicians' political positions indicating that the content of Twitter messages plausibly reflects the offline political landscape. We discuss the use of microblogging message content as a valid indicator of political sentiment and derive suggestions for further research.},
	eventtitle = {{INTERNATIONAL} {AAAI} {CONFERENCE} {ON} {WEBLOGS} {AND} {SOCIAL} {MEDIA}},
	booktitle = {Proceedings of the Fourth International Conference on Weblogs and Social Media},
	author = {Tumasjan, Andranik and Sprenger, Timm Oliver and Sandner, Philipp and Welpe, Isabell},
	urldate = {2018-03-17},
	date = {2010},
	langid = {english},
	file = {Snapshot:/Users/franzi/Zotero/storage/JHIPIJAW/215776042_Predicting_Elections_with_Twitter_What_140_Characters_Reveal_about_Political_Sentimen.html:text/html}
}

@online{noauthor_media_2014,
	title = {Media Freedom and Pluralism},
	url = {https://ec.europa.eu/digital-single-market/en/policies/media-freedom-and-pluralism},
	abstract = {The importance of transparency, freedom and diversity in Europe's media landscape. The European Commission commits to respect freedom and pluralism of media. In this page you can find several acts, documents and studies on the subject.},
	titleaddon = {Digital Single Market},
	urldate = {2018-03-14},
	date = {2014-02-17},
	langid = {english},
	file = {Snapshot:/Users/franzi/Zotero/storage/23P77YVX/media-freedom-and-pluralism.html:text/html}
}

@article{tetlock_more_2008,
	title = {More Than Words: Quantifying Language to Measure Firms' Fundamentals},
	volume = {63},
	issn = {1540-6261},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2008.01362.x/abstract},
	doi = {10.1111/j.1540-6261.2008.01362.x},
	shorttitle = {More Than Words},
	abstract = {We examine whether a simple quantitative measure of language can be used to predict individual firms' accounting earnings and stock returns. Our three main findings are: (1) the fraction of negative words in firm-specific news stories forecasts low firm earnings; (2) firms' stock prices briefly underreact to the information embedded in negative words; and (3) the earnings and return predictability from negative words is largest for the stories that focus on fundamentals. Together these findings suggest that linguistic media content captures otherwise hard-to-quantify aspects of firms' fundamentals, which investors quickly incorporate into stock prices.},
	pages = {1437--1467},
	number = {3},
	journaltitle = {The Journal of Finance},
	author = {Tetlock, Paul C. and Saar-Tsechansky, Maytal and Macskassy, Sofus},
	urldate = {2018-03-07},
	date = {2008-06-01},
	langid = {english},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/NTZB3TUK/Tetlock et al. - 2008 - More Than Words Quantifying Language to Measure F.pdf:application/pdf}
}

@article{hansen_shocking_2016,
	title = {Shocking language: Understanding the macroeconomic effects of central bank communication},
	volume = {99},
	issn = {0022-1996},
	url = {http://www.sciencedirect.com/science/article/pii/S0022199615001828},
	doi = {10.1016/j.jinteco.2015.12.008},
	series = {38th Annual {NBER} International Seminar on Macroeconomics},
	shorttitle = {Shocking language},
	abstract = {We explore how the multi-dimensional aspects of information released by the {FOMC} has effects on both market and real economic variables. Using tools from computational linguistics, we measure the information released by the {FOMC} on the state of economic conditions, as well as the guidance the {FOMC} provides about future monetary policy decisions. Employing these measures within a {FAVAR} framework, we find that shocks to forward guidance are more important than the {FOMC} communication of current economic conditions in terms of their effects on market and real variables. Nonetheless, neither communication has particularly strong effects on real economic variables.},
	pages = {S114--S133},
	journaltitle = {Journal of International Economics},
	shortjournal = {Journal of International Economics},
	author = {Hansen, Stephen and {McMahon}, Michael},
	urldate = {2018-03-07},
	date = {2016-03-01},
	keywords = {Communication, Monetary policy, Vector autoregression},
	file = {ScienceDirect Full Text PDF:/Users/franzi/Zotero/storage/AINER6IR/Hansen und McMahon - 2016 - Shocking language Understanding the macroeconomic.pdf:application/pdf;ScienceDirect Snapshot:/Users/franzi/Zotero/storage/39TT2SW9/S0022199615001828.html:text/html}
}

@article{tetlock_giving_2007-1,
	title = {Giving Content to Investor Sentiment: The Role of Media in the Stock Market},
	volume = {62},
	issn = {1540-6261},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2007.01232.x/abstract},
	doi = {10.1111/j.1540-6261.2007.01232.x},
	shorttitle = {Giving Content to Investor Sentiment},
	abstract = {I quantitatively measure the interactions between the media and the stock market using daily content from a popular Wall Street Journal column. I find that high media pessimism predicts downward pressure on market prices followed by a reversion to fundamentals, and unusually high or low pessimism predicts high market trading volume. These and similar results are consistent with theoretical models of noise and liquidity traders, and are inconsistent with theories of media content as a proxy for new information about fundamental asset values, as a proxy for market volatility, or as a sideshow with no relationship to asset markets.},
	pages = {1139--1168},
	number = {3},
	journaltitle = {The Journal of Finance},
	author = {Tetlock, Paul C.},
	urldate = {2018-03-07},
	date = {2007-06-01},
	langid = {english},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/RHRD6AQT/Tetlock - 2007 - Giving Content to Investor Sentiment The Role of .pdf:application/pdf}
}

@article{paul_qualitative_2017,
	title = {Qualitative and quantitative central bank communication and inflation expectations},
	volume = {17},
	url = {https://ideas.repec.org/a/bpj/bejmac/v17y2017i1p41n7.html},
	abstract = {We aim to investigate the simultaneous and interacted effects of central bank qualitative and quantitative communication on private inflation expectations, measured with survey and market-based measures. The effects of {ECB} inflation projections and Governing Council members’ speeches are identified through an instrumental-variables estimation using a principal component analysis to generate relevant instruments. We find that {ECB} projections have a positive effect on current-year forecasts, and that {ECB} projections and speeches are substitutes at longer horizons. Moreover, {ECB} speeches and the {ECB} rate reinforce the effect of {ECB} projections when they are consistent, and convey the same signal about inflationary pressures.},
	pages = {1--41},
	number = {1},
	journaltitle = {The B.E. Journal of Macroeconomics},
	author = {Paul, Hubert},
	urldate = {2018-03-07},
	date = {2017},
	langid = {english},
	keywords = {central bank communication, European central bank, monetary policy, principal component analysis},
	file = {Snapshot:/Users/franzi/Zotero/storage/55VNARXK/v17y2017i1p41n7.html:text/html}
}

@book{cage_information_2017,
	title = {L'information à tout prix},
	publisher = {Éditions de l’{INA}},
	author = {Cagé, Julia and Hervé, Nicolas and Viaud, Marie-Luce},
	date = {2017},
	langid = {french},
	file = {Snapshot:/Users/franzi/Zotero/storage/E5Z7FALC/Lire-L-information-a-tout-prix-de-Julia-Cage.html:text/html}
}

@article{nyman_news_2018,
	title = {News and narratives in financial systems: exploiting big data for systemic risk assessment {\textbar} Bank of England},
	volume = {704},
	url = {https://www.bankofengland.co.uk/working-paper/2018/news-and-narratives-in-financial-systems},
	journaltitle = {Bank of England Working Paper},
	author = {Nyman, Rickard and Kapadia, Sujit and Tuckett, David and Gregory, David and Ormerod, Paul and Smith, Robert},
	urldate = {2018-02-21},
	date = {2018-05-01},
	file = {News and narratives in financial systems\: exploiting big data for systemic risk assessment | Bank of England:/Users/franzi/Zotero/storage/BK83G72M/news-and-narratives-in-financial-systems.html:text/html}
}

@article{egami_how_2017,
	title = {How to Make Causal Inferences Using Texts},
	journaltitle = {Working Paper},
	author = {Egami, Naoki and Fong, Christian J. and Grimmer, Justin and Roberts, Margaret E. and Stewart, Brandon M.},
	date = {2017-12-13},
	file = {How to Make Causal Inferences Using Texts | Naoki Egami:/Users/franzi/Zotero/storage/LVA68CVK/how-make-causal-inferences-using-texts.html:text/html}
}

@book{ramage_labeled_2009,
	title = {Labeled {LDA}: A supervised topic model for credit attribution in multi-labeled corpora},
	shorttitle = {Labeled {LDA}},
	abstract = {A significant portion of the world’s text is tagged by readers on social bookmarking websites. Credit attribution is an inherent problem in these corpora because most pages have multiple tags, but the tags do not always apply with equal specificity across the whole document. Solving the credit attribution problem requires associating each word in a document with the most appropriate tags and vice versa. This paper introduces Labeled {LDA}, a topic model that constrains Latent Dirichlet Allocation by defining a one-to-one correspondence between {LDA}’s latent topics and user tags. This allows Labeled {LDA} to directly learn word-tag correspondences. We demonstrate Labeled {LDA}’s improved expressiveness over traditional {LDA} with visualizations of a corpus of tagged web pages from del.icio.us. Labeled {LDA} outperforms {SVMs} by more than 3 to 1 when extracting tag-specific document snippets. As a multi-label text classifier, our model is competitive with a discriminative baseline on a variety of datasets. 1},
	author = {Ramage, Daniel and Hall, David and Nallapati, Ramesh and Manning, Christopher D.},
	date = {2009},
	file = {Citeseer - Full Text PDF:/Users/franzi/Zotero/storage/WD2TZ58J/Ramage et al. - Labeled LDA A supervised topic model for credit a.pdf:application/pdf;Citeseer - Snapshot:/Users/franzi/Zotero/storage/W5DGAHT5/summary.html:text/html}
}

@inproceedings{chaney_visualizing_2012,
	title = {Visualizing Topic Models},
	rights = {Authors who publish a paper in this conference agree to the following terms:    1. Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence ({AAAI}), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.    2. The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.    3. The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify {AAAI}, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense {AAAI} may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to {AAAI} in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.    4. Author(s) retain all proprietary rights other than copyright (such as patent rights).    5. Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.    6. Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that {AAAI} copyright and the source are indicated, and that the copies are not used in a way that implies {AAAI} endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the {AAAI} electronic server, and shall not post other {AAAI} copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without {AAAI}’s written permission.    7. Author(s) may make limited distribution of all or portions of their article/paper prior to publication.    8. In the case of work performed under U.S. Government contract, {AAAI} grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.    9. In the event the above article/paper is not accepted and published by {AAAI}, or is withdrawn by the author(s) before acceptance by {AAAI}, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/ICWSM/ICWSM12/paper/view/4645},
	abstract = {Managing large collections of documents is an important problem for many areas of science, industry, and culture. Probabilistic topic modeling offers a promising solution. Topic modeling is an unsupervised machine learning method that learns the underlying themes in a large collection of otherwise unorganized documents. This discovered structure summarizes and organizes the documents. However, topic models are high-level statistical tools\&mdash;a user must scrutinize numerical distributions to understand and explore their results. In this paper, we present a method for visualizing topic models. Our method creates a navigator of the documents, allowing users to explore the hidden structure that a topic model discovers. These browsing interfaces reveal meaningful patterns in a collection, helping end-users explore and understand its contents in new ways. We provide open source software of our method.},
	eventtitle = {Sixth International {AAAI} Conference on Weblogs and Social Media},
	booktitle = {Sixth International {AAAI} Conference on Weblogs and Social Media},
	author = {Chaney, Allison June-Barlow and Blei, David M.},
	urldate = {2018-01-23},
	date = {2012-05-20},
	langid = {english},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/968W9M2F/Chaney und Blei - 2012 - Visualizing Topic Models.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/DXRG63IG/4645.html:text/html}
}

@inproceedings{wang_mining_2009,
	location = {New York, {NY}, {USA}},
	title = {Mining Common Topics from Multiple Asynchronous Text Streams},
	isbn = {978-1-60558-390-7},
	url = {http://doi.acm.org/10.1145/1498759.1498826},
	doi = {10.1145/1498759.1498826},
	series = {{WSDM} '09},
	abstract = {Text streams are becoming more and more ubiquitous, in the forms of news feeds, weblog archives and so on, which result in a large volume of data. An effective way to explore the semantic as well as temporal information in text streams is topic mining, which can further facilitate other knowledge discovery procedures. In many applications, we are facing multiple text streams which are related to each other and share common topics. The correlation among these streams can provide more meaningful and comprehensive clues for topic mining than those from each individual stream. However, it is nontrivial to explore the correlation with the existence of asynchronism among multiple streams, i.e. documents from different streams about the same topic may have different timestamps, which remains unsolved in the context of topic mining. In this paper, we formally address this problem and put forward a novel algorithm based on the generative topic model. Our algorithm consists of two alternate steps: the first step extracts common topics from multiple streams based on the adjusted timestamps by the second step; the second step adjusts the timestamps of the documents according to the time distribution of the discovered topics by the first step. We perform these two steps alternately and a monotone convergence of our objective function is guaranteed. The effectiveness and advantage of our approach were justified by extensive empirical studies on two real data sets consisting of six research paper streams and two news article streams, respectively.},
	pages = {192--201},
	booktitle = {Proceedings of the Second {ACM} International Conference on Web Search and Data Mining},
	publisher = {{ACM}},
	author = {Wang, Xiang and Zhang, Kai and Jin, Xiaoming and Shen, Dou},
	urldate = {2018-01-23},
	date = {2009},
	keywords = {asynchronous streams, temporal text mining, topic model}
}

@inproceedings{kim_topic_2011,
	title = {Topic Chains for Understanding a News Corpus},
	isbn = {978-3-642-19436-8 978-3-642-19437-5},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-19437-5_13},
	doi = {10.1007/978-3-642-19437-5_13},
	series = {Lecture Notes in Computer Science},
	abstract = {The Web is a great resource and archive of news articles for the world. We present a framework, based on probabilistic topic modeling, for uncovering the meaningful structure and trends of important topics and issues hidden within the news archives on the Web. Central in the framework is a topic chain, a temporal organization of similar topics. We experimented with various topic similarity metrics and present our insights on how best to construct topic chains. We discuss how to interpret the topic chains to understand the news corpus by looking at long-term topics, temporary issues, and shifts of focus in the topic chains. We applied our framework to nine months of Korean Web news corpus and present our findings.},
	eventtitle = {International Conference on Intelligent Text Processing and Computational Linguistics},
	pages = {163--176},
	booktitle = {Computational Linguistics and Intelligent Text Processing},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Kim, Dongwoo and Oh, Alice},
	urldate = {2018-01-23},
	date = {2011-02-20},
	langid = {english}
}

@article{newman_distributed_2009,
	title = {Distributed Algorithms for Topic Models},
	volume = {10},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=1577069.1755845},
	abstract = {We describe distributed algorithms for two widely-used topic models, namely the Latent Dirichlet Allocation ({LDA}) model, and the Hierarchical Dirichet Process ({HDP}) model. In our distributed algorithms the data is partitioned across separate processors and inference is done in a parallel, distributed fashion. We propose two distributed algorithms for {LDA}. The first algorithm is a straightforward mapping of {LDA} to a distributed processor setting. In this algorithm processors concurrently perform Gibbs sampling over local data followed by a global update of topic counts. The algorithm is simple to implement and can be viewed as an approximation to Gibbs-sampled {LDA}. The second version is a model that uses a hierarchical Bayesian extension of {LDA} to directly account for distributed data. This model has a theoretical guarantee of convergence but is more complex to implement than the first algorithm. Our distributed algorithm for {HDP} takes the straightforward mapping approach, and merges newly-created topics either by matching or by topic-id. Using five real-world text corpora we show that distributed learning works well in practice. For both {LDA} and {HDP}, we show that the converged test-data log probability for distributed learning is indistinguishable from that obtained with single-processor learning. Our extensive experimental results include learning topic models for two multi-million document collections using a 1024-processor parallel computer.},
	pages = {1801--1828},
	journaltitle = {J. Mach. Learn. Res.},
	author = {Newman, David and Asuncion, Arthur and Smyth, Padhraic and Welling, Max},
	urldate = {2018-01-23},
	date = {2009-12},
	file = {ACM Full Text PDF:/Users/franzi/Zotero/storage/8WPWT44A/Newman et al. - 2009 - Distributed Algorithms for Topic Models.pdf:application/pdf}
}

@inproceedings{he_detecting_2009,
	location = {New York, {NY}, {USA}},
	title = {Detecting Topic Evolution in Scientific Literature: How Can Citations Help?},
	isbn = {978-1-60558-512-3},
	url = {http://doi.acm.org/10.1145/1645953.1646076},
	doi = {10.1145/1645953.1646076},
	series = {{CIKM} '09},
	shorttitle = {Detecting Topic Evolution in Scientific Literature},
	abstract = {Understanding how topics in scientific literature evolve is an interesting and important problem. Previous work simply models each paper as a bag of words and also considers the impact of authors. However, the impact of one document on another as captured by citations, one important inherent element in scientific literature, has not been considered. In this paper, we address the problem of understanding topic evolution by leveraging citations, and develop citation-aware approaches. We propose an iterative topic evolution learning framework by adapting the Latent Dirichlet Allocation model to the citation network and develop a novel inheritance topic model. We evaluate the effectiveness and efficiency of our approaches and compare with the state of the art approaches on a large collection of more than 650,000 research papers in the last 16 years and the citation network enabled by {CiteSeerX}. The results clearly show that citations can help to understand topic evolution better.},
	pages = {957--966},
	booktitle = {Proceedings of the 18th {ACM} Conference on Information and Knowledge Management},
	publisher = {{ACM}},
	author = {He, Qi and Chen, Bi and Pei, Jian and Qiu, Baojun and Mitra, Prasenjit and Giles, Lee},
	urldate = {2018-01-23},
	date = {2009},
	keywords = {citations, inheritance topic model, topic evolution}
}

@article{endres_new_2003,
	title = {A new metric for probability distributions},
	volume = {49},
	issn = {0018-9448},
	doi = {10.1109/TIT.2003.813506},
	abstract = {We introduce a metric for probability distributions, which is bounded, information-theoretically motivated, and has a natural Bayesian interpretation. The square root of the well-known χ2 distance is an asymptotic approximation to it. Moreover, it is a close relative of the capacitory discrimination and Jensen-Shannon divergence.},
	pages = {1858--1860},
	number = {7},
	journaltitle = {{IEEE} Transactions on Information Theory},
	author = {Endres, D. M. and Schindelin, J. E.},
	date = {2003-07},
	keywords = {Adaptive estimation, Algorithm design and analysis, asymptotic approximation, Bayes methods, Bayesian interpretation, Bayesian methods, bounded information-theoretically motivated metric, capacitory discrimination, Convergence, Gaussian noise, information theory, Iterative algorithms, Jensen-Shannon divergence, probability, Probability distribution, probability distributions, square root, Wavelet analysis, White noise, Writing, χ2 distance},
	file = {IEEE Xplore Abstract Record:/Users/franzi/Zotero/storage/H6L3EIL4/1207388.html:text/html}
}

@inproceedings{bischof_summarizing_2012,
	location = {{USA}},
	title = {Summarizing Topical Content with Word Frequency and Exclusivity},
	isbn = {978-1-4503-1285-1},
	url = {http://dl.acm.org/citation.cfm?id=3042573.3042578},
	series = {{ICML}'12},
	abstract = {Recent work in text analysis commonly describes topics in terms of their most frequent words, but the exclusivity of words to topics is equally important for communicating content. We introduce Hierarchical Poisson Convolution ({HPC}), a model which infers regularized estimates of the differential use of words across topics as well as their frequency within topics. {HPC} uses known hierarchical structure on human-labeled topics to make focused comparisons of differential usage within each branch of the hierarchy of labels. We then infer a summary for each topic in terms of words that are both frequent and exclusive. We develop a parallelized Hamiltonian Monte Carlo sampler that allows for fast and scalable computation.},
	pages = {9--16},
	booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
	publisher = {Omnipress},
	author = {Bischof, Jonathan M. and Airoldi, Edoardo M.},
	urldate = {2018-01-19},
	date = {2012}
}

@article{braun_variational_2010,
	title = {Variational inference for large-scale models of discrete choice},
	volume = {105},
	issn = {0162-1459, 1537-274X},
	url = {http://arxiv.org/abs/0712.2526},
	doi = {10.1198/jasa.2009.tm08030},
	abstract = {Discrete choice models are commonly used by applied statisticians in numerous fields, such as marketing, economics, finance, and operations research. When agents in discrete choice models are assumed to have differing preferences, exact inference is often intractable. Markov chain Monte Carlo techniques make approximate inference possible, but the computational cost is prohibitive on the large data sets now becoming routinely available. Variational methods provide a deterministic alternative for approximation of the posterior distribution. We derive variational procedures for empirical Bayes and fully Bayesian inference in the mixed multinomial logit model of discrete choice. The algorithms require only that we solve a sequence of unconstrained optimization problems, which are shown to be convex. Extensive simulations demonstrate that variational methods achieve accuracy competitive with Markov chain Monte Carlo, at a small fraction of the computational cost. Thus, variational methods permit inferences on data sets that otherwise could not be analyzed without bias-inducing modifications to the underlying model.},
	pages = {324--335},
	number = {489},
	journaltitle = {Journal of the American Statistical Association},
	author = {Braun, Michael and {McAuliffe}, Jon},
	urldate = {2018-01-19},
	date = {2010-03},
	eprinttype = {arxiv},
	eprint = {0712.2526},
	keywords = {Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
	annotation = {Comment: 29 pages, 2 tables, 2 figures},
	file = {arXiv\:0712.2526 PDF:/Users/franzi/Zotero/storage/NWRMRPXH/Braun und McAuliffe - 2010 - Variational inference for large-scale models of di.pdf:application/pdf;arXiv.org Snapshot:/Users/franzi/Zotero/storage/V373IACQ/0712.html:text/html}
}

@article{erosheva_mixed-membership_2004,
	title = {Mixed-membership models of scientific publications},
	volume = {101},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/101/suppl_1/5220},
	doi = {10.1073/pnas.0307760101},
	abstract = {{PNAS} is one of world's most cited multidisciplinary scientific journals. The {PNAS} official classification structure of subjects is reflected in topic labels submitted by the authors of articles, largely related to traditionally established disciplines. These include broad field classifications into physical sciences, biological sciences, social sciences, and further subtopic classifications within the fields. Focusing on biological sciences, we explore an internal soft-classification structure of articles based only on semantic decompositions of abstracts and bibliographies and compare it with the formal discipline classifications. Our model assumes that there is a fixed number of internal categories, each characterized by multinomial distributions over words (in abstracts) and references (in bibliographies). Soft classification for each article is based on proportions of the article's content coming from each category. We discuss the appropriateness of the model for the {PNAS} database as well as other features of the data relevant to soft classification.},
	pages = {5220--5227},
	issue = {suppl 1},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Erosheva, Elena and Fienberg, Stephen and Lafferty, John},
	urldate = {2018-01-19},
	date = {2004-04-06},
	langid = {english},
	pmid = {15020766},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/EXNI8WAK/Erosheva et al. - 2004 - Mixed-membership models of scientific publications.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/CXECBUCC/5220.html:text/html}
}

@article{pritchard_association_2000,
	title = {Association Mapping in Structured Populations},
	volume = {67},
	issn = {0002-9297},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1287075/},
	abstract = {The use, in association studies, of the forthcoming dense genomewide collection of single-nucleotide polymorphisms ({SNPs}) has been heralded as a potential breakthrough in the study of the genetic basis of common complex disorders. A serious problem with association mapping is that population structure can lead to spurious associations between a candidate marker and a phenotype. One common solution has been to abandon case-control studies in favor of family-based tests of association, such as the transmission/disequilibrium test ({TDT}), but this comes at a considerable cost in the need to collect {DNA} from close relatives of affected individuals. In this article we describe a novel, statistically valid, method for case-control association studies in structured populations. Our method uses a set of unlinked genetic markers to infer details of population structure, and to estimate the ancestry of sampled individuals, before using this information to test for associations within subpopulations. It provides power comparable with the {TDT} in many settings and may substantially outperform it if there are conflicting associations in different subpopulations.},
	pages = {170--181},
	number = {1},
	journaltitle = {American Journal of Human Genetics},
	shortjournal = {Am J Hum Genet},
	author = {Pritchard, Jonathan K. and Stephens, Matthew and Rosenberg, Noah A. and Donnelly, Peter},
	urldate = {2018-01-19},
	date = {2000-07},
	pmid = {10827107},
	pmcid = {PMC1287075},
	file = {PubMed Central Full Text PDF:/Users/franzi/Zotero/storage/5C5QX5GU/Pritchard et al. - 2000 - Association Mapping in Structured Populations.pdf:application/pdf}
}

@article{shapiro_measuring_2017,
	title = {Measuring News Sentiment},
	url = {https://econpapers.repec.org/paper/fipfedfwp/2017-01.htm},
	abstract = {We develop and assess new time series measures of economic sentiment based on computational text analysis of economic and financial newspaper articles from January 1980 to April 2015. The text analysis is based on predictive models estimated using machine learning techniques from Kanjoya. We analyze four alternative news sentiment indexes. We find that the news sentiment indexes correlate strongly with contemporaneous business cycle indicators. We also find that innovations to news sentiment predict future economic activity. Furthermore, in most cases, the news sentiment measures outperform the University of Michigan and Conference board measures in predicting the federal funds rate, consumption, employment, inflation, industrial production, and the S\&P500. For some of these economic outcomes, there is evidence that the news sentiment measures have significant predictive power even after conditioning on these survey-based measures.},
	number = {2017},
	journaltitle = {Federal Reserve Bank of San Francisco Working Paper},
	author = {Shapiro, Adam and Sudhof, Moritz and Wilson, Daniel},
	urldate = {2018-01-11},
	date = {2017-01-05},
	file = {RePEc PDF:/Users/franzi/Zotero/storage/TB95H8YV/Shapiro et al. - 2017 - Measuring News Sentiment.pdf:application/pdf;RePEc Snapshot:/Users/franzi/Zotero/storage/CJ57433M/2017-01.html:text/html}
}

@article{loughran_when_2011,
	title = {When Is a Liability Not a Liability? Textual Analysis, Dictionaries, and 10-Ks},
	volume = {66},
	issn = {1540-6261},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2010.01625.x/abstract},
	doi = {10.1111/j.1540-6261.2010.01625.x},
	shorttitle = {When Is a Liability Not a Liability?},
	abstract = {Previous research uses negative word counts to measure the tone of a text. We show that word lists developed for other disciplines misclassify common words in financial text. In a large sample of 10-Ks during 1994 to 2008, almost three-fourths of the words identified as negative by the widely used Harvard Dictionary are words typically not considered negative in financial contexts. We develop an alternative negative word list, along with five other word lists, that better reflect tone in financial text. We link the word lists to 10-K filing returns, trading volume, return volatility, fraud, material weakness, and unexpected earnings.},
	pages = {35--65},
	number = {1},
	journaltitle = {The Journal of Finance},
	author = {Loughran, Tim and Mcdonald, Bill},
	urldate = {2018-01-11},
	date = {2011-02-01},
	langid = {english},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/ZFIUBAP9/Loughran und Mcdonald - 2011 - When Is a Liability Not a Liability Textual Analy.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/TE73EQLZ/abstract.html:text/html}
}

@article{samuelson_aspects_1958,
	title = {Aspects of Public Expenditure Theories},
	volume = {40},
	issn = {0034-6535},
	url = {http://www.jstor.org/stable/1926336},
	doi = {10.2307/1926336},
	pages = {332--338},
	number = {4},
	journaltitle = {The Review of Economics and Statistics},
	author = {Samuelson, Paul A.},
	date = {1958}
}

@article{gabszewicz_press_2001,
	title = {Press advertising and the ascent of the ‘Pensée Unique’},
	volume = {45},
	issn = {0014-2921},
	url = {http://www.sciencedirect.com/science/article/pii/S0014292101001398},
	doi = {10.1016/S0014-2921(01)00139-8},
	series = {15th Annual Congress of the European Economic Association},
	abstract = {The press industry depends in a crucial way on the possibility of financing an important fraction of its activities by advertising receipts. We show that this induces the editors of newspapers to moderate, in several cases, the political message they display to their readers, compared with the political opinions they would have expressed otherwise. To this end, we consider a three-stage game in which editors select sequentially their political image, the price of their newspaper and the advertising tariff they oppose to the advertisers. The intuition of the result lies in the fact that editors have to sell tasteless political messages to their readers in order to sell a larger audience to the advertisers.},
	pages = {641--651},
	number = {4},
	journaltitle = {European Economic Review},
	shortjournal = {European Economic Review},
	author = {Gabszewicz, Jean J. and Laussel, Dider and Sonnac, Nathalie},
	date = {2001-05-01},
	keywords = {advertising, Channels-program diversity, {TV}-broadcasting},
	file = {ScienceDirect Full Text PDF:/Users/franzi/Zotero/storage/8RHUZD5J/Gabszewicz et al. - 2001 - Press advertising and the ascent of the ‘Pensée Un.pdf:application/pdf;ScienceDirect Snapshot:/Users/franzi/Zotero/storage/JH6EJC28/S0014292101001398.html:text/html}
}

@article{steiner_program_1952,
	title = {Program Patterns and Preferences, and the Workability of Competition in Radio Broadcasting},
	volume = {66},
	issn = {0033-5533},
	url = {http://www.jstor.org/stable/1882942},
	doi = {10.2307/1882942},
	abstract = {I. Criteria for the appraisal of workability, 195.--{II}. The one period model, 197.--{III}. The model over time, 207.--{IV}. Relevance of the model to the market structure of the industry, 217.--V. Some suggestions for further analysis, 222.},
	pages = {194--223},
	number = {2},
	journaltitle = {The Quarterly Journal of Economics},
	author = {Steiner, Peter O.},
	date = {1952}
}

@article{arora_practical_2012,
	title = {A Practical Algorithm for Topic Modeling with Provable Guarantees},
	url = {https://arxiv.org/abs/1212.4777},
	author = {Arora, Sanjeev and Ge, Rong and Halpern, Yoni and Mimno, David and Moitra, Ankur and Sontag, David and Wu, Yichen and Zhu, Michael},
	urldate = {2017-12-07},
	date = {2012-12-19},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/KR8KRPIX/Arora et al. - 2012 - A Practical Algorithm for Topic Modeling with Prov.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/P63RKV2V/1212.html:text/html}
}

@report{evans_industrial_2005,
	title = {The Industrial Organization of Markets with Two-Sided Platforms},
	url = {http://www.nber.org/papers/w11603},
	abstract = {Two-sided platforms (2SPs) cater to two or more distinct groups of customers, facilitating value-creating interactions between them. The village market and the village matchmaker were 2SPs; {eBay} and Match.com are more recent examples. Other examples include payment card systems, magazines, shopping malls, and personal computer operating systems. Building on the seminal work of Rochet and Tirole (2003), a rapidly growing literature has illuminated the economic principles that apply to 2SPs generally. One key result is that 2SPs may find it profit-maximizing to charge prices for one customer group that are below marginal cost or even negative, and such skewed pricing pattern is prevalent, although not universal, in industries that appear to be based on 2SPs. Over the years, courts have also recognized that certain industries, notably payment card systems and newspapers, now understood to be based on 2SPs, are governed by unusual economic relationships. This chapter provides an introduction to the economics of 2SPs and its application to several competition policy issues.},
	number = {11603},
	institution = {National Bureau of Economic Research},
	type = {Working Paper},
	author = {Evans, David S. and Schmalensee, Richard},
	date = {2005-09},
	doi = {10.3386/w11603},
	file = {NBER Full Text PDF:/Users/franzi/Zotero/storage/EWJINGI8/Evans und Schmalensee - 2005 - The Industrial Organization of Markets with Two-Si.pdf:application/pdf}
}

@incollection{boogaart_linear_2013,
	title = {Linear Models for Compositions},
	isbn = {978-3-642-36808-0 978-3-642-36809-7},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-36809-7_5},
	series = {Use R!},
	abstract = {Compositions can play the role of dependent and independent variables in linear models. In both cases, the parameters of the linear models are again compositions of the same simplex as the data. Most methods for classical linear models have a close analog in these compositional linear models. This chapter addresses several questions on this subject. What are compositional linear models? How to visualize the dependence of compositions, already multivariable, with further external covariables? How to model and check such dependence with compositional linear models? What are the underlying assumptions? How can we check these assumptions? What is the compositional interpretation of the results? How to use linear models to provide statistical evidence with tests, confidence intervals, and predictive regions? How to visualize model results and model parameters? How to compare compositional linear models and how to find the most appropriate one?},
	pages = {95--175},
	booktitle = {Analyzing Compositional Data with R},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Boogaart, K. Gerald van den and Tolosana-Delgado, Raimon},
	urldate = {2017-11-29},
	date = {2013},
	langid = {english},
	doi = {10.1007/978-3-642-36809-7_5},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/XXRU4BZK/Boogaart und Tolosana-Delgado - 2013 - Linear Models for Compositions.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/66BBBZV2/978-3-642-36809-7_5.html:text/html}
}

@book{wiedmann_text_2016,
	location = {Wiesbaden},
	edition = {1},
	title = {Text Mining for Qualitative Data Analysis in the Social Sciences},
	url = {//www.springer.com/de/book/9783658153083},
	publisher = {{VS} Verlag für Sozialwissenschaften},
	author = {Wiedmann, Gregor},
	urldate = {2017-11-26},
	date = {2016},
	file = {Snapshot:/Users/franzi/Zotero/storage/7P4K4CSX/9783658153083.html:text/html}
}

@inproceedings{roberts_structural_2013,
	title = {The Structural Topic Model and Applied Social Science},
	booktitle = {Advances in Neural Information Processing Systems Workshop on Topic Models: Computation, Application, and Evaluation.},
	author = {Roberts, Margaret and Stewart, Brandon and Tingley, Dustin and Airoldi, Edoardo},
	date = {2013},
	file = {The Structural Topic Model and Applied Social Science | Brandon Stewart:/Users/franzi/Zotero/storage/69G2KU2F/structural-topic-model-and-applied-social-science.html:text/html}
}

@article{ellman_what_2009,
	title = {What do the Papers Sell? A Model of Advertising and Media Bias*},
	volume = {119},
	issn = {1468-0297},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1468-0297.2009.02218.x/abstract},
	doi = {10.1111/j.1468-0297.2009.02218.x},
	shorttitle = {What do the Papers Sell?},
	abstract = {We model the market for news as a two-sided market where newspapers sell news to readers who value accuracy and sell space to advertisers who value advert-receptive readers. In this setting, monopolistic newspapers under-report or bias news that sufficiently reduces advertiser profits. Paradoxically, increasing the size of advertising eventually leads competing newspapers to reduce advertiser bias. Nonetheless, advertisers can counter this effect if able to commit to news-sensitive cut-off strategies, potentially inducing as much bias as in the monopoly case. We use these results to explain contrasting historical and recent evidence on commercial bias and influence in the media.},
	pages = {680--704},
	number = {537},
	journaltitle = {The Economic Journal},
	author = {Ellman, Matthew and Germano, Fabrizio},
	date = {2009-04-01},
	langid = {english},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/ESUGMQCA/Ellman und Germano - 2009 - What do the Papers Sell A Model of Advertising an.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/E87BHVVH/abstract.html:text/html}
}

@article{evans_empirical_2003,
	title = {Some Empirical Aspects of Multi-sided Platform Industries},
	volume = {2},
	issn = {1446-9022},
	url = {https://www.degruyter.com/view/j/rne.2003.2.issue-3/rne.2003.2.3.1026/rne.2003.2.3.1026.xml},
	doi = {10.2202/1446-9022.1026},
	abstract = {Multi-sided platform markets have two or more different groups of customers that businesses have to get and keep on board to succeed. These industries range from dating clubs (men and women), to video game consoles (game developers and users), to payment cards (cardholders and merchants), to operating system software (application developers and users). They include some of the most important industries in the economy. A survey of businesses in these industries shows that multi-sided platform businesses devise entry strategies to get multiple sides of the market on board and devise pricing, product, and other competitive strategies to keep multiple customer groups on a common platform that internalizes externalities across members of these groups.},
	number = {3},
	journaltitle = {Review of Network Economics},
	author = {Evans, David S.},
	date = {2003},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/JXJD8ZCR/Evans - 2003 - Some Empirical Aspects of Multi-sided Platform Ind.pdf:application/pdf}
}

@article{blair_pricing_1993,
	title = {Pricing Decisions of the Newspaper Monopolist},
	volume = {59},
	issn = {0038-4038},
	url = {http://www.jstor.org/stable/1059734},
	doi = {10.2307/1059734},
	pages = {721--732},
	number = {4},
	journaltitle = {Southern Economic Journal},
	author = {Blair, Roger D. and Romano, Richard E.},
	date = {1993}
}

@article{gustafsson_circulation_1978,
	title = {The circulation spiral and the principle of household coverage},
	volume = {26},
	issn = {0358-5522},
	url = {https://doi.org/10.1080/03585522.1978.10407893},
	doi = {10.1080/03585522.1978.10407893},
	abstract = {The growth of oligopoly within the newspaper industry is a widespread phenomenon which has been examined by both researchers into the mass media and public enquiries into the press in a number of countries. Politicians recognise the development and want to modify the process of concentration, prevent newspaper closures, and even promote new ventures. Many western countries have taken measures to try to control forces bearing toward concentration in the newspaper industry. Such efforts, however, require a thorough knowledge of the market and its mechanism.},
	pages = {1--14},
	number = {1},
	journaltitle = {Scandinavian Economic History Review},
	author = {Gustafsson, Karl Erik},
	date = {1978-01-01},
	file = {Snapshot:/Users/franzi/Zotero/storage/D8TJWJWK/03585522.1978.html:text/html}
}

@article{corden_maximisation_1952,
	title = {The Maximisation of Profit by a Newspaper},
	volume = {20},
	issn = {0034-6527},
	url = {http://www.jstor.org/stable/2295888},
	doi = {10.2307/2295888},
	pages = {181--190},
	number = {3},
	journaltitle = {The Review of Economic Studies},
	author = {Corden, W. M.},
	date = {1952}
}

@article{deerwester_indexing_1990,
	title = {Indexing by latent semantic analysis},
	volume = {41},
	abstract = {A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 or-thogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are re-turned. initial tests find this completely automatic method for retrieval to be promising.},
	pages = {391--407},
	number = {6},
	journaltitle = {Journal of the American Society for Information Science},
	author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
	date = {1990},
	file = {Citeseer - Full Text PDF:/Users/franzi/Zotero/storage/EVGTHKF3/Deerwester et al. - 1990 - Indexing by latent semantic analysis.pdf:application/pdf;Citeseer - Snapshot:/Users/franzi/Zotero/storage/5KT8CZXC/summary.html:text/html}
}

@inproceedings{hofmann_probabilistic_1999,
	location = {New York, {NY}, {USA}},
	title = {Probabilistic Latent Semantic Indexing},
	isbn = {978-1-58113-096-6},
	url = {http://doi.acm.org/10.1145/312624.312649},
	doi = {10.1145/312624.312649},
	series = {{SIGIR} '99},
	pages = {50--57},
	booktitle = {Proceedings of the 22Nd Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Hofmann, Thomas},
	date = {1999}
}

@article{asuncion_smoothing_2012,
	title = {On Smoothing and Inference for Topic Models},
	url = {http://arxiv.org/abs/1205.2662},
	abstract = {Latent Dirichlet analysis, or topic modeling, is a flexible latent variable framework for modeling high-dimensional sparse count data. Various learning algorithms have been developed in recent years, including collapsed Gibbs sampling, variational inference, and maximum a posteriori estimation, and this variety motivates the need for careful empirical comparisons. In this paper, we highlight the close connections between these approaches. We find that the main differences are attributable to the amount of smoothing applied to the counts. When the hyperparameters are optimized, the differences in performance among the algorithms diminish significantly. The ability of these algorithms to achieve solutions of comparable accuracy gives us the freedom to select computationally efficient approaches. Using the insights gained from this comparative study, we show how accurate topic models can be learned in several seconds on text corpora with thousands of documents.},
	journaltitle = {{arXiv}:1205.2662 [cs, stat]},
	author = {Asuncion, Arthur and Welling, Max and Smyth, Padhraic and Teh, Yee Whye},
	date = {2012-05-09},
	eprinttype = {arxiv},
	eprint = {1205.2662},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	annotation = {Comment: Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence ({UAI}2009)},
	file = {arXiv\:1205.2662 PDF:/Users/franzi/Zotero/storage/MRRAS29S/Asuncion et al. - 2012 - On Smoothing and Inference for Topic Models.pdf:application/pdf;arXiv.org Snapshot:/Users/franzi/Zotero/storage/UHH4ICFA/1205.html:text/html}
}

@incollection{wallach_rethinking_2009,
	title = {Rethinking {LDA}: Why Priors Matter},
	url = {http://papers.nips.cc/paper/3854-rethinking-lda-why-priors-matter.pdf},
	shorttitle = {Rethinking {LDA}},
	pages = {1973--1981},
	booktitle = {Advances in Neural Information Processing Systems 22},
	publisher = {Curran Associates, Inc.},
	author = {Wallach, Hanna M. and Mimno, David M. and {McCallum}, Andrew},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
	date = {2009},
	file = {NIPS Full Text PDF:/Users/franzi/Zotero/storage/TPHKVI25/Wallach et al. - 2009 - Rethinking LDA Why Priors Matter.pdf:application/pdf;NIPS Snapshort:/Users/franzi/Zotero/storage/6FVP9RXD/3854-rethinking-lda-why-priors-matter.html:text/html}
}

@inproceedings{newman_automatic_2010,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Automatic Evaluation of Topic Coherence},
	isbn = {978-1-932432-65-7},
	url = {http://dl.acm.org/citation.cfm?id=1857999.1858011},
	series = {{HLT} '10},
	abstract = {This paper introduces the novel task of topic coherence evaluation, whereby a set of words, as generated by a topic model, is rated for coherence or interpretability. We apply a range of topic scoring models to the evaluation task, drawing on {WordNet}, Wikipedia and the Google search engine, and existing research on lexical similarity/relatedness. In comparison with human scores for a set of learned topics over two distinct datasets, we show a simple co-occurrence measure based on pointwise mutual information over Wikipedia data is able to achieve results for the task at or nearing the level of inter-annotator correlation, and that other Wikipedia-based lexical relatedness methods also achieve strong results. Google produces strong, if less consistent, results, while our results over {WordNet} are patchy at best.},
	pages = {100--108},
	booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Newman, David and Lau, Jey Han and Grieser, Karl and Baldwin, Timothy},
	date = {2010},
	file = {ACM Full Text PDF:/Users/franzi/Zotero/storage/QJIP655P/Newman et al. - 2010 - Automatic Evaluation of Topic Coherence.pdf:application/pdf}
}

@inproceedings{mimno_bayesian_2011,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Bayesian Checking for Topic Models},
	isbn = {978-1-937284-11-4},
	url = {http://dl.acm.org/citation.cfm?id=2145432.2145459},
	series = {{EMNLP} '11},
	abstract = {Real document collections do not fit the independence assumptions asserted by most statistical topic models, but how badly do they violate them? We present a Bayesian method for measuring how well a topic model fits a corpus. Our approach is based on posterior predictive checking, a method for diagnosing Bayesian models in user-defined ways. Our method can identify where a topic model fits the data, where it falls short, and in which directions it might be improved.},
	pages = {227--237},
	booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Mimno, David and Blei, David},
	date = {2011},
	file = {ACM Full Text PDF:/Users/franzi/Zotero/storage/MFUGSEJM/Mimno und Blei - 2011 - Bayesian Checking for Topic Models.pdf:application/pdf}
}

@inproceedings{mimno_optimizing_2011,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Optimizing Semantic Coherence in Topic Models},
	isbn = {978-1-937284-11-4},
	url = {http://dl.acm.org/citation.cfm?id=2145432.2145462},
	series = {{EMNLP} '11},
	abstract = {Latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces. In order for people to use such models, however, they must trust them. Unfortunately, typical dimensionality reduction methods for text, such as latent Dirichlet allocation, often produce low-dimensional subspaces (topics) that are obviously flawed to human domain experts. The contributions of this paper are threefold: (1) An analysis of the ways in which topics can be flawed; (2) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data; (3) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the National Institutes of Health ({NIH}).},
	pages = {262--272},
	booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Mimno, David and Wallach, Hanna M. and Talley, Edmund and Leenders, Miriam and {McCallum}, Andrew},
	date = {2011},
	file = {ACM Full Text PDF:/Users/franzi/Zotero/storage/8ATB9444/Mimno et al. - 2011 - Optimizing Semantic Coherence in Topic Models.pdf:application/pdf}
}

@inproceedings{alsumait_topic_2009,
	title = {Topic Significance Ranking of {LDA} Generative Models},
	isbn = {978-3-642-04179-2 978-3-642-04180-8},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-04180-8_22},
	doi = {10.1007/978-3-642-04180-8_22},
	series = {Lecture Notes in Computer Science},
	abstract = {Topic models, like Latent Dirichlet Allocation ({LDA}), have been recently used to automatically generate text corpora topics, and to subdivide the corpus words among those topics. However, not all the estimated topics are of equal importance or correspond to genuine themes of the domain. Some of the topics can be a collection of irrelevant words, or represent insignificant themes. Current approaches to topic modeling perform manual examination to find meaningful topics. This paper presents the first automated unsupervised analysis of {LDA} models to identify junk topics from legitimate ones, and to rank the topic significance. Basically, the distance between a topic distribution and three definitions of “junk distribution” is computed using a variety of measures, from which an expressive figure of the topic significance is implemented using 4-phase Weighted Combination approach. Our experiments on synthetic and benchmark datasets show the effectiveness of the proposed approach in ranking the topic significance.},
	eventtitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
	pages = {67--82},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	publisher = {Springer, Berlin, Heidelberg},
	author = {{AlSumait}, Loulwah and Barbará, Daniel and Gentle, James and Domeniconi, Carlotta},
	urldate = {2017-11-16},
	date = {2009-09-07},
	langid = {english},
	file = {Snapshot:/Users/franzi/Zotero/storage/WCPH8XR4/10.html:text/html}
}

@inproceedings{buntine_estimating_2009,
	location = {Berlin, Heidelberg},
	title = {Estimating Likelihoods for Topic Models},
	isbn = {978-3-642-05223-1},
	url = {http://dx.doi.org/10.1007/978-3-642-05224-8_6},
	doi = {10.1007/978-3-642-05224-8_6},
	series = {{ACML} '09},
	abstract = {Topic models are a discrete analogue to principle component analysis and independent component analysis that model {\textless}em{\textgreater}topic{\textless}/em{\textgreater} at the word level within a document. They have many variants such as {NMF}, {PLSI} and {LDA}, and are used in many fields such as genetics, text and the web, image analysis and recommender systems. However, only recently have reasonable methods for estimating the likelihood of unseen documents, for instance to perform testing or model comparison, become available. This paper explores a number of recent methods, and improves their theory, performance, and testing.},
	pages = {51--64},
	booktitle = {Proceedings of the 1st Asian Conference on Machine Learning: Advances in Machine Learning},
	publisher = {Springer-Verlag},
	author = {Buntine, Wray},
	date = {2009}
}

@incollection{hoffman_online_2010,
	title = {Online Learning for Latent Dirichlet Allocation},
	url = {http://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf},
	pages = {856--864},
	booktitle = {Advances in Neural Information Processing Systems 23},
	publisher = {Curran Associates, Inc.},
	author = {Hoffman, Matthew and Bach, Francis R. and Blei, David M.},
	editor = {Lafferty, J. D. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel, R. S. and Culotta, A.},
	date = {2010},
	file = {NIPS Full Text PDF:/Users/franzi/Zotero/storage/E3N5REJ7/Hoffman et al. - 2010 - Online Learning for Latent Dirichlet Allocation.pdf:application/pdf;NIPS Snapshort:/Users/franzi/Zotero/storage/NE5ZUD83/3902-online-learning-for-latent-dirichlet-allocation.html:text/html}
}

@incollection{steyvers_probabilistic_2006,
	title = {Probabilistic Topic Models},
	booktitle = {Latent Semantic Analysis: A Road to Meaning.},
	publisher = {Laurence Erlbaum},
	author = {Steyvers, Mark and Griffiths, Thomas L.},
	editor = {Landauer, L. and Mcnamara, D. and Dennis, S. and Kintsch, W.},
	date = {2006}
}

@report{heinrich_parameter_2004,
	title = {Parameter estimation for text analysis},
	abstract = {Abstract. Presents parameter estimation methods common with discrete probability distributions, which is of particular interest in text modeling. Starting with maximum likelihood, a posteriori and Bayesian estimation, central concepts like conjugate distributions and Bayesian networks are reviewed. As an application, the model of latent Dirichlet allocation ({LDA}) is explained in detail with a full derivation of an approximate inference algorithm based on Gibbs sampling, including a discussion of Dirichlet hyperparameter estimation. Finally, analysis methods of {LDA} models are discussed.},
	author = {Heinrich, Gregor},
	date = {2004},
	file = {Citeseer - Full Text PDF:/Users/franzi/Zotero/storage/PTHWFSPM/Heinrich - 2004 - Parameter estimation for text analysis.pdf:application/pdf;Citeseer - Snapshot:/Users/franzi/Zotero/storage/JZDEF9EX/summary.html:text/html}
}

@article{griffiths_probabilistic_2002,
	title = {A probabilistic approach to semantic representation},
	volume = {24},
	url = {https://escholarship.org/uc/item/44x9v7m7},
	number = {24},
	journaltitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
	author = {Griffiths, Thomas L. and Steyvers, Mark},
	urldate = {2017-11-16},
	date = {2002-01-01},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/6EQ8VAJR/Griffiths und Steyvers - 2002 - A probabilistic approach to semantic representatio.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/76HFDXBQ/44x9v7m7.pdf:application/pdf}
}

@article{blei_nested_2007,
	title = {The nested Chinese restaurant process and Bayesian nonparametric inference of topic hierarchies},
	url = {http://arxiv.org/abs/0710.0845},
	abstract = {We present the nested Chinese restaurant process ({nCRP}), a stochastic process which assigns probability distributions to infinitely-deep, infinitely-branching trees. We show how this stochastic process can be used as a prior distribution in a Bayesian nonparametric model of document collections. Specifically, we present an application to information retrieval in which documents are modeled as paths down a random tree, and the preferential attachment dynamics of the {nCRP} leads to clustering of documents according to sharing of topics at multiple levels of abstraction. Given a corpus of documents, a posterior inference algorithm finds an approximation to a posterior distribution over trees, topics and allocations of words to levels of the tree. We demonstrate this algorithm on collections of scientific abstracts from several journals. This model exemplifies a recent trend in statistical machine learning--the use of Bayesian nonparametric methods to infer distributions on flexible data structures.},
	journaltitle = {{arXiv}:0710.0845 [stat]},
	author = {Blei, David M. and Griffiths, Thomas L. and Jordan, Michael I.},
	date = {2007-10-03},
	eprinttype = {arxiv},
	eprint = {0710.0845},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:0710.0845 PDF:/Users/franzi/Zotero/storage/QF33HDD7/Blei et al. - 2007 - The nested Chinese restaurant process and Bayesian.pdf:application/pdf;arXiv.org Snapshot:/Users/franzi/Zotero/storage/GC56ZTK4/0710.html:text/html}
}

@incollection{griffiths_hierarchical_2004,
	title = {Hierarchical Topic Models and the Nested Chinese Restaurant Process},
	url = {http://papers.nips.cc/paper/2466-hierarchical-topic-models-and-the-nested-chinese-restaurant-process.pdf},
	pages = {17--24},
	booktitle = {Advances in Neural Information Processing Systems 16},
	publisher = {{MIT} Press},
	author = {Griffiths, Thomas L. and Jordan, Michael I. and Tenenbaum, Joshua B. and Blei, David M.},
	editor = {Thrun, S. and Saul, L. K. and Schölkopf, B.},
	date = {2004},
	file = {NIPS Full Text PDF:/Users/franzi/Zotero/storage/RC2JTMS3/Griffiths et al. - 2004 - Hierarchical Topic Models and the Nested Chinese R.pdf:application/pdf;NIPS Snapshort:/Users/franzi/Zotero/storage/H6Z7JS5C/2466-hierarchical-topic-models-and-the-nested-chinese-restaurant-process.html:text/html}
}

@inproceedings{blei_latent_2001,
	location = {Cambridge, {MA}, {USA}},
	title = {Latent Dirichlet Allocation},
	url = {http://dl.acm.org/citation.cfm?id=2980539.2980618},
	series = {{NIPS}'01},
	abstract = {We propose a generative model for text and other collections of discrete data that generalizes or improves on several previous models including naive Bayes/unigram, mixture of unigrams [6], and Hof-mann's aspect model, also known as probabilistic latent semantic indexing ({pLSI}) [3]. In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable. Inference and learning are carried out efficiently via variational algorithms. We present empirical results on applications of this model to problems in text modeling, collaborative filtering, and text classification.},
	pages = {601--608},
	booktitle = {Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic},
	publisher = {{MIT} Press},
	author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
	date = {2001}
}

@incollection{chang_reading_2009,
	title = {Reading Tea Leaves: How Humans Interpret Topic Models},
	url = {http://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf},
	shorttitle = {Reading Tea Leaves},
	pages = {288--296},
	booktitle = {Advances in Neural Information Processing Systems 22},
	publisher = {Curran Associates, Inc.},
	author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Boyd-graber, Jordan L. and Blei, David M.},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
	date = {2009},
	file = {NIPS Full Text PDF:/Users/franzi/Zotero/storage/UUPBMREK/Chang et al. - 2009 - Reading Tea Leaves How Humans Interpret Topic Mod.pdf:application/pdf;NIPS Snapshort:/Users/franzi/Zotero/storage/68XT476P/3700-reading-tea-leaves-how-humans-interpret-topic-models.html:text/html}
}

@inproceedings{wallach_evaluation_2009,
	location = {New York, {NY}, {USA}},
	title = {Evaluation Methods for Topic Models},
	isbn = {978-1-60558-516-1},
	url = {http://doi.acm.org/10.1145/1553374.1553515},
	doi = {10.1145/1553374.1553515},
	series = {{ICML} '09},
	abstract = {A natural evaluation metric for statistical topic models is the probability of held-out documents given a trained model. While exact computation of this probability is intractable, several estimators for this probability have been used in the topic modeling literature, including the harmonic mean method and empirical likelihood method. In this paper, we demonstrate experimentally that commonly-used methods are unlikely to accurately estimate the probability of held-out documents, and propose two alternative methods that are both accurate and efficient.},
	pages = {1105--1112},
	booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
	publisher = {{ACM}},
	author = {Wallach, Hanna M. and Murray, Iain and Salakhutdinov, Ruslan and Mimno, David},
	date = {2009}
}

@inproceedings{wei_lda-based_2006,
	location = {New York, {NY}, {USA}},
	title = {{LDA}-based Document Models for Ad-hoc Retrieval},
	isbn = {978-1-59593-369-0},
	url = {http://doi.acm.org/10.1145/1148170.1148204},
	doi = {10.1145/1148170.1148204},
	series = {{SIGIR} '06},
	abstract = {Search algorithms incorporating some form of topic model have a long history in information retrieval. For example, cluster-based retrieval has been studied since the 60s and has recently produced good results in the language model framework. An approach to building topic models based on a formal generative model of documents, Latent Dirichlet Allocation ({LDA}), is heavily cited in the machine learning literature, but its feasibility and effectiveness in information retrieval is mostly unknown. In this paper, we study how to efficiently use {LDA} to improve ad-hoc retrieval. We propose an {LDA}-based document model within the language modeling framework, and evaluate it on several {TREC} collections. Gibbs sampling is employed to conduct approximate inference in {LDA} and the computational complexity is analyzed. We show that improvements over retrieval using cluster-based models can be obtained with reasonable efficiency.},
	pages = {178--185},
	booktitle = {Proceedings of the 29th Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Wei, Xing and Croft, W. Bruce},
	date = {2006},
	keywords = {topic model, document model, Information retrieval, language model, latent dirichlet allocation ({LDA})}
}

@inproceedings{mimno_optimizing_2011-1,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Optimizing Semantic Coherence in Topic Models},
	isbn = {978-1-937284-11-4},
	url = {http://dl.acm.org/citation.cfm?id=2145432.2145462},
	series = {{EMNLP} '11},
	abstract = {Latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces. In order for people to use such models, however, they must trust them. Unfortunately, typical dimensionality reduction methods for text, such as latent Dirichlet allocation, often produce low-dimensional subspaces (topics) that are obviously flawed to human domain experts. The contributions of this paper are threefold: (1) An analysis of the ways in which topics can be flawed; (2) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data; (3) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the National Institutes of Health ({NIH}).},
	pages = {262--272},
	booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Mimno, David and Wallach, Hanna M. and Talley, Edmund and Leenders, Miriam and {McCallum}, Andrew},
	date = {2011},
	file = {ACM Full Text PDF:/Users/franzi/Zotero/storage/758M9D8V/Mimno et al. - 2011 - Optimizing Semantic Coherence in Topic Models.pdf:application/pdf}
}

@incollection{gabszewicz_media_2015,
	title = {Media as multi-sided platforms},
	series = {Handbook on the economics of the media. - Cheltenham, {UK} : Edward Elgar Publishing, {ISBN} 978-0-85793-888-6. - 2015, p. 3-35},
	booktitle = {Handbook on the economics of the media},
	author = {Gabszewicz, Jean Jaskold and Resende, Joana and Sonnac, Nathalie},
	date = {2015},
	file = {Media as multi-sided platforms - EconBiz:/Users/franzi/Zotero/storage/MS8D937F/10011339834.html:text/html}
}

@article{reich_computer-assisted_2014,
	title = {Computer-Assisted Reading and Discovery for Student Generated Text in Massive Open Online Courses},
	volume = {2},
	rights = {Copyright (c)},
	issn = {1929-7750},
	url = {http://www.learning-analytics.info/journals/index.php/JLA/article/view/4138},
	doi = {10.18608/jla.2015.21.8},
	abstract = {Dealing with the vast quantities of text that students generate in Massive Open Online Courses ({MOOCs}) and other large-scale online learning environments is a daunting challenge. Computational tools are needed to help instructional teams uncover themes and patterns as students write in forums, assignments, and surveys. This paper introduces to the learning analytics community the Structural Topic Model, an approach to language processing that can 1) ﬁnd syntactic patterns with semantic meaning in unstructured text, 2) identify variation in those patterns across covariates, and 3) uncover archetypal texts that exemplify the documents within a topical pattern. We show examples of computationally aided discovery and reading in three {MOOC} settings: mapping students’ self-reported motivations, identifying themes in discussion forums, and uncovering patterns of feedback in course evaluations.},
	pages = {156--184},
	number = {1},
	journaltitle = {Journal of Learning Analytics},
	author = {Reich, Justin and Tingley, Dustin and Leder-Luis, Jetson and Roberts, Margaret E. and Stewart, Brandon},
	urldate = {2017-11-09},
	date = {2014-11-18},
	langid = {english},
	keywords = {computer‐assisted reading, Massive Open Online Courses, text analysis, topic modelling},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/BPWP7FIR/Reich et al. - 2014 - Computer-Assisted Reading and Discovery for Studen.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/52JEF8DU/4138.html:text/html}
}

@article{farrell_corporate_2016,
	title = {Corporate funding and ideological polarization about climate change},
	volume = {113},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/113/1/92},
	doi = {10.1073/pnas.1509433112},
	abstract = {Drawing on large-scale computational data and methods, this research demonstrates how polarization efforts are influenced by a patterned network of political and financial actors. These dynamics, which have been notoriously difficult to quantify, are illustrated here with a computational analysis of climate change politics in the United States. The comprehensive data include all individual and organizational actors in the climate change countermovement (164 organizations), as well as all written and verbal texts produced by this network between 1993–2013 (40,785 texts, more than 39 million words). Two main findings emerge. First, that organizations with corporate funding were more likely to have written and disseminated texts meant to polarize the climate change issue. Second, and more importantly, that corporate funding influences the actual thematic content of these polarization efforts, and the discursive prevalence of that thematic content over time. These findings provide new, and comprehensive, confirmation of dynamics long thought to be at the root of climate change politics and discourse. Beyond the specifics of climate change, this paper has important implications for understanding ideological polarization more generally, and the increasing role of private funding in determining why certain polarizing themes are created and amplified. Lastly, the paper suggests that future studies build on the novel approach taken here that integrates large-scale textual analysis with social networks.},
	pages = {92--97},
	number = {1},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Farrell, Justin},
	urldate = {2017-11-09},
	date = {2016-01-05},
	langid = {english},
	pmid = {26598653},
	keywords = {climate change, computational social science, funding, polarization, politics},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/8M3IS9E4/Farrell - 2016 - Corporate funding and ideological polarization abo.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/9R5TQ8PC/92.html:text/html}
}

@report{law_constitutional_2016,
	location = {Rochester, {NY}},
	title = {Constitutional Archetypes},
	url = {https://papers.ssrn.com/abstract=2732519},
	abstract = {It is a core function of constitutions to justify the existence and organization of the state. The ideological narratives embedded in constitutions are not fundamentally unique, however, but instead derive from a limited number of competing models. Each model is defined by a particular type of justification for the existence and organization of the state, and by a symbiotic relationship with a particular legal tradition. These models are so ubiquitous and elemental that they amount to constitutional archetypes.This Article contends as an empirical matter that constitutional narratives of the state boil down to a combination of three basic archetypes–namely, a liberal archetype, a statist archetype, and a universalist archetype. The liberal archetype is closely identified with the common law tradition and views the state as a potentially oppressive concentration of authority in need of regulation and restraint. In keeping with this conception of the state, liberal constitutions emphasize the imposition of limits upon government in the form of negative and procedural rights, as well as a strong and independent judiciary to make these limits effective. The legitimacy of the state is contingent upon adherence to constitutional limits. Constitutions in this vein are largely agnostic as to what goals, if any, society as a whole should pursue through the mechanism of the state.The statist archetype, in contrast, is associated with the civil law tradition and hails the state as the embodiment of a distinctive community and the vehicle for the achievement of the community’s goals. The legitimacy of the state rests upon the strength of the state’s claim to represent the will of a community. Consequently, constitutions in this vein are attentive to the identity, membership, and symbols of the state. Other characteristics of a statist constitution include an emphasis on the articulation of collective goals and positive rights that contemplate an active role for the state, and an obligation on the part of citizens to cooperate with the state in the pursuit of shared goals.The universalist archetype, the newest and most prevalent of the three, is symbiotically intertwined with a post-World War {II}, post-Westphalian paradigm of international law that rests the legitimacy of the state upon the normative force of a global legal order that encompasses both constitutional law and international law. Characteristics of this archetype include explicit commitment to supranational institutions and supranational law and reliance on generic terms and concepts that can be found not only in a variety of national constitutions, but also in international legal instruments.Empirical evidence of the prevalence and content of these three basic archetypes can be found in the unlikeliest of places – namely, constitutional preambles. Preambles enjoy a reputation for expressing uniquely national values, identities, and narratives. If there is any part of a constitution that ought not to be reducible to a handful of recurring patterns, it is surely the preamble. Yet analysis of the world’s constitutional preambles using methods from computational linguistics suggests that they consist of a combination of the three archetypes. Estimation of a structural topic model yields a quantitative measure of the extent to which each preamble draws upon each archetype.The empirical analysis also highlights the growing commingling and interdependence of constitutional law and international law. The semantic patterns that characterize universalist preambles mirrors those found in leading international human rights instruments. The adoption of the same conceptual and normative vocabulary by both universalist constitutions and key international legal instruments signals the emergence of a globalized ideological dialect common to both domestic constitutional law and public international law. The rising use of this common language by constitutional drafters since World War {II} is a quantitative indicator of the growing extent to which constitutional law and public international law influence each other.},
	number = {{ID} 2732519},
	institution = {Social Science Research Network},
	type = {{SSRN} Scholarly Paper},
	author = {Law, David S.},
	urldate = {2017-11-09},
	date = {2016-12-05},
	keywords = {topic model, text analysis, archetype, automated, civil law, common law, constitution, constitution-making, constitution-writing, constitutional drafting, constitutional law, content analysis, empirical, ideology, international law, legal traditions, liberalism, preamble, statism, universalism},
	file = {Snapshot:/Users/franzi/Zotero/storage/IMHWX425/papers.html:text/html}
}

@report{mueller_reading_2016,
	location = {Rochester, {NY}},
	title = {Reading between the Lines: Prediction of Political Violence Using Newspaper Text},
	url = {https://papers.ssrn.com/abstract=2843535},
	shorttitle = {Reading between the Lines},
	abstract = {This article provides a new methodology to predict armed conflict by using newspaper text. Through machine learning, vast quantities of newspaper text are reduced to interpretable topics. We propose the use of the within-country variation of these topics to predict the timing of conflict. This allows us to avoid the tendency of predicting conflict only in countries where it occurred before. We show that the within-country variation of topics is an extremely robust predictor of conflict and becomes particularly useful when new conflict risks arise. Two aspects seem to be responsible for these features. Topics provide depth because they consist of changing, long lists of terms which makes them able to capture the changing context of conflict. At the same time topics provide width because they summarize all text, including coverage of stabilizing factors.},
	number = {{ID} 2843535},
	institution = {Social Science Research Network},
	type = {{SSRN} Scholarly Paper},
	author = {Mueller, Hannes Felix and Rauh, Christopher},
	urldate = {2017-11-09},
	date = {2016-09-01},
	keywords = {Civil War, conflict, Forecasting, Latent Dirichlet Allocation., Machine Learning, panel data, Topic Models},
	file = {Snapshot:/Users/franzi/Zotero/storage/R7FGX7V9/papers.html:text/html}
}

@article{lucas_computer_2015,
	title = {Computer assisted text analysis for comparative politics},
	volume = {23},
	pages = {254--277},
	number = {2},
	journaltitle = {Political Analysis},
	author = {Lucas, Christopher and Nielsen, Richard and Roberts, Margaret and Stewart, Brandon and Storer, Alex and Tingley, Dustin},
	date = {2015},
	file = {Computer assisted text analysis for comparative politics | Dustin Tingley:/Users/franzi/Zotero/storage/N5EV4K52/computer-assisted-text-analysis-comparative-politics.html:text/html}
}

@article{roberts_structural_2014,
	title = {Structural Topic Models for Open-Ended Survey Responses},
	volume = {58},
	issn = {1540-5907},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/ajps.12103/abstract},
	doi = {10.1111/ajps.12103},
	abstract = {Collection and especially analysis of open-ended survey responses are relatively rare in the discipline and when conducted are almost exclusively done through human coding. We present an alternative, semiautomated approach, the structural topic model ({STM}) (Roberts, Stewart, and Airoldi 2013; Roberts et al. 2013), that draws on recent developments in machine learning based analysis of textual data. A crucial contribution of the method is that it incorporates information about the document, such as the author's gender, political affiliation, and treatment assignment (if an experimental study). This article focuses on how the {STM} is helpful for survey researchers and experimentalists. The {STM} makes analyzing open-ended responses easier, more revealing, and capable of being used to estimate treatment effects. We illustrate these innovations with analysis of text from surveys and experiments.},
	pages = {1064--1082},
	number = {4},
	journaltitle = {American Journal of Political Science},
	shortjournal = {American Journal of Political Science},
	author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin and Lucas, Christopher and Leder-Luis, Jetson and Gadarian, Shana Kushner and Albertson, Bethany and Rand, David G.},
	date = {2014-10-01},
	langid = {english},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/FD9RK878/Roberts et al. - 2014 - Structural Topic Models for Open-Ended Survey Resp.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/6GGMT3JM/abstract.html:text/html}
}

@article{das_trends_2017,
	title = {Trends in Transportation Research},
	volume = {2614},
	issn = {0361-1981},
	url = {http://trrjournalonline.trb.org/doi/abs/10.3141/2614-04},
	doi = {10.3141/2614-04},
	abstract = {Proceedings of journal and conference papers are good sources of big textual data to examine research trends in various branches of science. The contents, usually unstructured in nature, require fast machine-learning algorithms to be deciphered. Exploratory analysis through text mining usually provides the descriptive nature of the contents but lacks quantification of the topics and their correlations. Topic models are algorithms designed to discover the main theme or trend in massive collections of unstructured documents. Through the use of a structural topic model, an extension of latent Dirichlet allocation, this study introduced distinct topic models on the basis of the relative frequencies of the words used in the abstracts of 15,357 {TRB} compendium papers. With data from 7 years (2008 through 2014) of {TRB} annual meeting compendium papers, the 20 most dominant topics emerged from a bag of 4 million words. The findings of this study contributed to the understanding of topical trends in the complex and evolving field of transportation engineering research.},
	pages = {27--38},
	journaltitle = {Transportation Research Record: Journal of the Transportation Research Board},
	shortjournal = {Transportation Research Record: Journal of the Transportation Research Board},
	author = {Das, Subasish and Dixon, Karen and Sun, Xiaoduan and Dutta, Anandi and Zupancich, Michelle},
	date = {2017-01-01},
	file = {Snapshot:/Users/franzi/Zotero/storage/FTII7NXR/2614-04.html:text/html}
}

@article{haixia_extracting_2016,
	title = {Extracting Topics of Computer Science Literature with {LDA} Model, Extracting Topics of Computer Science Literature with {LDA} Model},
	volume = {32},
	issn = {2096-3467},
	url = {http://manu44.magtech.com.cn/Jwk_infotech_wk3/EN/abstract/abstract4288.shtml},
	doi = {10.11925/infotech.1003-3513.2016.11.03},
	pages = {20--26},
	number = {11},
	journaltitle = {Data Analysis and Knowledge Discovery},
	shortjournal = {Data Analysis and Knowledge Discovery},
	author = {Haixia, Yang and Baojun, Gao and Hanlin, Sun and Haixia, Yang and Baojun, Gao and Hanlin, Sun},
	urldate = {2017-11-09},
	date = {2016-12-20},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/PR58B4CK/Haixia et al. - 2016 - Extracting Topics of Computer Science Literature w.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/DDFB9U5P/abstract4288.html:text/html}
}

@incollection{hagen_data_2018,
	title = {Data Analytics for Policy Informatics: The Case of E-Petitioning},
	isbn = {978-3-319-61761-9 978-3-319-61762-6},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-61762-6_9},
	series = {Public Administration and Information Technology},
	shorttitle = {Data Analytics for Policy Informatics},
	abstract = {To contribute to the development of policy informatics, we discuss the benefits of analyzing electronic petitions (e-petitions), a form of citizen-government discourse with deep historic roots that has recently transitioned into a technologically-enabled and novel form of political communication. We begin by presenting a rationale for the analysis of e-petitions as a type of e-participation that can contribute to the development of public policy, provided that it is possible to analyze the large volumes of data produced in petitioning processes. From there we consider two data analytic strategies that offer promising approaches to the analysis of e-petitions and that lend themselves to the future creation of policy informatics tools. We discuss the application of topic modeling to the analysis of e-petition textual data to identify emergent topics of substantial concern to the public. We further propose the application of social network analysis to data related to the dynamics of petitioning processes, such as the social connections between petition initiators and signers, and tweets that solicit petition signatures in petitioning campaigns; both may be useful in revealing patterns of collective action. The paper concludes by reflecting on issues that should be brought to bear on the construction of policy informatics tools that make use of e-petitioning data.},
	pages = {205--224},
	booktitle = {Policy Analytics, Modelling, and Informatics},
	publisher = {Springer, Cham},
	author = {Hagen, Loni and Harrison, Teresa M. and Dumas, Catherine L.},
	urldate = {2017-11-09},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-3-319-61762-6_9},
	file = {Snapshot:/Users/franzi/Zotero/storage/QBNNBEIJ/978-3-319-61762-6_9.html:text/html}
}

@article{baturo_what_2017,
	title = {What Drives the International Development Agenda? An {NLP} Analysis of the United Nations General Debate 1970-2016},
	url = {http://arxiv.org/abs/1708.05873},
	shorttitle = {What Drives the International Development Agenda?},
	abstract = {There is surprisingly little known about agenda setting for international development in the United Nations ({UN}) despite it having a significant influence on the process and outcomes of development efforts. This paper addresses this shortcoming using a novel approach that applies natural language processing techniques to countries' annual statements in the {UN} General Debate. Every year {UN} member states deliver statements during the General Debate on their governments' perspective on major issues in world politics. These speeches provide invaluable information on state preferences on a wide range of issues, including international development, but have largely been overlooked in the study of global politics. This paper identifies the main international development topics that states raise in these speeches between 1970 and 2016, and examine the country-specific drivers of international development rhetoric.},
	journaltitle = {{arXiv}:1708.05873 [cs]},
	author = {Baturo, Alexander and Dasandi, Niheer and Mikhaylov, Slava J.},
	date = {2017-08-19},
	eprinttype = {arxiv},
	eprint = {1708.05873},
	keywords = {Computer Science - Computation and Language, Computer Science - Social and Information Networks},
	file = {arXiv\:1708.05873 PDF:/Users/franzi/Zotero/storage/FSRGQVHA/Baturo et al. - 2017 - What Drives the International Development Agenda .pdf:application/pdf;arXiv.org Snapshot:/Users/franzi/Zotero/storage/H2JRUWQF/1708.html:text/html}
}

@report{grajzl_structural_2017,
	location = {Rochester, {NY}},
	title = {A Structural Topic Model of the Features and the Cultural Origins of Bacon's Ideas},
	url = {https://papers.ssrn.com/abstract=2944816},
	abstract = {We use machine-learning methods to study the features and origins of the thought of Francis Bacon, a key figure in the development of a cultural paradigm that provided intellectual roots for modern economic development. We estimate a structural topic model, a state-of-the-art methodology for analysis of text corpora. The estimates uncover sixteen topics prominent in Bacon's opus. Two are central in the ideas usually associated with Bacon: inductive epistemology and fact-seeking. While Bacon's epistemology is strongly connected with his jurisprudence, fact-seeking is more isolated from Bacon's other intellectual pursuits. The utilitarian promise of science and the central organization of the scientific quest, embraced by Bacon's followers, were not emphasized by him, a finding suggesting that these aspects of the 'Baconian' culture owed little to Bacon's own contributions. Bacon's use of different topics varies notably with intended audience and chosen medium.},
	number = {{ID} 2944816},
	institution = {Social Science Research Network},
	type = {{SSRN} Scholarly Paper},
	author = {Grajzl, Peter and Murrell, Peter},
	urldate = {2017-11-07},
	date = {2017-10-20},
	keywords = {politics, culture, Francis Bacon, knowledge, law, natural philosophy, religion},
	file = {Snapshot:/Users/franzi/Zotero/storage/B862TXNA/papers.html:text/html}
}

@incollection{roberts_navigating_2016,
	location = {New York},
	title = {Navigating the Local Modes of Big Data: The Case of Topic Models.},
	booktitle = {Computational Social Science: Discovery and Prediction},
	publisher = {Cambridge University Press},
	author = {Roberts, Margaret and Stewart, Brandon and Tingley, Dustin},
	date = {2016},
	file = {Navigating the Local Modes of Big Data\: The Case of Topic Models | Dustin Tingley:/Users/franzi/Zotero/storage/TW4DVST8/navigating-local-modes-big-data-case-topic-models.html:text/html}
}

@article{roberts_stm:_2016,
	title = {stm: R Package for Structural Topic Models},
	volume = {forthcoming},
	shorttitle = {stm},
	journaltitle = {Journal of Statistical Software},
	author = {Roberts, Margaret and Stewart, Brandon and Tingley, Dustin},
	date = {2016-12-01},
	file = {stm\: R Package for Structural Topic Models | Dustin Tingley:/Users/franzi/Zotero/storage/FAWQJE6X/stm-r-package-structural-topic-models.html:text/html}
}

@inproceedings{minka_expectation-propagation_2002,
	location = {San Francisco, {CA}, {USA}},
	title = {Expectation-propagation for the Generative Aspect Model},
	isbn = {978-1-55860-897-9},
	url = {http://dl.acm.org/citation.cfm?id=2073876.2073918},
	series = {{UAI}'02},
	abstract = {The generative aspect model is an extension of the multinomial model for text that allows word probabilities to vary stochastically across documents. Previous results with aspect models have been promising, but hindered by the computational difficulty of carrying out inference and learning. This paper demonstrates that the simple variational methods of Blei et al. (2001) can lead to inaccurate inferences and biased learning for the generative aspect model. We develop an alternative approach that leads to higher accuracy at comparable cost. An extension of Expectation-Propagation is used for inference and then embedded in an {EM} algorithm for learning. Experimental results are presented for both synthetic and real data sets.},
	pages = {352--359},
	booktitle = {Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Minka, Thomas and Lafferty, John},
	date = {2002}
}

@article{bholat_text_2015,
	title = {Text Mining for Central Banks},
	issn = {1556-5068},
	url = {http://www.academia.edu/13430482/Text_mining_for_central_banks},
	abstract = {Although often applied in other social sciences, text mining has been less frequently used in economics and in policy circles, particularly inside central banks. This Handbook is a brief introduction to the field, discussing how text mining is useful},
	journaltitle = {{SSRN} Electronic Journal},
	author = {Bholat, David M. and Hansen, Stephen and Santos, Pedro M. and Schonhardt-Bailey, Cheryl},
	urldate = {2017-11-06},
	date = {2015-06-29},
	file = {Snapshot:/Users/franzi/Zotero/storage/J462FFQU/Text_mining_for_central_banks.html:text/html}
}

@article{schiller_development_2016,
	title = {Development of the Social Network Usage in Germany since 2012},
	journaltitle = {Working Paper {TU} Darmstadt},
	author = {Schiller, Benjamin and Heimbach, Irina and Strufe, Thorsten and Hinz, Oliver},
	date = {2016}
}

@inproceedings{rabinovich_inverse_2014,
	location = {Beijing, China},
	title = {The Inverse Regression Topic Model},
	url = {http://dl.acm.org/citation.cfm?id=3044805.3044829},
	series = {{ICML}'14},
	abstract = {Taddy (2013) proposed multinomial inverse regression ({MNIR}) as a new model of annotated text based on the influence of metadata and response variables on the distribution of words in a document. While effective, {MNIR} has no way to exploit structure in the corpus to improve its predictions or facilitate exploratory data analysis. On the other hand, traditional probabilistic topic models (like latent Dirichlet allocation) capture natural heterogeneity in a collection but do not account for external variables. In this paper, we introduce the inverse regression topic model ({IRTM}), a mixed-membership extension of {MNIR} that combines the strengths of both methodologies. We present two inference algorithms for the {IRTM}: an efficient batch estimation algorithm and an online variant, which is suitable for large corpora. We apply these methods to a corpus of 73K Congressional press releases and another of 150K Yelp reviews, demonstrating that the {IRTM} outperforms both {MNIR} and supervised topic models on the prediction task. Further, we give examples showing that the {IRTM} enables systematic discovery of in-topic lexical variation, which is not possible with previous supervised topic models.},
	pages = {I--199--I--207},
	booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
	publisher = {{JMLR}.org},
	author = {Rabinovich, Maxim and Blei, David M.},
	date = {2014}
}

@inproceedings{phan_learning_2008,
	location = {Beijing, China},
	title = {Learning to Classify Short and Sparse Text \& Web with Hidden Topics from Large-Scale Data Collections},
	doi = {10.1145/1367497.1367510},
	abstract = {This paper presents a general framework for building classi- fiers that deal with short and sparse text \& Web segments by making the most of hidden topics discovered from large- scale data collections. The main motivation of this work is that many classification tasks working with short segments of text \& Web, such as search snippets, forum \& chat mes- sages, blog \& news feeds, product reviews, and book \& movie summaries, fail to achieve high accuracy due to the data sparseness. We, therefore, come up with an idea of gaining external knowledge to make the data more related as well as expand the coverage of classifiers to handle future data bet- ter. The underlying idea of the framework is that for each classification task, we collect a large-scale external data col- lection called "universal dataset", and then build a classifier on both a (small) set of labeled training data and a rich set of hidden topics discovered from that data collection. The framework is general enough to be applied to different data domains and genres ranging from Web search results to medical text. We did a careful evaluation on several hundred megabytes of Wikipedia (30M words) and {MEDLINE} (18M words) with two tasks: "Web search domain disambiguation" and "disease categorization for medical text", and achieved significant quality enhancement.},
	pages = {91--100},
	booktitle = {Proceedings of the 17th International World Wide Web Conference ({WWW} 2008)},
	author = {Phan, Xuan-Hieu and Nguyen, Le and Horiguchi, Susumu},
	date = {2008-01-01},
	file = {Snapshot:/Users/franzi/Zotero/storage/SP33XWPF/221023426_Learning_to_Classify_Short_and_Sparse_Text_Web_with_Hidden_Topics_from_Large-Scale_Dat.pdf:application/pdf}
}

@article{silge_tidytext:_2016,
	title = {tidytext: Text Mining and Analysis Using Tidy Data Principles in R},
	doi = {10.21105/joss.00037},
	shorttitle = {tidytext},
	journaltitle = {The Journal of Open Source Software},
	author = {Silge, Julia and Robinson, David},
	date = {2016-07-11},
	file = {Snapshot:/Users/franzi/Zotero/storage/KBIXHDAH/305219559_tidytext_Text_Mining_and_Analysis_Using_Tidy_Data_Principles_in_R.pdf:application/pdf}
}

@thesis{ponweiser_latent_2012,
	title = {Latent Dirichlet Allocation in R},
	url = {http://epub.wu.ac.at/3558/},
	abstract = {Topic models are a new research field within the computer sciences information retrieval and text mining. They are generative probabilistic models of text corpora inferred by machine learning and they can be used for retrieval and text mining tasks. The most prominent topic model is latent Dirichlet allocation ({LDA}), which was introduced in 2003 by Blei et al. and has since then sparked off the development of other topic models for domain-specific purposes.
This thesis focuses on {LDA}'s practical application. Its main goal is the replication of the data analyses from the 2004 {LDA} paper ``Finding scientific topics'' by Thomas Griffiths and Mark Steyvers within the framework of the R statistical programming language and the R{\textasciitilde}package topicmodels by Bettina Grün and Kurt Hornik. The complete process, including extraction of a text corpus from the {PNAS} journal's website, data preprocessing, transformation into a document-term matrix, model selection, model estimation, as well as presentation of the results, is fully documented and commented. The outcome closely matches the analyses of the original paper, therefore the research by Griffiths/Steyvers can be reproduced. Furthermore, this thesis proves the suitability of the R environment for text mining with {LDA}. (author's abstract)},
	institution = {{WU} Vienna University of Economics and Business},
	type = {Theses / Institute for Statistics and Mathematics},
	author = {Ponweiser, Martin},
	urldate = {2017-10-13},
	date = {2012-05},
	langid = {english},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/AKASJPTP/Ponweiser - 2012 - Latent Dirichlet Allocation in R.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/E5J47GZP/3558.html:text/html}
}

@article{genkin_large-scale_2007,
	title = {Large-Scale Bayesian Logistic Regression for Text Categorization},
	volume = {49},
	issn = {0040-1706},
	url = {http://dx.doi.org/10.1198/004017007000000245},
	doi = {10.1198/004017007000000245},
	abstract = {Logistic regression analysis of high-dimensional data, such as natural language text, poses computational and statistical challenges. Maximum likelihood estimation often fails in these applications. We present a simple Bayesian logistic regression approach that uses a Laplace prior to avoid overfitting and produces sparse predictive models for text data. We apply this approach to a range of document classification problems and show that it produces compact predictive models at least as effective as those produced by support vector machine classifiers or ridge logistic regression combined with feature selection. We describe our model fitting algorithm, our open source implementations ({BBR} and {BMR}), and experimental results.},
	pages = {291--304},
	number = {3},
	journaltitle = {Technometrics},
	author = {Genkin, Alexander and Lewis, David D. and Madigan, David},
	date = {2007-08-01},
	keywords = {Information retrieval, Lasso, Penalization, Ridge regression, Support vector classifier, Variable selection},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/9JI658VW/Genkin et al. - 2007 - Large-Scale Bayesian Logistic Regression for Text .pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/9XV4KG9V/004017007000000245.html:text/html}
}

@article{erosheva_mixed-membership_2004-1,
	title = {Mixed-membership models of scientific publications},
	volume = {101},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/101/suppl_1/5220},
	doi = {10.1073/pnas.0307760101},
	abstract = {{PNAS} is one of world's most cited multidisciplinary scientific journals. The {PNAS} official classification structure of subjects is reflected in topic labels submitted by the authors of articles, largely related to traditionally established disciplines. These include broad field classifications into physical sciences, biological sciences, social sciences, and further subtopic classifications within the fields. Focusing on biological sciences, we explore an internal soft-classification structure of articles based only on semantic decompositions of abstracts and bibliographies and compare it with the formal discipline classifications. Our model assumes that there is a fixed number of internal categories, each characterized by multinomial distributions over words (in abstracts) and references (in bibliographies). Soft classification for each article is based on proportions of the article's content coming from each category. We discuss the appropriateness of the model for the {PNAS} database as well as other features of the data relevant to soft classification.},
	pages = {5220--5227},
	issue = {suppl 1},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Erosheva, Elena and Fienberg, Stephen and Lafferty, John},
	urldate = {2017-10-12},
	date = {2004-04-06},
	langid = {english},
	pmid = {15020766},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/QX4H27E9/Erosheva et al. - 2004 - Mixed-membership models of scientific publications.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/NVF8AVJ7/5220.html:text/html}
}

@article{griffiths_finding_2004,
	title = {Finding scientific topics},
	volume = {101},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/101/suppl_1/5228},
	doi = {10.1073/pnas.0307752101},
	abstract = {A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. \& Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from {PNAS} by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying “hot topics” by examining temporal dynamics and tagging abstracts to illustrate semantic content.},
	pages = {5228--5235},
	issue = {suppl 1},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Griffiths, Thomas L. and Steyvers, Mark},
	urldate = {2017-10-12},
	date = {2004-04-06},
	langid = {english},
	pmid = {14872004},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/9P87NGIJ/Griffiths und Steyvers - 2004 - Finding scientific topics.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/TX4TP3NE/5228.html:text/html}
}

@inproceedings{mishler_using_2015,
	title = {Using Structural Topic Modeling to Detect Events and Cluster Twitter Users in the Ukrainian Crisis},
	isbn = {978-3-319-21379-8 978-3-319-21380-4},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-21380-4_108},
	doi = {10.1007/978-3-319-21380-4_108},
	series = {Communications in Computer and Information Science},
	abstract = {Structural topic modeling ({STM}) is a recently introduced technique to model how the content of a collection of documents changes as a function of variables such as author identity or time of writing. We present two proof-of-concept applications of {STM} using Russian social media data. In our first study, we model how topics change over time, showing that {STM} can be used to detect significant events such as the downing of Malaysia Air Flight 17. In our second study, we model how topical content varies across a set of authors, showing that {STM} can be used to cluster Twitter users who are sympathetic to Ukraine versus Russia as well as to cluster accounts that are suspected to belong to the same individual (so-called “sockpuppets”). Structural topic modeling shows promise as a tool for analyzing social media data, a domain that has been largely ignored in the topic modeling literature.},
	eventtitle = {International Conference on Human-Computer Interaction},
	pages = {639--644},
	booktitle = {{HCI} International 2015 - Posters’ Extended Abstracts},
	publisher = {Springer, Cham},
	author = {Mishler, Alan and Crabb, Erin Smith and Paletz, Susannah and Hefright, Brook and Golonka, Ewa},
	urldate = {2017-10-12},
	date = {2015-08-02},
	langid = {english},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/ACUCPA8U/Mishler et al. - 2015 - Using Structural Topic Modeling to Detect Events a.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/NG9CQ4T9/978-3-319-21380-4_108.html:text/html}
}

@book{airoldi_handbook_2014,
	title = {Handbook of mixed membership models and their applications},
	isbn = {978-1-4665-0408-0},
	url = {http://cds.cern.ch/record/1974849},
	abstract = {In response to scientific needs for more diverse and structured explanations of statistical data, researchers have discovered how to model individual data points as belonging to multiple groups. Handbook of Mixed Membership Models and Their Applications shows you how to use these flexible modeling tools to uncover hidden patterns in modern high-dimensional multivariate data. It explores the use of the models in various application settings, including survey data, population genetics, text analysis, image processing and annotation, and molecular biology.Through examples using real data sets, yo},
	publisher = {Taylor and Francis},
	author = {Airoldi, Edoardo M. and Erosheva, Elena A. and Fienberg, Stephen E. and Blei, David},
	urldate = {2017-10-12},
	date = {2014},
	file = {Snapshot:/Users/franzi/Zotero/storage/P7XE3MF4/1974849.html:text/html}
}

@article{roberts_model_2016,
	title = {A Model of Text for Experimentation in the Social Sciences},
	volume = {111},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1080/01621459.2016.1141684},
	doi = {10.1080/01621459.2016.1141684},
	abstract = {Statistical models of text have become increasingly popular in statistics and computer science as a method of exploring large document collections. Social scientists often want to move beyond exploration, to measurement and experimentation, and make inference about social and political processes that drive discourse and content. In this article, we develop a model of text data that supports this type of substantive research. Our approach is to posit a hierarchical mixed membership model for analyzing topical content of documents, in which mixing weights are parameterized by observed covariates. In this model, topical prevalence and topical content are specified as a simple generalized linear model on an arbitrary number of document-level covariates, such as news source and time of release, enabling researchers to introduce elements of the experimental design that informed document collection into the model, within a generally applicable framework. We demonstrate the proposed methodology by analyzing a collection of news reports about China, where we allow the prevalence of topics to evolve over time and vary across newswire services. Our methods quantify the effect of news wire source on both the frequency and nature of topic coverage. Supplementary materials for this article are available online.},
	pages = {988--1003},
	number = {515},
	journaltitle = {Journal of the American Statistical Association},
	author = {Roberts, Margaret E. and Stewart, Brandon M. and Airoldi, Edoardo M.},
	date = {2016-07-02},
	keywords = {text analysis, Causal inference, Experimentation, High-dimensional inference, Social sciences, Variational approximation},
	file = {Snapshot:/Users/franzi/Zotero/storage/ZMEDWA9E/01621459.2016.html:text/html}
}

@article{quinn_how_2010,
	title = {How to Analyze Political Attention with Minimal Assumptions and Costs},
	volume = {54},
	issn = {1540-5907},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1540-5907.2009.00427.x/abstract},
	doi = {10.1111/j.1540-5907.2009.00427.x},
	abstract = {Previous methods of analyzing the substance of political attention have had to make several restrictive assumptions or been prohibitively costly when applied to large-scale political texts. Here, we describe a topic model for legislative speech, a statistical learning model that uses word choices to infer topical categories covered in a set of speeches and to identify the topic of specific speeches. Our method estimates, rather than assumes, the substance of topics, the keywords that identify topics, and the hierarchical nesting of topics. We use the topic model to examine the agenda in the U.S. Senate from 1997 to 2004. Using a new database of over 118,000 speeches (70,000,000 words) from the Congressional Record, our model reveals speech topic categories that are both distinctive and meaningfully interrelated and a richer view of democratic agenda dynamics than had previously been possible.},
	pages = {209--228},
	number = {1},
	journaltitle = {American Journal of Political Science},
	author = {Quinn, Kevin M. and Monroe, Burt L. and Colaresi, Michael and Crespin, Michael H. and Radev, Dragomir R.},
	date = {2010-01-01},
	langid = {english},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/59JK6JEX/Quinn et al. - 2010 - How to Analyze Political Attention with Minimal As.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/QSVKCVD5/abstract.html:text/html}
}

@incollection{socher_bayesian_2009,
	title = {A Bayesian Analysis of Dynamics in Free Recall},
	url = {http://papers.nips.cc/paper/3720-a-bayesian-analysis-of-dynamics-in-free-recall.pdf},
	pages = {1714--1722},
	booktitle = {Advances in Neural Information Processing Systems 22},
	publisher = {Curran Associates, Inc.},
	author = {Socher, Richard and Gershman, Samuel and Sederberg, Per and Norman, Kenneth and Perotte, Adler J. and Blei, David M.},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
	date = {2009},
	file = {NIPS Full Text PDF:/Users/franzi/Zotero/storage/JQWKTPGH/Socher et al. - 2009 - A Bayesian Analysis of Dynamics in Free Recall.pdf:application/pdf;NIPS Snapshort:/Users/franzi/Zotero/storage/ZFW3QA7C/3720-a-bayesian-analysis-of-dynamics-in-free-recall.html:text/html}
}

@article{grimmer_text_2013,
	title = {Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts},
	volume = {21},
	shorttitle = {Text as Data},
	pages = {267--297},
	journaltitle = {Political Analysis},
	author = {Grimmer, Justin and Stewart, Brandon},
	date = {2013},
	file = {Text as Data\: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts | Brandon Stewart:/Users/franzi/Zotero/storage/FF9QRDJ4/text-data-promise-and-pitfalls-automatic-content-analysis-methods-political.html:text/html}
}

@article{airoldi_improving_2016,
	title = {Improving and Evaluating Topic Models and Other Models of Text},
	volume = {111},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1080/01621459.2015.1051182},
	doi = {10.1080/01621459.2015.1051182},
	abstract = {An ongoing challenge in the analysis of document collections is how to summarize content in terms of a set of inferred themes that can be interpreted substantively in terms of topics. The current practice of parameterizing the themes in terms of most frequent words limits interpretability by ignoring the differential use of words across topics. Here, we show that words that are both frequent and exclusive to a theme are more effective at characterizing topical content, and we propose a regularization scheme that leads to better estimates of these quantities. We consider a supervised setting where professional editors have annotated documents to topic categories, organized into a tree, in which leaf-nodes correspond to more specific topics. Each document is annotated to multiple categories, at different levels of the tree. We introduce a hierarchical Poisson convolution model to analyze these annotated documents. A parallelized Hamiltonian Monte Carlo sampler allows the inference to scale to millions of documents. The model leverages the structure among categories defined by professional editors to infer a clear semantic description for each topic in terms of words that are both frequent and exclusive. In this supervised setting, we validate the efficacy of word frequency and exclusivity at characterizing topical content on two very large collections of documents, from Reuters and the New York Times. In an unsupervised setting, we then consider a simplified version of the model that shares the same regularization scheme with the previous model. We carry out a large randomized experiment on Amazon Mechanical Turk to demonstrate that topic summaries based on frequency and exclusivity, estimated using the proposed regularization scheme, are more interpretable than currently established frequency-based summaries, and that the proposed model produces more efficient estimates of exclusivity than the currently established models.},
	pages = {1381--1403},
	number = {516},
	journaltitle = {Journal of the American Statistical Association},
	author = {Airoldi, Edoardo M. and Bischof, Jonathan M.},
	date = {2016-10-01},
	keywords = {text analysis, Categorical data, Hamiltonian Monte Carlo, High-dimensional data, Parallel inference},
	file = {Snapshot:/Users/franzi/Zotero/storage/HWNC8RRH/01621459.2015.html:text/html}
}

@inproceedings{blei_dynamic_2006,
	location = {New York, {NY}, {USA}},
	title = {Dynamic Topic Models},
	isbn = {978-1-59593-383-6},
	url = {http://doi.acm.org/10.1145/1143844.1143859},
	doi = {10.1145/1143844.1143859},
	series = {{ICML} '06},
	abstract = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the {OCR}'ed archives of the journal Science from 1880 through 2000.},
	pages = {113--120},
	booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
	publisher = {{ACM}},
	author = {Blei, David M. and Lafferty, John D.},
	date = {2006}
}

@article{teh_hierarchical_2006,
	title = {Hierarchical Dirichlet Processes},
	volume = {101},
	issn = {0162-1459},
	url = {http://amstat.tandfonline.com/doi/abs/10.1198/016214506000000302},
	doi = {10.1198/016214506000000302},
	abstract = {We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the “Chinese restaurant franchise.” We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling.},
	pages = {1566--1581},
	number = {476},
	journaltitle = {Journal of the American Statistical Association},
	shortjournal = {Journal of the American Statistical Association},
	author = {Teh, Yee Whye and Jordan, Michael I and Beal, Matthew J and Blei, David M},
	date = {2006-12-01},
	file = {Snapshot:/Users/franzi/Zotero/storage/67GVBDZ8/016214506000000302.html:text/html}
}

@article{airoldi_reconceptualizing_2010,
	title = {Reconceptualizing the classification of {PNAS} articles},
	volume = {107},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/107/49/20899},
	doi = {10.1073/pnas.1013452107},
	abstract = {{PNAS} article classification is rooted in long-standing disciplinary divisions that do not necessarily reflect the structure of modern scientific research. We reevaluate that structure using latent pattern models from statistical machine learning, also known as mixed-membership models, that identify semantic structure in co-occurrence of words in the abstracts and references. Our findings suggest that the latent dimensionality of patterns underlying {PNAS} research articles in the Biological Sciences is only slightly larger than the number of categories currently in use, but it differs substantially in the content of the categories. Further, the number of articles that are listed under multiple categories is only a small fraction of what it should be. These findings together with the sensitivity analyses suggest ways to reconceptualize the organization of papers published in {PNAS}.},
	pages = {20899--20904},
	number = {49},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Airoldi, Edoardo M. and Erosheva, Elena A. and Fienberg, Stephen E. and Joutard, Cyrille and Love, Tanzy and Shringarpure, Suyash},
	urldate = {2017-10-07},
	date = {2010-12-07},
	langid = {english},
	pmid = {21078953},
	keywords = {text analysis, Dirichlet process, hierarchical modeling, Monte Carlo Markov chain, variational inference},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/3HPSSBQ2/Airoldi et al. - 2010 - Reconceptualizing the classification of PNAS artic.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/3SR8QVPZ/20899.html:text/html}
}

@inproceedings{taddy_estimation_2012,
	title = {On estimation and selection for topic models},
	booktitle = {Proceedings of the 15th International Conference on Artificial Intelligence and Statistics},
	author = {Taddy, Matt},
	date = {2012}
}

@book{balasubramanyan_tweets_2010,
	title = {From Tweets to Polls : Linking Text Sentiment to Public Opinion Time Series},
	shorttitle = {From Tweets to Polls},
	abstract = {We connect measures of public opinion measured from polls with sentiment measured from text. We analyze several surveys on consumer confidence and political opinion over the 2008 to 2009 period, and find they correlate to sentiment word frequencies in contemporaneous Twitter messages. While our results vary across datasets, in several cases the correlations are as high as 80\%, and capture important large-scale trends. The results highlight the potential of text streams as a substitute and supplement for traditional polling.},
	author = {Balasubramanyan, Ramnath and Routledge, Bryan R. and Smith, Noah A.},
	date = {2010},
	file = {Citeseer - Full Text PDF:/Users/franzi/Zotero/storage/5NZK966N/Balasubramanyan et al. - 2010 - From Tweets to Polls  Linking Text Sentiment to P.pdf:application/pdf;Citeseer - Snapshot:/Users/franzi/Zotero/storage/27STA53M/summary.html:text/html}
}

@incollection{naskar_sentiment_2016,
	title = {Sentiment Analysis in Social Networks through Topic modeling},
	url = {/paper/Sentiment-Analysis-in-Social-Networks-through-Topic-Naskar-Mokaddem/00052e8713adf52290c260208cafec05f70035d6},
	abstract = {In this paper, we analyze the sentiments derived from the conversations that occur in social networks. Our goal is to identify the sentiments of the users in the social network through their conversations. We conduct a study to determine whether users of social networks (twitter in particular) tend to gather together according to the likeness of their sentiments. In our proposed framework, (1) we use {ANEW}, a lexical dictionary to identify affective emotional feelings associated to a message according to the Russell’s model of affection; (2) we design a topic modeling mechanism called Sent {LDA}, based on the Latent Dirichlet Allocation ({LDA}) generative model, which allows us to find the topic distribution in a general conversation and we associate topics with emotions; (3) we detect communities in the network according to the density and frequency of the messages among the users; and (4) we compare the sentiments of the communities by using the Russell’s model of affect versus polarity and we measure the extent to which topic distribution strengthen likeness in the sentiments of the users of a community. This works contributes with a topic modeling methodology to analyze the sentiments in conversations that take place in social networks.},
	booktitle = {{LREC}},
	author = {Naskar, Debashis and Mokaddem, Sidahmed and Rebollo, Miguel and Onaindia, Eva},
	urldate = {2018-08-13},
	date = {2016},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/WKUIPZ8D/Naskar et al. - 2016 - Sentiment Analysis in Social Networks through Topi.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/HDI4WHSA/00052e8713adf52290c260208cafec05f70035d6.html:text/html}
}

@article{duru_relevance_2018,
	title = {{THE} {RELEVANCE} {OF} {AGENDA}-{SETTING} {THEORY} {IN} {TWENTY} {FIRST} {CENTURY} {JOURNALISM} {PRACTISE}},
	volume = {8},
	rights = {Copyright (c) 2018 International Journal of Social Sciences and Humanities Review},
	issn = {2276-8645},
	url = {http://ijsshr.com/journal/index.php/IJSSHR/article/view/423},
	abstract = {This paper examines the relevance of Agenda Setting theory in twenty first century Journalism practice. Agenda-setting theory describes the capacity of news media to influence and guide public discourse. That is, if a news item is covered frequently and prominently, the audience will regard the issue as more important. Agenda-setting theory was formally developed by Dr. Max {McCombs} and Dr. Donald Shaw in a study on the 1968 presidential election. In the 1968 ‘Chapel Hill study,’ {McCombs} and Shaw demonstrated a strong correlation between what 100 residents of Chapel Hill, North Carolina thought was the most important election issue and what the local and national news media reported was the most important issue. By comparing the salience of issues in news content with the public's perceptions of the most important election issue, {McCombs} and Shaw were able to determine the degree to which the media determines public opinion. Since the 1968 study, published in a 1972 edition of Public Opinion Quarterly, more than 400 studies have been published on the agenda-setting function of the mass media. The argument that the Agenda-Setting Theory is still very relevant in contemporary Journalism practice was advanced in the discourse. Based on the Social Responsibility and Gate- Keeping Theories, the paper focused on the Agenda Setting Theory of Communication, its implications, applications and continued relevance, highlighting the functions of the mass media. It is recommended that media stakeholders do more to ensure that issues that will enhance the socio- politico- economic well-being of the people are hyped regularly, to ensure the achievement of set objectives.},
	number = {2},
	journaltitle = {International Journal of Social Sciences and Humanities Review},
	author = {Duru, Chike Walter},
	urldate = {2018-08-14},
	date = {2018-06-30},
	langid = {english},
	keywords = {Agenda Setting Theory, Journalism, Mass Media, Twenty first Century},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/U3ZQAQHA/Duru - 2018 - THE RELEVANCE OF AGENDA-SETTING THEORY IN TWENTY F.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/XRBMQKIQ/423.html:text/html}
}

@article{savigny_public_2002,
	title = {Public Opinion, Political Communication and the Internet},
	volume = {22},
	rights = {© Political Studies Association, 2002.},
	issn = {1467-9256},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9256.00152},
	doi = {10.1111/1467-9256.00152},
	abstract = {In contemporary society public opinion is generally mediated by the mass media, which has come to encompass the Habermasian ‘public sphere’. This arena is now characterised by the conflict between market and democratic principles, by competing interests of politicians and the media. The presentation of information for debate becomes distorted. The opinion of the ‘public’ is no longer created through deliberation, but is constructed through systems of communication, in conflict with political actors, who seek to retain control of the dissemination of information. The expansion of the internet as a new method of communication provides a potential challenge to the primacy of the traditional media and political parties as formers of public opinion.},
	pages = {1--8},
	number = {1},
	journaltitle = {Politics},
	author = {Savigny, Heather},
	urldate = {2018-08-14},
	date = {2002-02-01},
	langid = {english},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/MFG6M57V/Savigny - 2002 - Public Opinion, Political Communication and the In.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/55HY587S/1467-9256.html:text/html}
}

@article{benewick_floating_1969,
	title = {{THE} {FLOATING} {VOTER} {AND} {THE} {LIBERAL} {VIEW} {OF} {REPRESENTATIONa}},
	volume = {17},
	issn = {1467-9248},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9248.1969.tb00634.x},
	doi = {10.1111/j.1467-9248.1969.tb00634.x},
	pages = {177--195},
	number = {2},
	journaltitle = {Political Studies},
	author = {Benewick, R. J. and Birch, A. H. and Blumler, J. G. and Ewbank, Alison},
	urldate = {2018-08-14},
	date = {1969-06-01},
	langid = {english},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/Z53NBN9F/Benewick et al. - 1969 - THE FLOATING VOTER AND THE LIBERAL VIEW OF REPRESE.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/C96GQEHU/j.1467-9248.1969.tb00634.html:text/html}
}

@article{mccombs_agenda-setting_1972,
	title = {The Agenda-Setting Function of Mass Media},
	volume = {36},
	issn = {0033-362X},
	url = {https://www.jstor.org/stable/2747787},
	abstract = {In choosing and displaying news, editors, newsroom staff, and broadcasters play an important part in shaping political reality. Readers learn not only about a given issue, but also how much importance to attach to that issue from the amount of information in a news story and its position. In reflecting what candidates are saying during a campaign, the mass media may well determine the impirtant issues-that is, the media may set the "agenda" of the compaign.},
	pages = {176--187},
	number = {2},
	journaltitle = {The Public Opinion Quarterly},
	author = {{McCombs}, Maxwell E. and Shaw, Donald L.},
	urldate = {2018-08-14},
	date = {1972}
}

@article{dalessio_media_2000,
	title = {Media Bias in Presidential Elections: A Meta-Analysis},
	volume = {50},
	issn = {0021-9916},
	url = {https://academic.oup.com/joc/article/50/4/133/4110147},
	doi = {10.1111/j.1460-2466.2000.tb02866.x},
	shorttitle = {Media Bias in Presidential Elections},
	abstract = {Abstract.  A meta-analysis considered 59 quantitative studies containing data concerned with partisan media bias in presidential election campaigns since 1948.},
	pages = {133--156},
	number = {4},
	journaltitle = {Journal of Communication},
	shortjournal = {J Commun},
	author = {D'Alessio, Dave and Allen, Mike},
	urldate = {2018-08-14},
	date = {2000-12-01},
	langid = {english},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/Q67V2BQV/D'Alessio und Allen - 2000 - Media Bias in Presidential Elections A Meta-Analy.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/PW67GGZB/4110147.html:text/html}
}

@article{junque_de_fortuny_media_2012,
	title = {Media coverage in times of political crisis: A text mining approach},
	volume = {39},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417412006100},
	doi = {10.1016/j.eswa.2012.04.013},
	shorttitle = {Media coverage in times of political crisis},
	abstract = {At the year end of 2011 Belgium formed a government, after a world record breaking period of 541days of negotiations. We have gathered and analysed 68,000 related on-line news articles published in 2011 in Flemish newspapers. These articles were analysed by a custom-built expert system. The results of our text mining analyses show interesting differences in media coverage and votes for several political parties and politicians. With opinion mining, we are able to automatically detect the sentiment of each article, thereby allowing to visualise how the tone of reporting evolved throughout the year, on a party, politician and newspaper level. Our suggested framework introduces a generic text mining approach to analyse media coverage on political issues, including a set of methodological guidelines, evaluation metrics, as well as open source opinion mining tools. Since all analyses are based on automated text mining algorithms, an objective overview of the manner of reporting is provided. The analysis shows peaks of positive and negative sentiments during key moments in the negotiation process.},
	pages = {11616--11622},
	number = {14},
	journaltitle = {Expert Systems with Applications},
	shortjournal = {Expert Systems with Applications},
	author = {Junqué de Fortuny, Enric and De Smedt, Tom and Martens, David and Daelemans, Walter},
	urldate = {2018-08-14},
	date = {2012-10-15},
	keywords = {Coverage, Data mining, Opinion mining, Politics, Sentiment mining},
	file = {ScienceDirect Snapshot:/Users/franzi/Zotero/storage/KVJFUAGS/S0957417412006100.html:text/html}
}

@article{soelistio_simple_2015,
	title = {Simple Text Mining for Sentiment Analysis of Political Figure Using Naive Bayes Classifier Method},
	url = {http://arxiv.org/abs/1508.05163},
	doi = {10.12962/p9772338185001.a18},
	abstract = {Text mining can be applied to many fields. One of the application is using text mining in digital newspaper to do politic sentiment analysis. In this paper sentiment analysis is applied to get information from digital news articles about its positive or negative sentiment regarding particular politician. This paper suggests a simple model to analyze digital newspaper sentiment polarity using naive Bayes classifier method. The model uses a set of initial data to begin with which will be updated when new information appears. The model showed promising result when tested and can be implemented to some other sentiment analysis problems.},
	journaltitle = {{arXiv}:1508.05163 [cs]},
	author = {Soelistio, Yustinus Eko and Surendra, Martinus Raditia Sigit},
	urldate = {2018-08-15},
	date = {2015-08-20},
	eprinttype = {arxiv},
	eprint = {1508.05163},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	annotation = {Comment: 5 pages, published in the Proceedings of the 7th {ICTS}},
	file = {arXiv\:1508.05163 PDF:/Users/franzi/Zotero/storage/7KG932ER/Soelistio und Surendra - 2015 - Simple Text Mining for Sentiment Analysis of Polit.pdf:application/pdf;arXiv.org Snapshot:/Users/franzi/Zotero/storage/9UUDH8WG/1508.html:text/html}
}

@article{oliveira_can_2017,
	title = {Can social media reveal the preferences of voters? A comparison between sentiment analysis and traditional opinion polls},
	volume = {14},
	issn = {1933-1681},
	url = {https://doi.org/10.1080/19331681.2016.1214094},
	doi = {10.1080/19331681.2016.1214094},
	shorttitle = {Can social media reveal the preferences of voters?},
	abstract = {This study aims to determine whether the results obtained by applying sentiment analysis to data extracted from social media can reveal the political preferences of citizens to a greater degree of accuracy than traditional public opinion surveys. The researcher collected and analyzed 92,441 tweets related to the Brazilian presidential candidates during the second round of elections in 2014. The analysis results were compared with six voter preference polls conducted by the Datafolha Research Institute. The results showed that sentiment analysis may indicate voter preferences at an accuracy that was only 1\% to 8\% different than that of traditional research, which has an average accuracy of 81.05\%.},
	pages = {34--45},
	number = {1},
	journaltitle = {Journal of Information Technology \& Politics},
	author = {Oliveira, Daniel José Silva and Bermejo, Paulo Henrique de Souza and Santos, Pâmela Aparecida dos},
	urldate = {2018-08-15},
	date = {2017-01-02},
	keywords = {social media, Elections, opinion mining, opinion polls, sentiment analysis, Twitter},
	file = {Snapshot:/Users/franzi/Zotero/storage/VE3AT736/19331681.2016.html:text/html}
}

@article{enikolopov_media_2011,
	title = {Media and Political Persuasion: Evidence from Russia},
	volume = {101},
	issn = {0002-8282},
	url = {https://www.jstor.org/stable/41408737},
	shorttitle = {Media and Political Persuasion},
	abstract = {This paper compares electoral outcomes of 1999 parliamentary elections in Russia among geographical areas with differential access to the only national {TV} channel independent from the government It was available to three-quarters of Russia's population and its signal availability was idiosyncratic, conditional on observables. Independent {TV} decreased aggregate vote for the government party by 8.9 percentage points, increased the combined vote for major opposition parties by 6.3 percentage points, and decreased turnout by 3.8 percentage points. The probability of voting for opposition parties increased for individuals who watched independent {TV} even controlling for voting intentions measured one month before elections.},
	pages = {3253--3285},
	number = {7},
	journaltitle = {The American Economic Review},
	author = {Enikolopov, Ruben and Petrova, Maria and Zhuravskaya, Ekaterina},
	urldate = {2018-08-22},
	date = {2011}
}

@report{dellavigna_fox_2006,
	title = {The Fox News Effect: Media Bias and Voting},
	url = {http://www.nber.org/papers/w12169},
	shorttitle = {The Fox News Effect},
	abstract = {Does media bias affect voting? We address this question by looking at the entry of Fox News in cable markets and its impact on voting. Between October 1996 and November 2000, the conservative Fox News Channel was introduced in the cable programming of 20 percent of {US} towns. Fox News availability in 2000 appears to be largely idiosyncratic. Using a data set of voting data for 9,256 towns, we investigate if Republicans gained vote share in towns where Fox News entered the cable market by the year 2000. We find a significant effect of the introduction of Fox News on the vote share in Presidential elections between 1996 and 2000. Republicans gain 0.4 to 0.7 percentage points in the towns which broadcast Fox News. The results are robust to town-level controls, district and county fixed effects, and alternative specifications. We also find a significant effect of Fox News on Senate vote share and on voter turnout. Our estimates imply that Fox News convinced 3 to 8 percent of its viewers to vote Republican. We interpret the results in light of a simple model of voter learning about media bias and about politician quality. The Fox News effect could be a temporary learning effect for rational voters, or a permanent effect for voters subject to non-rational persuasion.},
	number = {12169},
	institution = {National Bureau of Economic Research},
	type = {Working Paper},
	author = {{DellaVigna}, Stefano and Kaplan, Ethan},
	urldate = {2018-08-22},
	date = {2006-04},
	doi = {10.3386/w12169},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/NHXK22WD/DellaVigna und Kaplan - 2006 - The Fox News Effect Media Bias and Voting.pdf:application/pdf}
}

@article{snyder_press_2010,
	title = {Press Coverage and Political Accountability},
	volume = {118},
	issn = {0022-3808},
	url = {https://www.jstor.org/stable/10.1086/652903},
	doi = {10.1086/652903},
	abstract = {We estimate the impact of press coverage on citizen knowledge, politicians’ actions, and policy. We find that voters living in areas where, for exogenous reasons, the press covers their U.S. House representative less are less likely to recall their representative’s name and less able to describe and rate him or her. Congressmen who are less covered by the local press work less for their constituencies: they are less likely to stand witness before congressional hearings, to serve on constituency‐oriented committees (perhaps), and to vote against the party line. Finally, federal spending is lower in areas with exogenously lower press coverage of congressmen.},
	pages = {355--408},
	number = {2},
	journaltitle = {Journal of Political Economy},
	author = {Snyder, James M. and Strömberg, David},
	urldate = {2018-08-22},
	date = {2010},
	file = {JSTOR Full Text PDF:/Users/franzi/Zotero/storage/J8FIZV9G/Snyder und Strömberg - 2010 - Press Coverage and Political Accountability.pdf:application/pdf}
}

@article{brandenburg_party_2006,
	title = {Party Strategy and Media Bias: A Quantitative Analysis of the 2005 {UK} Election Campaign},
	volume = {16},
	issn = {1745-7289},
	url = {https://doi.org/10.1080/13689880600716027},
	doi = {10.1080/13689880600716027},
	shorttitle = {Party Strategy and Media Bias},
	abstract = {This article investigates the current state of press partisanship in the {UK}. Utilizing content analysis data from the 2005 General Election campaign, recent hypotheses about press dealignment are tested with quantitative methods. Partisan tendencies in reporting are measured in terms of coverage bias, statement bias, and agenda bias. As the governing party, Labour benefits from coverage bias in all papers, while the Liberal Democrats remain marginalized. It can be shown that increasingly ambiguous endorsements in broadsheet and tabloid press alike translate into a general absence of open support for political parties. At best, endorsed parties receive neutral treatment, with their opponents being harshly criticized. Partisan tendencies do, however, manifest themselves in other patterns of campaign coverage. Even weakly partisan papers engage in strategic behaviour, most notably by reinforcing the issue agendas of endorsed parties. With both the Independent and the Guardian lending strategic support to the Liberal Democrats, and the Murdoch press being largely non‐committal, the analysis hints at an erosion of support for New Labour.},
	pages = {157--178},
	number = {2},
	journaltitle = {Journal of Elections, Public Opinion and Parties},
	author = {Brandenburg, Heinz},
	urldate = {2018-09-25},
	date = {2006-07-01},
	file = {Snapshot:/Users/franzi/Zotero/storage/DJPIC8M4/13689880600716027.html:text/html}
}

@article{druckman_impact_2005,
	title = {The Impact of Media Bias: How Editorial Slant Affects Voters},
	volume = {67},
	issn = {0022-3816},
	url = {https://www.journals.uchicago.edu/doi/full/10.1111/j.1468-2508.2005.00349.x},
	doi = {10.1111/j.1468-2508.2005.00349.x},
	shorttitle = {The Impact of Media Bias},
	abstract = {We investigate how editorial slant—defined as the quantity and tone of a newspaper's candidate coverage as influenced by its editorial position—shapes candidate evaluations and vote choice. We avoid various methodological pitfalls by focusing on a single Senate campaign in a single market with two competing, editorially distinct newspapers. Combining comprehensive content analyses of the papers with an Election Day exit poll, we assess the slant of campaign coverage and its effects on voters. We find compelling evidence that editorial slant influences voters’ decisions. Our results raise serious questions about the media's place in democratic processes.},
	pages = {1030--1049},
	number = {4},
	journaltitle = {The Journal of Politics},
	shortjournal = {The Journal of Politics},
	author = {Druckman, James N. and Parkin, Michael},
	urldate = {2018-09-25},
	date = {2005-11-01},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/PURKC6GV/Druckman und Parkin - 2005 - The Impact of Media Bias How Editorial Slant Affe.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/V5PB5D3R/j.1468-2508.2005.00349.html:text/html}
}

@article{ravi_survey_2015,
	title = {A survey on opinion mining and sentiment analysis: Tasks, approaches and applications},
	volume = {89},
	issn = {0950-7051},
	url = {http://www.sciencedirect.com/science/article/pii/S0950705115002336},
	doi = {10.1016/j.knosys.2015.06.015},
	shorttitle = {A survey on opinion mining and sentiment analysis},
	abstract = {With the advent of Web 2.0, people became more eager to express and share their opinions on web regarding day-to-day activities and global issues as well. Evolution of social media has also contributed immensely to these activities, thereby providing us a transparent platform to share views across the world. These electronic Word of Mouth ({eWOM}) statements expressed on the web are much prevalent in business and service industry to enable customer to share his/her point of view. In the last one and half decades, research communities, academia, public and service industries are working rigorously on sentiment analysis, also known as, opinion mining, to extract and analyze public mood and views. In this regard, this paper presents a rigorous survey on sentiment analysis, which portrays views presented by over one hundred articles published in the last decade regarding necessary tasks, approaches, and applications of sentiment analysis. Several sub-tasks need to be performed for sentiment analysis which in turn can be accomplished using various approaches and techniques. This survey covering published literature during 2002–2015, is organized on the basis of sub-tasks to be performed, machine learning and natural language processing techniques used and applications of sentiment analysis. The paper also presents open issues and along with a summary table of a hundred and sixty-one articles.},
	pages = {14--46},
	journaltitle = {Knowledge-Based Systems},
	shortjournal = {Knowledge-Based Systems},
	author = {Ravi, Kumar and Ravi, Vadlamani},
	urldate = {2018-10-11},
	date = {2015-11-01},
	keywords = {Opinion mining, Lexica creation, Machine learning, Micro blog, Ontology, Sentiment analysis, Social media},
	file = {ScienceDirect Snapshot:/Users/franzi/Zotero/storage/VYW3NR9I/S0950705115002336.html:text/html}
}

@article{boumans_taking_2016,
	title = {Taking Stock of the Toolkit},
	volume = {4},
	issn = {2167-0811},
	url = {https://doi.org/10.1080/21670811.2015.1096598},
	doi = {10.1080/21670811.2015.1096598},
	abstract = {When analyzing digital journalism content, journalism scholars are confronted with a number of substantial differences compared to traditional journalistic content. The sheer amount of data and the unique features of digital content call for the application of valuable new techniques. Various other scholarly fields are already applying computational methods to study digital journalism data. Often, their research interests are closely related to those of journalism scholars. Despite the advantages that computational methods have over traditional content analysis methods, they are not commonplace in digital journalism studies. To increase awareness of what computational methods have to offer, we take stock of the toolkit and show the ways in which computational methods can aid journalism studies. Distinguishing between dictionary-based approaches, supervised machine learning, and unsupervised machine learning, we present a systematic inventory of recent applications both inside as well as outside journalism studies. We conclude with suggestions for how the application of new techniques can be encouraged.},
	pages = {8--23},
	number = {1},
	journaltitle = {Digital Journalism},
	author = {Boumans, Jelle W. and Trilling, Damian},
	urldate = {2018-10-11},
	date = {2016-01-02},
	keywords = {computational social science, automated content analysis, digital data, journalism studies, review},
	file = {Snapshot:/Users/franzi/Zotero/storage/JAPM52RU/21670811.2015.html:text/html}
}

@article{soelistio_simple_2015-1,
	title = {Simple Text Mining for Sentiment Analysis of Political Figure Using Naive Bayes Classifier Method},
	url = {http://arxiv.org/abs/1508.05163},
	doi = {10.12962/p9772338185001.a18},
	abstract = {Text mining can be applied to many fields. One of the application is using text mining in digital newspaper to do politic sentiment analysis. In this paper sentiment analysis is applied to get information from digital news articles about its positive or negative sentiment regarding particular politician. This paper suggests a simple model to analyze digital newspaper sentiment polarity using naive Bayes classifier method. The model uses a set of initial data to begin with which will be updated when new information appears. The model showed promising result when tested and can be implemented to some other sentiment analysis problems.},
	journaltitle = {{arXiv}:1508.05163 [cs]},
	author = {Soelistio, Yustinus Eko and Surendra, Martinus Raditia Sigit},
	urldate = {2018-10-11},
	date = {2015-08-20},
	eprinttype = {arxiv},
	eprint = {1508.05163},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	annotation = {Comment: 5 pages, published in the Proceedings of the 7th {ICTS}},
	file = {arXiv\:1508.05163 PDF:/Users/franzi/Zotero/storage/JQX3ZGVX/Soelistio und Surendra - 2015 - Simple Text Mining for Sentiment Analysis of Polit.pdf:application/pdf;arXiv.org Snapshot:/Users/franzi/Zotero/storage/IN7HF2YE/1508.html:text/html}
}

@article{junque_de_fortuny_evaluating_2014,
	title = {Evaluating and understanding text-based stock price prediction models},
	volume = {50},
	issn = {0306-4573},
	url = {http://www.sciencedirect.com/science/article/pii/S0306457313001143},
	doi = {10.1016/j.ipm.2013.12.002},
	abstract = {Despite the fact that both the Efficient Market Hypothesis and Random Walk Theory postulate that it is impossible to predict future stock prices based on currently available information, recent advances in empirical research have been proving the opposite by achieving what seems to be better than random prediction performance. We discuss some of the (dis)advantages of the most widely used performance metrics and conclude that is difficult to assess the external validity of performance using some of these measures. Moreover, there remain many questions as to the real-world applicability of these empirical models. In the first part of this study we design novel stock price prediction models, based on state-of-the-art text-mining techniques to assert whether we can predict the movement of stock prices more accurately by including indicators of irrationality. Along with this, we discuss which metrics are most appropriate for which scenarios in order to evaluate the models. Finally, we discuss how to gain insight into text-mining-based stock price prediction models in order to evaluate, validate and refine the models.},
	pages = {426--441},
	number = {2},
	journaltitle = {Information Processing \& Management},
	shortjournal = {Information Processing \& Management},
	author = {Junqué de Fortuny, Enric and De Smedt, Tom and Martens, David and Daelemans, Walter},
	urldate = {2018-10-11},
	date = {2014-03-01},
	keywords = {Sentiment mining, Evaluation, Prediction, Stock, Support Vector Machine},
	file = {ScienceDirect Snapshot:/Users/franzi/Zotero/storage/WENRITVN/S0306457313001143.html:text/html}
}

@article{ngai_review_2016,
	title = {A {REVIEW} {OF} {THE} {LITERATURE} {ON} {APPLICATIONS} {OF} {TEXT} {MINING} {IN} {POLICY} {MAKING}},
	url = {https://aisel.aisnet.org/pacis2016/343},
	journaltitle = {{PACIS} 2016 Proceedings},
	author = {Ngai, E. W. T. and Lee, P. T. Y.},
	date = {2016-06-26},
	file = {"A REVIEW OF THE LITERATURE ON APPLICATIONS OF TEXT MINING IN POLICY MA" by E. W. T. Ngai and P.T.Y. Lee:/Users/franzi/Zotero/storage/WA8JR4XD/343.html:text/html}
}

@inproceedings{esuli_sentiwordnet:_2006,
	title = {{SENTIWORDNET}: A Publicly Available Lexical Resource for Opinion Mining},
	shorttitle = {{SENTIWORDNET}},
	abstract = {Opinion mining ({OM}) is a recent subdiscipline at the crossroads of information retrieval and computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses. {OM} has a rich set of applications, ranging from tracking users' opinions about products or about political candidates as expressed in online forums, to customer relationship management. In order to aid the extraction of opinions from text, recent research has tried to automatically determine the "{PN}-polarity" of subjective terms, i.e. identify whether a term that is a marker of opinionated content has a positive or a negative connotation. Research on determining whether a term is indeed a marker of opinionated content (a subjective term) or not (an objective term) has been, instead, much more scarce. In this work we describe {SENTIWORDNET}, a lexical resource in which each {WORDNET} synset s is associated to three numerical scores Obj(s), Pos(s) and Neg(s), describing how objective, positive, and negative the terms contained in the synset are. The method used to develop {SENTIWORDNET} is based on the quantitative analysis of the glosses associated to synsets, and on the use of the resulting vectorial term representations for semi-supervised synset classification. The three scores are derived by combining the results produced by a committee of eight ternary classifiers, all characterized by similar accuracy levels but different classification behaviour. {SENTIWORDNET} is freely available for research purposes, and is endowed with a Web-based graphical user interface.},
	pages = {417--422},
	booktitle = {In Proceedings of the 5th Conference on Language Resources and Evaluation ({LREC}’06},
	author = {Esuli, Andrea and Sebastiani, Fabrizio},
	date = {2006},
	file = {Citeseer - Full Text PDF:/Users/franzi/Zotero/storage/5ZZV2UAA/Esuli und Sebastiani - 2006 - SENTIWORDNET A Publicly Available Lexical Resourc.pdf:application/pdf;Citeseer - Snapshot:/Users/franzi/Zotero/storage/HP7P833M/summary.html:text/html}
}

@article{holig_reuters_2018,
	title = {Reuters Institute Digital News Report 2018 – Ergebnisse für Deutschland},
	volume = {44},
	issn = {1435‐9413},
	journaltitle = {Arbeitspapiere des Hans‐Bredow‐ Instituts},
	author = {Hölig, Sascha and Hasebrink, Uwe},
	date = {2018-06}
}

@article{eberl_one_2017,
	title = {One Bias Fits All? Three Types of Media Bias and Their Effects on Party Preferences},
	volume = {44},
	issn = {0093-6502},
	url = {https://doi.org/10.1177/0093650215614364},
	doi = {10.1177/0093650215614364},
	shorttitle = {One Bias Fits All?},
	abstract = {Bias in political news coverage may have a profound influence on voter opinions and preferences. However, the concept of media bias actually encompasses different sub-types: Visibility bias is the salience of political actors, tonality bias the evaluation of these actors, and agenda bias the extent to which parties address preferred issues in media coverage. The present study is the first to explore how each type of bias influences party preferences. Using data from the Austrian parliamentary election campaign of 2013, we combine an online panel survey (n = 1,285) with measures of media bias from content analyses of party press releases (n = 1,922) and media coverage in eight newspapers (n = 6,970). We find substantial effects on party preferences for tonality bias and agenda bias, while visibility bias has no clear impact. Voters who are less politically sophisticated and lack a party identification are more susceptible to bias, and media bias can also reinforce existing partisan identities.},
	pages = {1125--1148},
	number = {8},
	journaltitle = {Communication Research},
	shortjournal = {Communication Research},
	author = {Eberl, Jakob-Moritz and Boomgaarden, Hajo G. and Wagner, Markus},
	urldate = {2018-10-20},
	date = {2017-12-01},
	langid = {english},
	file = {SAGE PDF Full Text:/Users/franzi/Zotero/storage/X2Z8NJYK/Eberl et al. - 2017 - One Bias Fits All Three Types of Media Bias and T.pdf:application/pdf}
}

@article{ferree_four_2002,
	title = {Four Models of the Public Sphere in Modern Democracies},
	volume = {31},
	issn = {0304-2421},
	url = {https://www.jstor.org/stable/658129},
	pages = {289--324},
	number = {3},
	journaltitle = {Theory and Society},
	author = {Ferree, Myra Marx and Gamson, William A. and Gerhards, Jürgen and Rucht, Dieter},
	urldate = {2018-10-20},
	date = {2002}
}

@article{stromback_four_2008,
	title = {Four Phases of Mediatization: An Analysis of the Mediatization of Politics},
	volume = {13},
	issn = {1940-1612},
	url = {https://doi.org/10.1177/1940161208319097},
	doi = {10.1177/1940161208319097},
	shorttitle = {Four Phases of Mediatization},
	abstract = {Two concepts that have been used to describe the changes with regards to media and politics during the last fifty years are the concepts of mediation and mediatization . However, both these concepts are used more often than they are properly defined. Moreover, there is a lack of analysis of the process of mediatization, although the concept as such denotes a process.Thus the purpose of this article is to analyze the concepts of mediated and mediatized politics from a process-oriented perspective. The article argues that mediatization is a multidimensional and inherently process-oriented concept and that it is possible to make a distinction between four phases of mediatization. Each of these phases is analyzed.The conclusion is that as politics becomes increasingly mediatized, the important question no longer is related to the independence of the media from politics and society. The important question becomes the independence of politics and society from the media.},
	pages = {228--246},
	number = {3},
	journaltitle = {The International Journal of Press/Politics},
	shortjournal = {The International Journal of Press/Politics},
	author = {Strömbäck, Jesper},
	urldate = {2018-10-20},
	date = {2008-07-01},
	langid = {english},
	file = {SAGE PDF Full Text:/Users/franzi/Zotero/storage/IHSBRJN6/Strömbäck - 2008 - Four Phases of Mediatization An Analysis of the M.pdf:application/pdf}
}

@article{mccombs_look_2005,
	title = {A Look at Agenda-setting: past, present and future},
	volume = {6},
	issn = {1461-670X},
	url = {https://doi.org/10.1080/14616700500250438},
	doi = {10.1080/14616700500250438},
	shorttitle = {A Look at Agenda-setting},
	pages = {543--557},
	number = {4},
	journaltitle = {Journalism Studies},
	author = {{McCombs}, Maxwell},
	urldate = {2018-10-20},
	date = {2005-11-01},
	file = {Snapshot:/Users/franzi/Zotero/storage/ZHVLE86I/14616700500250438.html:text/html}
}

@article{lenz_looking_2011,
	title = {Looking the Part: Television Leads Less Informed Citizens to Vote Based on Candidates’ Appearance},
	volume = {55},
	rights = {©2011, Midwest Political Science Association},
	issn = {1540-5907},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-5907.2011.00511.x},
	doi = {10.1111/j.1540-5907.2011.00511.x},
	shorttitle = {Looking the Part},
	abstract = {As long as there has been democratic government, skeptics have worried that citizens would base their choices and their votes on superficial considerations. A series of recent studies seems to validate these fears, suggesting that candidates who merely look more capable or attractive perform better in elections. In this article, we examine the underlying process behind the appearance effect. Specifically, we test whether the effect of appearance is more pronounced among those who know little about politics but are exposed to visual images of candidates. To do so, we combine appearance-based assessments of U.S. Senate and gubernatorial candidates with individual-level survey data measuring vote intent, political knowledge, and television exposure. Confirming long-standing concerns about image and television, we find that appealing-looking politicians benefit disproportionately from television exposure, primarily among less knowledgeable individuals.},
	pages = {574--589},
	number = {3},
	journaltitle = {American Journal of Political Science},
	author = {Lenz, Gabriel S. and Lawson, Chappell},
	urldate = {2018-10-20},
	date = {2011-07-01},
	langid = {english},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/JT5RJNTV/Lenz und Lawson - 2011 - Looking the Part Television Leads Less Informed C.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/EHR2VTFT/j.1540-5907.2011.00511.html:text/html}
}

@article{prior_incumbent_2006,
	title = {The Incumbent in the Living Room: The Rise of Television and the Incumbency Advantage in U.S. House Elections},
	volume = {68},
	issn = {1468-2508},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-2508.2006.00452.x},
	doi = {10.1111/j.1468-2508.2006.00452.x},
	shorttitle = {The Incumbent in the Living Room},
	abstract = {This study shows that the growth of television contributed to the rise in the incumbency advantage in U.S. House elections during the 1960s. Incumbents received positive coverage throughout their term and were generally more newsworthy and better funded than their challengers during the campaign. Less-educated voters, for whom television presented a new, less demanding source of news, were most affected by local television. Analysis of National Elections Studies data reveals that less-educated respondents were more knowledgeable about the incumbent and more likely to vote for the incumbent in districts with television stations. Aggregate analysis shows that incumbents’ vote margins increased in proportion to the number of television stations in their districts.},
	pages = {657--673},
	number = {3},
	journaltitle = {Journal of Politics},
	author = {Prior, Markus},
	urldate = {2018-10-20},
	date = {2006-08-01},
	langid = {english},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/YW6ETPHF/Prior - 2006 - The Incumbent in the Living Room The Rise of Tele.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/RKPHGYD2/j.1468-2508.2006.00452.html:text/html}
}

@article{boomgaarden_reporting_2007,
	title = {Reporting Germany's 2005 Bundestag Election Campaign: Was Gender an Issue?},
	volume = {12},
	pages = {154--171},
	number = {4},
	journaltitle = {Harvard International Journal of Press/Politics},
	author = {Boomgaarden, Hajo G. and Semetko, Holli A.},
	date = {2007-10-01}
}

@article{lengauer_candidate_2013,
	title = {Candidate and party bias in the news and its effects on party choice: Evidence from Austria},
	volume = {13},
	issn = {1424-4896},
	url = {http://www.sciencedirect.com/science/article/pii/S1424489613000143},
	doi = {10.1016/j.scoms.2013.04.011},
	shorttitle = {Candidate and party bias in the news and its effects on party choice},
	abstract = {This paper investigates the impact of exposure to actor-related news bias on party choice in the 2008 Austrian elections. More specifically, this work introduces relative and relational measures of party and candidate biases in electoral reporting and outlines their effects on vote choice. The applied integrative understanding of party and candidate bias in the news, combining visibility and valence, results in a single measure classifying parties and party leaders in media-outlet-specific bias spectra. To measure bias exposure effects on party choice, the candidate and party bias levels in each newspaper are weighted with the individual exposure to these papers. This study rests on the analytical linkage between a media content analysis of Austrian daily newspapers and a representative post-election survey. Firstly, this investigation shows that the Austrian newspaper coverage is characterized by clear, not uniform but rather outlet-specific, biases toward parties and their leaders. Above all, the tabloid press primarily focuses on a strong polarization and duelization regarding the parties and the leaders contesting for chancellorship. Secondly, in a case study investigating the effects of news bias exposure on party choice regarding the governing parties, we find a positive relation between high exposure to advantageous party and candidate biases in newspapers and the probability to vote for the respective party.},
	pages = {41--49},
	number = {1},
	journaltitle = {Studies in Communication Sciences},
	shortjournal = {Studies in Communication Sciences},
	author = {Lengauer, Günther and Johann, David},
	urldate = {2018-10-20},
	date = {2013-01-01},
	keywords = {Content analysis, Election news, Media bias, Media coverage, Media effects, Party choice, Voting behavior},
	file = {ScienceDirect Snapshot:/Users/franzi/Zotero/storage/C3LEBPI7/S1424489613000143.html:text/html}
}

@article{de_vreese_valenced_2006,
	title = {Valenced news frames and public support for the {EU}},
	volume = {28},
	issn = {1613-4087},
	url = {https://www.degruyter.com/view/j/comm.2003.28.issue-4/comm.2003.024/comm.2003.024.xml},
	doi = {10.1515/comm.2003.024},
	pages = {361--381},
	number = {4},
	journaltitle = {Communications},
	author = {de Vreese, Claes and Boomgaarden, Hajo G.},
	urldate = {2018-10-20},
	date = {2006},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/NKPLQKWU/2006 - Valenced news frames and public support for the EU.pdf:application/pdf}
}

@article{boomgaarden_nachrichten-bias:_2012,
	title = {Nachrichten-Bias: Medieninhalte, Bevölkerungswahrnehmungen und Wahlentscheidungen bei der Bundestagswahl 2009},
	url = {https://dare.uva.nl/search?identifier=abe81e9d-edf1-49d4-af5b-9c06183a576e},
	shorttitle = {Nachrichten-Bias},
	pages = {442--464},
	journaltitle = {Politische Vierteljahresschrift. Sonderheft},
	author = {Boomgaarden, H. G. and Semetko, H. A.},
	urldate = {2018-10-20},
	date = {2012},
	langid = {english},
	file = {Snapshot:/Users/franzi/Zotero/storage/R2USHNUR/search.html:text/html}
}

@article{brandenburg_political_2005,
	title = {Political Bias in the Irish Media: A Quantitative Study of Campaign Coverage during the 2002 General Election},
	volume = {20},
	issn = {0790-7184},
	url = {https://doi.org/10.1080/07907180500359350},
	doi = {10.1080/07907180500359350},
	shorttitle = {Political Bias in the Irish Media},
	abstract = {The aim of this study is to give some systematic insights into how Irish media tend to report an election campaign. The main focus will be on their attitudes toward and treatment of the competing parties and candidates. Content analysis data from television newscasts and campaign stories in four of the largest newspapers is used to investigate three different forms of media bias: coverage bias, agenda bias, and statement bias. We find that Irish media tend to grant disproportionate amounts of coverage to the government parties, Fianna Fáil and the Progressive Democrats; the more prominent the coverage, the less proportionate it becomes. The extent to which media take the freedom to ‘distort’ party agendas in their reporting appears to depend on party size, campaign strategy and the acquired status and acceptance of a party amongst the political and media establishment. Most notable, however, is the predominantly negative attitude of all Irish print media towards political actors. Instead of a polarised partisan press, as for example in the {UK}, in Ireland we seem to be faced with a rather homogenous anti‐politics bias.},
	pages = {297--322},
	number = {3},
	journaltitle = {Irish Political Studies},
	author = {Brandenburg, Heinz},
	urldate = {2018-10-20},
	date = {2005-09-01},
	file = {Snapshot:/Users/franzi/Zotero/storage/TS2G9CU6/07907180500359350.html:text/html}
}

@article{bennett_new_2008,
	title = {A New Era of Minimal Effects? The Changing Foundations of Political Communication},
	volume = {58},
	rights = {© 2008 International Communication Association},
	issn = {1460-2466},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1460-2466.2008.00410.x},
	doi = {10.1111/j.1460-2466.2008.00410.x},
	shorttitle = {A New Era of Minimal Effects?},
	pages = {707--731},
	number = {4},
	journaltitle = {Journal of Communication},
	author = {Bennett, W. Lance and Iyengar, Shanto},
	urldate = {2018-10-20},
	date = {2008-12-01},
	langid = {english},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/G2DCD998/Bennett und Iyengar - 2008 - A New Era of Minimal Effects The Changing Foundat.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/9D8T2CAU/j.1460-2466.2008.00410.html:text/html}
}

@inproceedings{holbert_new_2010,
	title = {A New Era of Minimal Effects ? A Response to Bennett and Iyengar},
	shorttitle = {A New Era of Minimal Effects ?},
	abstract = {This article takes up Bennett and Iyengar’s (2008) call for debate about the future of political communication effects research. We outline 4 key criticisms. First, Bennett and Iyengar are too quick to dismiss the importance of attitude reinforcement, long recognized as an important type of political media influence. Second, the authors take too narrow a view of the sources of political information, remaining fixated on news. Third, they offer an incomplete portrayal of selective exposure, exaggerating the extent to which individuals avoid attitudediscrepant information. Finally, they lean toward determinism when describing the role technologies play in shaping our political environment. In addition, we challenge Bennett and Iyengar’s assertion that only brand new theory can serve to help researchers understand today’s political communication landscape. We argue that existing tools, notably the Elaboration Likelihood Model ({ELM}), retain much utility for examining political media effects. Contrary to Bennett and Iyengar’s claims, the {ELM} suggests that the contemporary political information environment does not necessarily lead to minimal effects.},
	author = {Holbert, R. Lance and Garrett, R. Kelly and Gleason, Laurel S.},
	date = {2010},
	keywords = {Assertion (software development), Elaboration likelihood model, Noise shaping},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/6V2NVRW3/Holbert et al. - 2010 - A New Era of Minimal Effects  A Response to Benne.pdf:application/pdf}
}

@article{hopmann_political_2012,
	title = {Political balance in the news: A review of concepts, operationalizations and key findings},
	volume = {13},
	issn = {1464-8849},
	url = {https://doi.org/10.1177/1464884911427804},
	doi = {10.1177/1464884911427804},
	shorttitle = {Political balance in the news},
	abstract = {Balance is a notoriously difficult concept to operationalize. It has typically been investigated by examining the issues raised in elections, as well as the volume and favorability of coverage of political actors. However, even after collecting these measures, it is difficult to determine precisely what would constitute ‘balanced’ coverage. Based on a comprehensive overview of previous research in western democracies, we argue that political balance can be defined according to a political system perspective (where coverage reflects politically defined norms or regulation) or a media routine perspective (where coverage results from journalistic norms). Unless forced to follow norms, western broadcasting seems to comply with a media routine perspective. Empirically, newspaper coverage is sometimes imbalanced according to both perspectives. Finally, we discuss why only a systematic analysis of explanations across time and space makes it possible to determine whether politically ‘imbalanced’ news is the result of partisan bias or not.},
	pages = {240--257},
	number = {2},
	journaltitle = {Journalism},
	shortjournal = {Journalism},
	author = {Hopmann, David Nicolas and Van Aelst, Peter and Legnante, Guido},
	urldate = {2018-11-10},
	date = {2012-02-01},
	langid = {english}
}

@article{balahur_sentiment_2013,
	title = {Sentiment Analysis in the News},
	url = {http://arxiv.org/abs/1309.6202},
	abstract = {Recent years have brought a significant growth in the volume of research in sentiment analysis, mostly on highly subjective text types (movie or product reviews). The main difference these texts have with news articles is that their target is clearly defined and unique across the text. Following different annotation efforts and the analysis of the issues encountered, we realised that news opinion mining is different from that of other text types. We identified three subtasks that need to be addressed: definition of the target; separation of the good and bad news content from the good and bad sentiment expressed on the target; and analysis of clearly marked opinion that is expressed explicitly, not needing interpretation or the use of world knowledge. Furthermore, we distinguish three different possible views on newspaper articles - author, reader and text, which have to be addressed differently at the time of analysing sentiment. Given these definitions, we present work on mining opinions about entities in English language news, in which (a) we test the relative suitability of various sentiment dictionaries and (b) we attempt to separate positive or negative opinion from good or bad news. In the experiments described here, we tested whether or not subject domain-defining vocabulary should be ignored. Results showed that this idea is more appropriate in the context of news opinion mining and that the approaches taking this into consideration produce a better performance.},
	journaltitle = {{arXiv}:1309.6202 [cs]},
	author = {Balahur, Alexandra and Steinberger, Ralf and Kabadjov, Mijail and Zavarella, Vanni and van der Goot, Erik and Halkia, Matina and Pouliquen, Bruno and Belyaeva, Jenya},
	urldate = {2018-11-15},
	date = {2013-09-24},
	eprinttype = {arxiv},
	eprint = {1309.6202},
	keywords = {Computer Science - Computation and Language, H.3.1, H.3.3, I.2.7, J.4},
	file = {arXiv\:1309.6202 PDF:/Users/franzi/Zotero/storage/AGKXBNSY/Balahur et al. - 2013 - Sentiment Analysis in the News.pdf:application/pdf;arXiv.org Snapshot:/Users/franzi/Zotero/storage/UKL3LFKZ/1309.html:text/html}
}

@article{hurtikova_importance_2017,
	title = {The Importance of Valence-Framing in the Process of Political Communicati on: Effects on the Formation of Political Attitudes among Viewers of Television News in the Czech Republic {\textbar} Media Studies},
	volume = {8},
	url = {https://hrcak.srce.hr/ojs/index.php/medijske-studije/article/view/6200},
	shorttitle = {The Importance of Valence-Framing in the Process of Political Communicati on},
	number = {15},
	author = {Hurtíková, Hanna},
	urldate = {2018-11-16},
	date = {2017-12-21},
	langid = {american},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/RA2RTB5M/The Importance of Valence-Framing in the Process o.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/76ZUI94X/6200.html:text/html}
}

@article{groseclose_measure_2005,
	title = {A Measure of Media Bias},
	volume = {120},
	issn = {0033-5533},
	url = {https://www.jstor.org/stable/25098770},
	abstract = {[We measure media bias by estimating ideological scores for several major media outlets. To compute this, we count the times that a particular media outlet cites various think tanks and policy groups, and then compare this with the times that members of Congress cite the same groups. Our results show a strong liberal bias: all of the news outlets we examine, except Fox News' Special Report and the Washington Times, received scores to the left of the average member of Congress. Consistent with claims made by conservative critics, {CBS} Evening News and the New York Times received scores far to the left of center. The most centrist media outlets were {PBS} {NewsHour}, {CNN}'s Newsnight, and {ABC}'s Good Morning America; among print outlets, {USA} Today was closest to the center. All of our findings refer strictly to news content; that is, we exclude editorials, letters, and the like.]},
	pages = {1191--1237},
	number = {4},
	journaltitle = {The Quarterly Journal of Economics},
	author = {Groseclose, Tim and Milyo, Jeffrey},
	urldate = {2019-01-07},
	date = {2005}
}

@article{gentzkow_what_2010,
	title = {What Drives Media Slant? Evidence From U.S. Daily Newspapers},
	volume = {78},
	rights = {© 2010 The Econometric Society},
	issn = {1468-0262},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA7195},
	doi = {10.3982/ECTA7195},
	shorttitle = {What Drives Media Slant?},
	abstract = {We construct a new index of media slant that measures the similarity of a news outlet's language to that of a congressional Republican or Democrat. We estimate a model of newspaper demand that incorporates slant explicitly, estimate the slant that would be chosen if newspapers independently maximized their own profits, and compare these profit-maximizing points with firms' actual choices. We find that readers have an economically significant preference for like-minded news. Firms respond strongly to consumer preferences, which account for roughly 20 percent of the variation in measured slant in our sample. By contrast, the identity of a newspaper's owner explains far less of the variation in slant.},
	pages = {35--71},
	number = {1},
	journaltitle = {Econometrica},
	author = {Gentzkow, Matthew and Shapiro, Jesse M.},
	urldate = {2019-01-07},
	date = {2010},
	langid = {english},
	keywords = {Bias, media ownership, text categorization},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/9S2AALCS/Gentzkow and Shapiro - 2010 - What Drives Media Slant Evidence From U.S. Daily .pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/5AGBX3N8/ECTA7195.html:text/html}
}

@article{lott_is_2014,
	title = {Is newspaper coverage of economic events politically biased?},
	volume = {160},
	issn = {1573-7101},
	url = {https://doi.org/10.1007/s11127-014-0171-5},
	doi = {10.1007/s11127-014-0171-5},
	abstract = {This paper develops an econometric technique to test for political bias in news reports that controls for the underlying character of the news reported. Because of the changing availability of the number of newspapers in Nexis/Lexis, two sets of time are examined: from January 1991 to May 2004 and from January 1985 to May 2004. Our results suggest that American newspapers tend to give more positive coverage to the same economic news when Democrats are in the White House than when Republicans are; a similar though smaller effect is found for Democratic control of Congress. Our results reject the claim that “reader diversity is a powerful force toward accuracy.” When all types of news are pooled into a single analysis, our results are significant. However, the results vary greatly depending upon which types of economic data are being reported. When newspapers are examined individually the only support that Republicans appear to obtain is from the president’s home state newspapers during his term. This is true for the Houston Chronicle under both Bushes and the Los Angeles Times during the Reagan administration. Contrary to rational expectations, media coverage affects people’s perceptions of the economy.},
	pages = {65--108},
	number = {1},
	journaltitle = {Public Choice},
	shortjournal = {Public Choice},
	author = {Lott, John R. and Hassett, Kevin A.},
	urldate = {2019-01-07},
	date = {2014-07-01},
	langid = {english},
	keywords = {Media bias, Campaigns, Voting},
	file = {Springer Full Text PDF:/Users/franzi/Zotero/storage/EDJFMSP7/Lott and Hassett - 2014 - Is newspaper coverage of economic events political.pdf:application/pdf}
}

@article{stromberg_radios_2004,
	title = {Radio's Impact on Public Spending},
	volume = {119},
	issn = {0033-5533},
	url = {https://academic.oup.com/qje/article/119/1/189/1876059},
	doi = {10.1162/003355304772839560},
	abstract = {Abstract.  If informed voters receive favorable policies, then the invention of a new mass medium may affect government policies since it affects who is informe},
	pages = {189--221},
	number = {1},
	journaltitle = {The Quarterly Journal of Economics},
	shortjournal = {Q J Econ},
	author = {Strömberg, David},
	urldate = {2019-01-11},
	date = {2004-02-01},
	langid = {english},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/NUNIULNN/Strömberg - 2004 - Radio's Impact on Public Spending.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/UAJ5MGAP/1876059.html:text/html}
}

@article{gentzkow_media_2004,
	title = {Media, Education and Anti-Americanism in the Muslim World},
	volume = {18},
	issn = {0895-3309},
	url = {https://www.aeaweb.org/articles?id=10.1257/0895330042162313},
	doi = {10.1257/0895330042162313},
	abstract = {Recent surveys in the United States and the Muslim world show widespread misinformation about the events of September 11, 2001. Using data from 9 predominantly Muslim countries, we study how such beliefs depend on exposure to news media and levels of education. Standard economic theory would predict that increased access to information should cause beliefs to converge. More recent models of biased belief formation suggest that this result might hinge critically on who is providing the information. Consistent with the latter, we find that overall intensity of media use and level of education have at best a weak correlation with beliefs, while particular information sources have strong and divergent effects. Compared to those with little media exposure or schooling, individuals watching Arab news channels or educated in schools with little Western influence are less likely to agree that the September 11 attacks were carried out by Arab terrorists. Those exposed to media or education from Western sources are more likely to agree. Belief that the attacks were morally justified and general attitudes toward the {US} are also strongly correlated with source of information. These findings survive controls for demographic characteristics and are robust to identifying media effects using cross-country variation in language.},
	pages = {117--133},
	number = {3},
	journaltitle = {Journal of Economic Perspectives},
	author = {Gentzkow, Matthew A. and Shapiro, Jesse M.},
	urldate = {2019-01-11},
	date = {2004-09},
	langid = {english},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/DBBKM6N8/Gentzkow and Shapiro - 2004 - Media, Education and Anti-Americanism in the Musli.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/5N7JWXRT/articles.html:text/html}
}

@article{gerber_does_2009,
	title = {Does the Media Matter? A Field Experiment Measuring the Effect of Newspapers on Voting Behavior and Political Opinions},
	volume = {1},
	issn = {1945-7782},
	url = {https://www.aeaweb.org/articles?id=10.1257/app.1.2.35},
	doi = {10.1257/app.1.2.35},
	shorttitle = {Does the Media Matter?},
	abstract = {We conducted a field experiment to measure the effect of exposure
to newspapers on political behavior and opinion. Before the 2005
Virginia gubernatorial election, we randomly assigned individuals to
a Washington Post free subscription treatment, a Washington Times
free subscription treatment, or a control treatment. We find no effect
of either paper on political knowledge, stated opinions, or turnout in
post-election survey and voter data. However, receiving either paper
led to more support for the Democratic candidate, suggesting that
media slant mattered less in this case than media exposure. Some evidence
from voting records also suggests that receiving either paper led
to increased 2006 voter turnout. ({JEL} D72, L82)},
	pages = {35--52},
	number = {2},
	journaltitle = {American Economic Journal: Applied Economics},
	author = {Gerber, Alan S. and Karlan, Dean and Bergan, Daniel},
	urldate = {2019-01-11},
	date = {2009-04},
	langid = {english},
	keywords = {Media, Models of Political Processes: Rent-seeking, Elections, Legislatures, and Voting Behavior, Entertainment},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/9HHDV4FA/Gerber et al. - 2009 - Does the Media Matter A Field Experiment Measurin.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/ICJ94SJT/articles.html:text/html}
}

@article{dellavigna_fox_2007,
	title = {The Fox News Effect: Media Bias and Voting},
	volume = {122},
	issn = {0033-5533},
	url = {https://academic.oup.com/qje/article/122/3/1187/1879517},
	doi = {10.1162/qjec.122.3.1187},
	shorttitle = {The Fox News Effect},
	abstract = {Abstract.  Does media bias affect voting? We analyze the entry of Fox News in cable markets and its impact on voting. Between October 1996 and November 2000, th},
	pages = {1187--1234},
	number = {3},
	journaltitle = {The Quarterly Journal of Economics},
	shortjournal = {Q J Econ},
	author = {{DellaVigna}, Stefano and Kaplan, Ethan},
	urldate = {2019-01-11},
	date = {2007-08-01},
	langid = {english},
	file = {Full Text PDF:/Users/franzi/Zotero/storage/MFV7TPM5/DellaVigna and Kaplan - 2007 - The Fox News Effect Media Bias and Voting.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/65VZTT3U/1879517.html:text/html}
}

@book{hamilton_all_2004,
	title = {All the News That's Fit to Sell: How the Market Transforms Information into News},
	isbn = {978-0-691-12367-7},
	url = {https://www.jstor.org/stable/j.ctt7smgs},
	shorttitle = {All the News That's Fit to Sell},
	abstract = {That market forces drive the news is not news. Whether a story appears in print, on television, or on the Internet depends on who is interested, its value to advertisers, the costs of assembling the details, and competitors' products. But in \textit{All the News That's Fit to Sell} , economist James Hamilton shows just how this happens. Furthermore, many complaints about journalism--media bias, soft news, and pundits as celebrities--arise from the impact of this economic logic on news judgments.  This is the first book to develop an economic theory of news, analyze evidence across a wide range of media markets on how incentives affect news content, and offer policy conclusions. Media bias, for instance, was long a staple of the news. Hamilton's analysis of newspapers from 1870 to 1900 reveals how nonpartisan reporting became the norm. A hundred years later, some partisan elements reemerged as, for example, evening news broadcasts tried to retain young female viewers with stories aimed at their (Democratic) political interests. Examination of story selection on the network evening news programs from 1969 to 1998 shows how cable competition, deregulation, and ownership changes encouraged a shift from hard news about politics toward more soft news about entertainers.  Hamilton concludes by calling for lower costs of access to government information, a greater role for nonprofits in funding journalism, the development of norms that stress hard news reporting, and the defining of digital and Internet property rights to encourage the flow of news. Ultimately, this book shows that by more fully understanding the economics behind the news, we will be better positioned to ensure that the news serves the public good.},
	publisher = {Princeton University Press},
	author = {Hamilton, James T.},
	urldate = {2019-01-19},
	date = {2004},
	doi = {10.2307/j.ctt7smgs}
}

@article{puglisi_being_2011,
	title = {Being The New York Times: the Political Behaviour of a Newspaper},
	volume = {11},
	url = {https://econpapers.repec.org/article/bpjbejeap/v_3a11_3ay_3a2011_3ai_3a1_3an_3a20.htm},
	shorttitle = {Being The New York Times},
	abstract = {I analyse a dataset of news from The New York Times, from 1946 to 1997. Controlling for the activity of the incumbent president and the U.S. Congress across issues, I find that during a presidential campaign, The New York Times gives more emphasis to topics on which the Democratic party is perceived as more competent (civil rights, health care, labor and social welfare) when the incumbent president is a Republican. This is consistent with the hypothesis that The New York Times has a Democratic partisanship, with some “anti-incumbent” aspects, in that—during a presidential campaign—it gives more emphasis to issues over which the (Republican) incumbent is weak. To the extent that the interest of readers across issues is not systematically related with the political affiliation of the incumbent president and the election cycle, the observed changes in news coverage are consistent with The New York Times departing from demand-driven news coverage. In fact, I show that these findings are robust to controlling for Gallup data on the most important problem facing the country, which I use as a proxy for issue tastes of Times’ readers.},
	pages = {1--34},
	number = {1},
	journaltitle = {The B.E. Journal of Economic Analysis \& Policy},
	author = {Puglisi, Riccardo},
	urldate = {2019-01-19},
	date = {2011},
	file = {RePEc Snapshot:/Users/franzi/Zotero/storage/FAGD2JNZ/v_3a11_3ay_3a2011_3ai_3a1_3an_3a20.html:text/html}
}

@article{larcinese_partisan_2011,
	title = {Partisan bias in economic news: Evidence on the agenda-setting behavior of U.S. newspapers},
	volume = {95},
	issn = {0047-2727},
	url = {http://www.sciencedirect.com/science/article/pii/S0047272711000715},
	doi = {10.1016/j.jpubeco.2011.04.006},
	series = {Special Issue: The Role of Firms in Tax Systems},
	shorttitle = {Partisan bias in economic news},
	abstract = {We study the agenda-setting political behavior of a large sample of U.S. newspapers during the 1996–2005 period. Our purpose is to examine the intensity of coverage of economic issues as a function of the underlying economic conditions and the political affiliation of the incumbent president, focusing on unemployment, inflation, the federal budget and the trade deficit. We investigate whether there is any significant correlation between the endorsement policy of newspapers, and the differential coverage of bad/good economic news as a function of the president's political affiliation. We find evidence that newspapers with pro-Democratic endorsement pattern systematically give more coverage to high unemployment when the incumbent president is a Republican than when the president is Democratic, compared to newspapers with pro-Republican endorsement pattern. This result is robust to controlling for the partisanship of readers. We find similar but less robust results for the trade deficit. We also find some evidence that newspapers cater to the partisan tastes of readers in the coverage of the budget deficit. We find no evidence of a partisan bias – or at least of a bias that is correlated with the endorsement or reader partisanship – for stories on inflation.},
	pages = {1178--1189},
	number = {9},
	journaltitle = {Journal of Public Economics},
	shortjournal = {Journal of Public Economics},
	author = {Larcinese, Valentino and Puglisi, Riccardo and Snyder, James M.},
	urldate = {2019-01-19},
	date = {2011-10-01},
	keywords = {Information, Media bias, Mass media, News},
	file = {Full Text:/Users/franzi/Zotero/storage/T7Z3DYRR/Larcinese et al. - 2011 - Partisan bias in economic news Evidence on the ag.pdf:application/pdf;ScienceDirect Snapshot:/Users/franzi/Zotero/storage/EVVMYWAY/S0047272711000715.html:text/html}
}

@article{baron_persistent_2006,
	title = {Persistent media bias},
	volume = {90},
	issn = {0047-2727},
	url = {http://www.sciencedirect.com/science/article/pii/S0047272705000216},
	doi = {10.1016/j.jpubeco.2004.10.006},
	abstract = {The news media plays an essential role in society, but surveys indicate that the public views the media as biased. This paper presents a theory of media bias that originates with private information obtained by journalists through their investigations and persists despite profit-maximizing news organizations and rivalry from other news organizations. Bias has two effects on the demand for news. First, rational individuals are more skeptical of potentially biased news and thus rely less on it in their decision-making. This skepticism reduces demand and leads the news organization to set a lower price for its publication the greater is the bias it tolerates. Lower quality news thus commands a lower price. Second, bias makes certain stories more likely than others. Given their private information, journalists may bias their stories if their career prospects can be advanced by being published on the front page. News organizations can control bias by restricting the discretion allowed to journalists, but granting discretion and tolerating bias can increase profits if it allows journalists to be hired at a lower wage. Bias is not driven from the market by a rival news organization nor by a news organization with an opposing bias, and the profits of a high-bias news organization can be higher than the profits of a low bias one. Moreover, bias can be greater with competition than with a monopoly news organization. If individuals collectively choose regulation in place of their individual decision-making, bias increases the expected stringency of regulation.},
	pages = {1--36},
	number = {1},
	journaltitle = {Journal of Public Economics},
	shortjournal = {Journal of Public Economics},
	author = {Baron, David P.},
	urldate = {2019-01-19},
	date = {2006-01-01},
	keywords = {Media, Bias, News organizations},
	file = {ScienceDirect Snapshot:/Users/franzi/Zotero/storage/ESVNXLRG/S0047272705000216.html:text/html}
}

@article{mullainathan_market_2005,
	title = {The Market for News},
	volume = {95},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257/0002828054825619},
	doi = {10.1257/0002828054825619},
	abstract = {We investigate the market for news under two assumptions: that readers hold beliefs which they like to see confirmed, and that newspapers can slant stories toward these beliefs. We show that, on the topics where readers share common beliefs, one should not expect accuracy even from competitive media: competition results in lower prices, but common slanting toward reader biases. On topics where reader beliefs diverge (such as politically divisive issues), however, newspapers segment the market and slant toward extreme positions. Yet in the aggregate, a reader with access to all news sources could get an unbiased perspective. Generally speaking, reader heterogeneity is more important for accuracy in media than competition per se.},
	pages = {1031--1053},
	number = {4},
	journaltitle = {American Economic Review},
	author = {Mullainathan, Sendhil and Shleifer, Andrei},
	urldate = {2019-01-19},
	date = {2005-09},
	langid = {english},
	file = {Snapshot:/Users/franzi/Zotero/storage/2YJNP8J6/articles.html:text/html;Submitted Version:/Users/franzi/Zotero/storage/RVCNEDQH/Mullainathan and Shleifer - 2005 - The Market for News.pdf:application/pdf}
}

@article{gentzkow_television_2006,
	title = {Television and Voter Turnout},
	volume = {121},
	issn = {0033-5533},
	url = {https://academic.oup.com/qje/article/121/3/931/1917885},
	doi = {10.1162/qjec.121.3.931},
	abstract = {Abstract.  I use variation across markets in the timing of television's introduction to identify its impact on voter turnout. The estimated effect is significan},
	pages = {931--972},
	number = {3},
	journaltitle = {The Quarterly Journal of Economics},
	shortjournal = {Q J Econ},
	author = {Gentzkow, Matthew},
	urldate = {2019-01-19},
	date = {2006-08-01},
	langid = {english},
	file = {Snapshot:/Users/franzi/Zotero/storage/RH435L8T/1917885.html:text/html;Submitted Version:/Users/franzi/Zotero/storage/JK7HZM2M/Gentzkow - 2006 - Television and Voter Turnout.pdf:application/pdf}
}

@article{suen_self-perpetuation_2004,
	title = {The Self-Perpetuation of Biased Beliefs},
	volume = {114},
	url = {https://ideas.repec.org/a/ecj/econjl/v114y2004i495p377-396.html},
	abstract = {To overcome strong prior beliefs, strong evidence to the contrary is needed. If a person is predisposed to choosing a certain action, the advice from an advisor who sets a low threshold for recommending the alternative action is not of much use. The preference for like-minded advisors who supply coarse information implies that the advice a person receives is likely to reinforce his existing priors. This effect can lead to polarisation of opinion and the emergence of self-serving beliefs. The learning process is prolonged and the induced short run bias can become perpetual if information is costly. Copyright 2004 Royal Economic Society.},
	pages = {377--396},
	number = {495},
	journaltitle = {Economic Journal},
	author = {Suen, Wing},
	urldate = {2019-01-19},
	date = {2004},
	langid = {english},
	file = {Snapshot:/Users/franzi/Zotero/storage/BSUZCHV8/v114y2004i495p377-396.html:text/html}
}

@incollection{kepplinger_einfluss_2004,
	location = {Wiesbaden},
	title = {Der Einfluss der Pressemitteilungen der Bundesparteien auf die Berichterstattung im Bundestagswahlkampf 2002},
	isbn = {978-3-322-83381-5},
	url = {https://doi.org/10.1007/978-3-322-83381-5_9},
	abstract = {Startschüsse sind notwendig, laut und sehen. Dies trifft auch auf die Studie zum Einfluss der Pressearbeit der Parteien in Nordrhein-Westfalen auf die Berichterstattung regionaler Tageszeitungen zu, die Barbara Baerns bereits 1981 abgeschlossen, jedoch erst später unter dem Titel „Öffentlichkeitsarbeit oder Joumalismus? Zum Einflu{\textbackslash}s s im Mediensystem“ (Baerns 1985) publizierte. Sie öffnete ein weites Forschungsfeld, das zwar viele geahnt und wortreich umschrieben haben, aber niemand auf breiter Basis empirisch angegangen war. Nicht neu, aber wegweisend war die Anlage der Untersuchung als Input-Output-Analyse, die allerdings auf einen Vergleich der Stuktur des Angebotes an {PR}-Meldungen mit der Stmktur der abgedmck- ten Angebote beschränkt blieb (Baerns 1985: 70-73). Spektakulär und viel zitiert war ihr Befund, dass knapp zwei Drittel der publizierten Beiträge auf {PR}-Quellen bemhten (Baerns 1985: 66-68). Er wurde zur Gmndlage der „Determiniemngs These“, die angesichts wachsender {PR}-Abteilungen und schmmpfender Redaktionen besondere Aktualität besitzt.},
	pages = {113--124},
	booktitle = {Quo vadis Public Relations? Auf dem Weg zum Kommunikationsmanagement: Bestandsaufnahmen und Entwicklungen},
	publisher = {{VS} Verlag für Sozialwissenschaften},
	author = {Kepplinger, Hans Mathias and Maurer, Marcus},
	editor = {Raupp, Juliana and Klewes, Joachim},
	date = {2004},
	doi = {10.1007/978-3-322-83381-5_9}
}

@report{newman_reuters_2018,
	title = {Reuters Institute Digital News Report 2018},
	url = {http://media.digitalnewsreport.org/wp-content/uploads/2018/06/digital-news-report-2018.pdf?x89475},
	institution = {Reuters Institute for the Study of Journalism},
	author = {Newman, Nic and Fletcher, Richard and Kalogeropoulos, Antonis and Levy, David and Kleis Nielsen, Rasmus},
	date = {2018}
}

@article{besley_handcuffs_2006,
	title = {Handcuffs for the Grabbing Hand? Media Capture and Government Accountability},
	volume = {96},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.96.3.720},
	doi = {10.1257/aer.96.3.720},
	shorttitle = {Handcuffs for the Grabbing Hand?},
	abstract = {It has long been recognized that the media play an essential role in government
accountability. Even in the absence of censorship, however, the government may
influence news content by maintaining a "cozy" relationship with the media. This
paper develops a model of democratic politics in which media capture is endogenous.
The model offers insights into the features of the media market that determine
the ability of the government to exercise such capture and hence to influence
political outcomes. ({JEL} D72, D73, L82)},
	pages = {720--736},
	number = {3},
	journaltitle = {American Economic Review},
	author = {Besley, Timothy and Prat, Andrea},
	urldate = {2019-03-30},
	date = {2006-06},
	langid = {english},
	file = {Full Text:/Users/franzi/Zotero/storage/6H3PQSX8/Besley and Prat - 2006 - Handcuffs for the Grabbing Hand Media Capture and.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/GPVE7D97/articles.html:text/html}
}

@article{gentzkow_media_2006,
	title = {Media Bias and Reputation},
	volume = {114},
	issn = {0022-3808},
	url = {https://www.journals.uchicago.edu/doi/10.1086/499414},
	doi = {10.1086/499414},
	abstract = {A Bayesian consumer who is uncertain about the quality of an information source will infer that the source is of higher quality when its reports conform to the consumer’s prior expectations. We use this fact to build a model of media bias in which firms slant their reports toward the prior beliefs of their customers in order to build a reputation for quality. Bias emerges in our model even though it can make all market participants worse off. The model predicts that bias will be less severe when consumers receive independent evidence on the true state of the world and that competition between independently owned news outlets can reduce bias. We present a variety of empirical evidence consistent with these predictions.},
	pages = {280--316},
	number = {2},
	journaltitle = {Journal of Political Economy},
	shortjournal = {Journal of Political Economy},
	author = {Gentzkow, Matthew and Shapiro, Jesse M.},
	urldate = {2019-03-30},
	date = {2006-04-01},
	file = {Full Text:/Users/franzi/Zotero/storage/68DMKQDE/Gentzkow and Shapiro - 2006 - Media Bias and Reputation.pdf:application/pdf;Snapshot:/Users/franzi/Zotero/storage/SHBPKLPN/499414.html:text/html}
}

@article{brandenburg_political_2005-1,
	title = {Political Bias in the Irish Media: A Quantitative Study of Campaign Coverage during the 2002 General Election},
	volume = {20},
	issn = {0790-7184},
	url = {https://doi.org/10.1080/07907180500359350},
	doi = {10.1080/07907180500359350},
	shorttitle = {Political Bias in the Irish Media},
	abstract = {The aim of this study is to give some systematic insights into how Irish media tend to report an election campaign. The main focus will be on their attitudes toward and treatment of the competing parties and candidates. Content analysis data from television newscasts and campaign stories in four of the largest newspapers is used to investigate three different forms of media bias: coverage bias, agenda bias, and statement bias. We find that Irish media tend to grant disproportionate amounts of coverage to the government parties, Fianna Fáil and the Progressive Democrats; the more prominent the coverage, the less proportionate it becomes. The extent to which media take the freedom to ‘distort’ party agendas in their reporting appears to depend on party size, campaign strategy and the acquired status and acceptance of a party amongst the political and media establishment. Most notable, however, is the predominantly negative attitude of all Irish print media towards political actors. Instead of a polarised partisan press, as for example in the {UK}, in Ireland we seem to be faced with a rather homogenous anti‐politics bias.},
	pages = {297--322},
	number = {3},
	journaltitle = {Irish Political Studies},
	author = {Brandenburg, Heinz},
	urldate = {2019-04-06},
	date = {2005-09-01},
	file = {Snapshot:/Users/franzi/Zotero/storage/VUZHXEED/07907180500359350.html:text/html}
}

@incollection{anderson_media_2006,
	title = {The Media and Advertising: A Tale of Two-Sided Markets},
	volume = {1},
	url = {http://www.sciencedirect.com/science/article/pii/S1574067606010180},
	shorttitle = {Chapter 18 The Media and Advertising},
	abstract = {Media industries are important drivers of popular culture. A large fraction of leisure time is devoted to radio, magazines, newspapers, the Internet, and television (the illustrative example henceforth). Most advertising expenditures are incurred for these media. They are also mainly supported by advertising revenue. Early work stressed possible market failures in program duplication and catering to the Lowest Common Denominator, indicating lack of cultural diversity and quality. The business model for most media industries is underscored by advertisers' demand to reach prospective customers. This business model has important implications for performance in the market since viewer sovereignty is indirect. Viewers are attracted by programming, though they dislike the ads it carries, and advertisers want viewers as potential consumers. The two sides are coordinated by broadcasters (or “platforms”) that choose ad levels and program types, and advertising finances the programming. Competition for viewers of the demographics most desired by advertisers implies that programming choices will be biased towards the tastes of those with such demographics. The ability to use subscription pricing may help improve performance by catering to the tastes of those otherwise under-represented, though higher full prices tend to favor broadcasters at the expense of viewers and advertisers. If advertising demand is weak, program equilibrium program selection may be too extreme as broadcasters strive to avoid ruinous subscription price competition, but strong advertising demand may lead to strong competition for viewers and hence minimum differentiation (“la pensée unique”). Markets (such as newspapers) with a high proportion of ad-lovers may be served only by monopoly due to a circulation spiral: advertisers want to place ads in the paper with most readers, but readers want to buy the paper with more ads.},
	pages = {567--614},
	booktitle = {Handbook of the Economics of Art and Culture},
	publisher = {Elsevier},
	author = {Anderson, Simon P. and Gabszewicz, Jean J.},
	editor = {Ginsburg, Victor A. and Throsby, David},
	urldate = {2019-04-19},
	date = {2006-01-01},
	doi = {10.1016/S1574-0676(06)01018-0},
	keywords = {platform competition, two-sided markets, advertising finance, circulation spiral, pensee unique},
	file = {ScienceDirect Snapshot:/Users/franzi/Zotero/storage/SVCVBE4B/S1574067606010180.html:text/html}
}

@article{oegema_personalization_2009,
	title = {Personalization in political Television News: A 13-Wave Survey Study to Assess Effects of Text and Footage},
	volume = {25},
	issn = {1613-4087},
	url = {https://www.degruyter.com/view/j/comm.2000.25.issue-1/comm.2000.25.1.43/comm.2000.25.1.43.xml},
	doi = {10.1515/comm.2000.25.1.43},
	shorttitle = {Personalization in political Television News},
	pages = {43--60},
	number = {1},
	journaltitle = {Communications},
	author = {Oegema, Dirk and Kleinnijenhuis, Jan},
	urldate = {2019-04-19},
	date = {2009}
}